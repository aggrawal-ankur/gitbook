{"/gitbook/blogs/":{"data":{"":"I am on a quest to learn better everyday and in this process I discover ways to learn better.\nThis is the place where I share my findings."},"title":"Learning By Learning"},"/gitbook/blogs/learning-process-optimization/":{"data":{"":"Got the idea to manage this on 17 September 2025, 5:30 PM\nMy gitbook is the proof of all the low level knowledge and understanding I have about computers.\nI write everything I learn in a teaching format so that I can find patches in my understanding. This helps in strengthening my overall understanding of a topic.\nIn this rigorous process, I invest a lot of time and energy in optimizing my write ups so that I can understand to the best of my capabilities. Often this comes at a cost of time and cognition.\nJust to refine my understanding more and more to the point that I can say “I really understand it”, I do end up investing a lot more time and energy than probably required.\nFor many it is a waste, but not for me. Such pursuits always reveal something about the process that I can optimize next. Each such exploration becomes a tool which is going to save a lot of my time and energy in future explorations.\nAnd this is where I am listing all those tools with some examples in my own write ups.\nHope they help you as well.\nSeptember 14, 2025\nSearching for in-depth information on google is sometimes a nightmare.\nIn my journey, I try to avoid google searches and stick to formal documentations and ChatGPT because they save my time and energy.\nI ask ChatGPT to introduce me to a concept and then I ask for documents if any. I ask it if I can write some code myself to try it out. This way, I am able to build a deeper understanding of the concept, rather than reading half-baked articles claiming for an in-depth dive.\nMany articles on internet aren’t even exactly for the thing you search for. Take dlmalloc.\nWhen I was understanding allocators, I can’t find anything relevant but heap exploitation. Let me understand heap before I understand its exploitation? And the information shared there about dlmalloc is literally of no use to me. And I have wasted almost 4-5 days in them. Then I started understanding the dlmalloc annotations and asking ChatGPT what I don’t understand. Once I start to feel confident, I document my understanding here. Then I polish them. In this process, I have found that the images tab in google search is a goldmine for websites that can’t make up on the first page. I have found so many websites with great content (obviously not related to the current pursuit but something later).\nSo, image tab is always worth a try.\nSeptember 15, 2025\nASCII art is a very great tool for visualizing boring theoretical concepts.\nWhy theory is hated is because you can’t visualize it. While I was learning low level stuff, I was basically studying a lot of theoretical concepts which I can’t see properly.\nTake stack. You can use gdb to look for a process’s running stack but it is just addresses. But if you map that theory in an ASCII diagram, you get exactly how a stack of plates looks like.\nEveryone just says that a process’s stack is just like a stack of plates, but can you prove that? Without any cryptic looking addresses which just haunt beginners? Why not? You can absolutely do that. And you don’t need fancy canvas tools to draw stack. Just use ASCII. And by the way, just open VS Code and paste any ASCII art from my write ups, it looks absolutely gorgeous. How stack frame looks like? Virtual memory layout\nYou absolutely don’t need fancy tools to draw theory. Theory shouldn’t be daunting.\nSeptember 17, 2025\nThe best way to represent “type” related information is a table. A table can consolidate huge amounts of information in a very small form factor.\nIt reduce the mental overhead of “too much information, not my cup of tea”. Plus, a table looks prettier than a dense paragraph.\nThat’s why if there is a possibility to explain something using a table, I am always up for that and it is very much reflected in my notes.\nDoug Lea’s memory model Types of bins September 21, 2025\nHow to ensure that my approach positions me better in the future?\nThere can be multiple answers to this question but almost all of them boils down to one thing — prioritize long term over short term, my short term must align with my long term goals.\nAnd a few moments before, I found a way to implement it.\nRecently I finished the theoretical part of understanding dlmalloc, which itself is a part of understand dynamic memory allocation. Now I want to verify all the theory I was learning since 2 weeks.\nFor this I have to use dlmalloc as the memory allocator for my experimental programs and inspect the process as a sentinel being. I already got dlmalloc setup and running inside a VM, I didn’t expect a 13 years code to run without any compilation errors, so a big thank you to Professor Doug Lea for maintaining the code quality to such a level. Now this inspection can only be done via debugger. GDB is quite popular on Linux so that’s my choice. Now there are two ways to approach this.\nI just use and memorize gdb commands and do the inspection. That ensures speed. Or I take a long road to understand the “fundamentals of debugging” and then learn how gdb implements them. As you have guessed, I am more of type 2. And this is exactly what I did with dynamic memory allocation. I didn’t read some articles on heap exploitation which touched dlmalloc on surface, I chose to explore dlmalloc myself. And there is a long way to go before I can say “I am done with dynamic memory for the time being.”\nWhy the 2nd approach?\nThe second approach maximizes learning the fundamental concepts on which everything is built. It is like maximizing for transferable skills, which remain consistent across different systems and architectures. This approach ensures that I understand what the debugger is actually doing and it doesn’t seem like black magic.\nSeptember 24, 2025\nNever ever disrespect theory. If you disrespect theory, you will pay for it, in hefty sums.\nRight now I am learning GDB so that I can verify all the theory I have understood about dlmalloc. As usual, before learning the main thing, I learn the peripherals. But now I am at the commands.\nFor everyone, these commands are just another commands. But the way I am seeing them is a treasure.\nI have been diving into theory since May 2025. I started with x64 assembly, then hello world exploration for 35 days, ELF internals, more assembly, C to assembly mapping, virtual memory, dynamic memory allocation and then dlmalloc.\nUp until this time, there was no way for me to testify my understanding. But now, I feel like I have found a cave full of treasure. It’s like I have found they key to Ali Baba and the Forty Thieves cave.\nI am not even doing any practical work here, I am just exploring what is all that can be done with gdb and while learning stack management, I found that local variables are present exactly the same way I learned about them. Registers are managed the same way I understood.\nI am so excited that my own internal instruction pointer is lost right now. I am not able to understand which theory I should verify first.\nIf you have asked me this in August itself, I was literally going through agitation, frustration, and anger. There was no day when I would say “I am done with this.” Now, you can feel how happy I am.\nThat’s why I am telling YOU, don’t ever escape theory, never ever despise it. It’s brutal, it’s boring, it’s non-interesting, everything bad that you can think about; but it is the only thing that makes the future interesting and incredibly rewarding.\nI feel proud of myself that in all those days of anger, frustration, agitation, I didn’t chose to quit.\nI did stopped a lot. But I can’t prove much of it. Only my mother can prove that because she has seen me packing my laptop and keyboard and putting them aside in another room and sitting alone at my place for multiple days doing nothing, just because he’s so tired of doing it. And she has also seen days when I go to her and say “today I learned for 8 hours”. And now, I don’t need anyone to rely on for this, because I myself am the proof that I did that, and the universe rewarded me later.\nSo, never ever think about skipping theory."},"title":"Learning Process Optimization"},"/gitbook/blogs/switched-to-hugo/":{"data":{"":"Finally, the switch to GitHub Pages + Hugo + Hextra is done."},"title":"I Switched To Hugo"},"/gitbook/docs/all-roads-to-memory/":{"data":{"all-roads-to-memory#All Roads To Memory":"All Roads To MemoryAugust 9, 2025 (init write up)\nSeptember 13, 2025 (rewriting that because of dynamic memory allocation)\nAssembly said that memory is a flat array of 1-byte blocks and the ELF parser project proved that. Now it’s time to dive in the real structure of memory.\nWe are not diving into physical memory yet. We will start with virtual memory as it is where everything begins from."},"title":"All Roads To Memory"},"/gitbook/docs/all-roads-to-memory/dma/":{"data":{"":"9, 10, 15 September 2025\nThese are the primary functions used for dynamic memory allocation in C. These are wrappers in libc/glibc. The underlying machinery that powers them is called the allocator program.\nReturned pointer is aligned for any built-in type.\nAssembly just calls these functions and waits for rax to get the pointer to the memory.\nAn allocator is the program that manages dynamic memory requirements.\nEach process gets its own allocator instance, which manages its dynamic memory requirements. There are multiple implementations of allocator, each designed for specific project requirements. We can create our own allocator program as well.\nThe most widely used allocators include:\nptmalloc: GNU’s implementation for glibc, written by Wolfram Gloger. tcmalloc : Google’s implementation for their C/C++ projects. jemalloc : Created by Jason Evans for a programming language project but came out as a great memory allocator which was integrated into FreeBSD and various other platforms like Facebook and Firefox use it. Link But the father of all allocators, not the first memory allocator, but definitely the most significant and influential one was dlmalloc. This is what we are going to start with.\ndlmalloc stands for Doug Lea’s memory allocator implementation.\nHe is a professor of CS at State University of New York at Oswego. This repository has every version of dlmalloc. This repository has a clear version of dlmalloc as the the original version uses a strange dialect of C which is not beginner friendly. This repository has multiple allocator implementations under one roof."},"title":"Dynamic Memory Allocation"},"/gitbook/docs/all-roads-to-memory/dma/deallocation/":{"data":{"":"10 September 2025\nDeallocation as a process can be divided into two parts.\nMaking the memory inaccessible. Reclamation by kernel. There are hundreds of process getting created and exiting simultaneously. All of them are accessing the same computer memory.\nAlthough the virtual address space is quite large, the physical space is still limited. And the virtual space is mapped with the physical space by the MMU.\nWhen a process used a part of memory and now it’s time for deallocation, there can be two path. Either you zero the whole memory used by the process or you make it inaccessible.\nPath 1 sounds more safe as it ensures that there is no possibility of data leaks. Path 2 sounds more like a irresponsible way to handle memory. But is it really? As we have seen before, there are hundreds of processes in action simultaneously. And their virtual address space do differs but at the end of the day, it is going to be mapped with physical memory.\nWhen memory is constantly in use, it is so complex to leak data. It is highly unlikely that you won’t find the memory location with some data on it, but the question is, how you are going to make sense of it? How you are going to establish the integrity that this data belongs to that process specifically?\nThis is the reason that makes path 2 a clever way instead of an irresponsible one. Because deallocation will come at a cost. You have to put zero or any sentinel value on all the memory locations. While this is possible for smaller allocations, this can be really a resource exhausting take on deallocation. That is why making memory inaccessible is cheaper and makes logically sense as well.\nNow comes part 2, reclamation by the kernel.\nWhen we call free(ptr), it just tells the allocator that this part of memory can be reused by the process. The allocator marks this memory as free to use.\nThis memory is still mapped in that process as it is allocated to it. The memory still has the contents but they aren’t accessible (directly). free(ptr) never really frees anything in an absolute sense. It just makes the memory inaccessible via usual means. As we know, there can be two ways in which dynamic memory is allocated, brk() and mmap().\nFor brk() based allocation, if the top of the heap is free, the allocator can move the program break back down and this is called as heap trimming. This part of heap is now unmapped and the kernel reclaims it instantly, mid-process. For mmap() based allocation, on a call to free(ptr) , the kernel calls munmap() , which stands for memory unmapping. And the kernel reclaims the memory instantly, again mid-process. Stack also grows automatically with page faults. if you don’t use much of the space, the kernel can unmap and reclaim the space instantly. In all of these cases, the kernel destroys the part of the data structure which was accounting the memory allocation for that part in that process. Every region, stack/heap/mmap becomes unmapped and the memory is reclaimed. And the memory reclaimed is no longer accessible via any means. When the process dies, the kernel destroys the entire data structure which was accounting the memory allocation for that process. Every region, stack/heap/mmap becomes unmapped and the memory is reclaimed.\nWhen a new process spawns and demands memory, the kernel zeroes the memory before making it available for that process. This guarantees no cross-process leaks. Zeroing happens on demand, so idle freed pages aren’t wasted effort.\nSo effectively, there are two cases:\nFreed but not reclaimed. Freed and reclaimed. And zeroing happens for inter-process, not intra-process."},"title":"Deallocation"},"/gitbook/docs/all-roads-to-memory/dma/dma-syscalls/":{"data":{"":"10 September 2025","brk#brk()":"There is a syscall named brk() which is used to extend the program break. What is program break?\nIn the early days of dynamic memory allocation, the data segment was data/bss and heap together.\nIt is perfectly logical as compilation already reveals how much space you need for static/globals so the lower part of the data segment was reserved for static/globals and the upper part was reserved for heap. Therefore, program break is the boundary which logically separates the data/bss part from heap.\nFor example, the data segment starts at 0d1000 and ends at 0d1015 . This means that 16 bytes are required for data/bss. Now the program break is at 0d1016 just one byte after the data/bss allocation. If any function from malloc family is called, the brk() is executed to extend the program break. And this new space is what heap is.\nbrk() takes an address and changes the program break to it. But how we are supposed to know where the current program break is?\nbrk(0) gives the current program break. But a problem with brk() is that it doesn’t return the pointer to the newly allocated space.","mmap#mmap()":"mmap is a Linux syscall and mmap() is libc wrapper around it.\nmmap stands for memory map which lets a process map files or anonymous memory into its virtual address space.\nUnlike brk/sbrk, which adjust the heap break, mmap can allocate memory anywhere in the mmap region of the virtual address space, not just growing the heap upward.\nIf mmap() was successful, it returns a pointer to the allocated memory. If failed, (void *) -1.\nEvery time we run a program on Linux, the dynamic linker (ld.so) uses mmap to load shared libraries (.so files) in our address space. So, we don’t use mmap directly, unless we’re doing systems programming; but we are incomplete without it.\nIn a more melodramatic way, we might not use it directly, but Its presence is a boon to us. mmap has a variety of use cases and dynamic memory allocation is one of them.\nFile mapping. Map a file into memory and access it like an array. Anonymous mapping: Heap-like memory without touching the process break. Shared memory: Two processes can map the same file and see each other’s updates.","sbrk#sbrk()":"sbrk() is a C library function, which is a wrapper over the actual brk() syscall.\nbrk() returns 0 on success and -1 on failure. sbrk() returns the pointer to the newly allocated memory or the previous program break on success and (void *)-1 on failure.\nIn practice, we use sbrk() not brk(). Although the use of both is not recommended today; instead we should use functions from malloc family.\nA key thing about sbrk(n) is that it extends heap contiguously. And we can prove this by a simple example:\n#include #include int main() { void *initial_break = sbrk(0); // get current break // Allocate 32 bytes using sbrk void *new_mem = sbrk(32); if (new_mem == (void*) -1) { perror(\"sbrk failed\"); return 1; } // New program break void *after_alloc = sbrk(0); printf(\"Initial program break: %p\\n\", initial_break); printf(\"Allocated 32 bytes at: %p\\n\", new_mem); printf(\"Program break after sbrk: %p\\n\", after_alloc); return 0; } The output is:\n$ gcc main.c $ ./a.out Initial program break: 0x55bc200da000 Allocated 32 bytes at: 0x55bc200da000 Program break after sbrk: 0x55bc200da020 0x55bc200da000 = 0d94266479976448 0x55bc200da020 = 0d94266479976480 The difference is exactly 32 bytes.\nBut, if you do this:\n#include #include int main() { void *initial_break = sbrk(0); // get current break printf(\"Initial program break: %p\\n\", initial_break); // Allocate 32 bytes using sbrk void *new_mem = sbrk(32); if (new_mem == (void*) -1) { perror(\"sbrk failed\"); return 1; } printf(\"Allocated 32 bytes at: %p\\n\", new_mem); // New program break void *after_alloc = sbrk(0); printf(\"Program break after sbrk: %p\\n\", after_alloc); return 0; } the output changes significantly.\n$ gcc main.c $ ./a.out Initial program break: 0x559b4fc4b000 Allocated 32 bytes at: 0x559b4fc6c000 Program break after sbrk: 0x559b4fc6c020 Just by the output we can see that the jump in address is way too much. This behavior might be attributed to printf calling malloc internally for its requirements.\nTherefore, never mix the two.","sbrk-or-mmap#sbrk() or mmap()":"How the allocator decides whether to use sbrk() or mmap() ?\nAlthough the exact implementation can vary, the concept remains the same. Small allocations via sbrk() and large allocations via mmap(). The definition of small and large can be allocator specific which we will explore later. And we are done with the syscalls enabling dynamic memory allocation.","user-space-layout#User Space Layout":"This is the virtual address space layout for user space memory. For more information, checkout virtual-memory-layout.md\nUser Space Memory Layout *--------------------------* | High Memory (~128 TiB) | | *-----------------* | | | Stack (↓) | | | *-----------------* | | | Mmap region | | | *-----------------* | | | Free Space | | | *-----------------* | | | Heap (↑) | | | *-----------------* | | | Data (data/bss) | | | *-----------------* | | | Code | | | *-----------------* | | Low Memory (0..0) | *--------------------------* The “heap” region and the “mmap” region both supports dynamic memory allocation. But both are managed differently, which is why we have two different methods for dynamic memory allocation.\nThey are sbrk() and mmap() . sbrk() manages the heap region and mmap() manages the mmap region.","where-are-the-boundaries#Where are the boundaries?":"If you notice, stack is free flowing, heap is free flowing and mmap is free flowing. Where are the boundaries that prevent collision?\nCode section is fixed at compile-time. data/bss size is known at compile-time, so that is also fixed. The start of heap is fixed, just after data/bss. But the end is floating. The start of stack is fixed at the top of user space and grows downwards. But the end is floating again, depending on stack pointer. At last we have mmap region, which is surrounded by floating regions. The answer is that there are no boundaries. First of all, the virtual address space is large enough to make this problem insignificant for normal use case.\nSecond, the kernel has data structures which keep every allocation in control and lets the kernel not allocate memory when there is a point of conflict."},"title":"Linux Syscalls For DMA"},"/gitbook/docs/all-roads-to-memory/dma/doug-leas-memory-model/":{"data":{"":"11, 15, 17 September 2025"},"title":"Doug Lea's Memory Model"},"/gitbook/docs/all-roads-to-memory/dma/free-chunk-management/":{"data":{"":"17, 18, 19, 20 September 2025 (definitions taken out from a previous write up, written on 11, 15, 16 September 2025, optimized)\nIn-use chunks are self-owned. Their bookkeeping lives in the chunk header itself and no external registry maintains them. Free chunks require management.\nLet’s explore how free chunks are managed by the allocator.","arena#Arena":"The first malloc request sets up the arena.\nLet’s say we requested 10 bytes. The allocator is guaranteed to receive at least one page. On Linux, 4 KiB pages are more popular. So the allocator is guaranteed to setup an arena of at least 4096 bytes in in the first request.\nDepending on the system (32-bit/64-bit), the allocator will carve a chunk of size 24/48 bytes because of double-word alignment rule. So a minimum of 4072/4048 bytes will be left unused in the arena.\nThese ~4k bytes are unallocated. Where do they live?\nRemember program break we read about in syscalls section? Link. The program break is just after the data segment. When we extend the heap, the kernel releases more memory than requested, and that memory is not used all at once, but it is allocated to the allocator. The program break is the partition between the used arena and the unallocated arena. When 24 bytes are used in the arena, the program break would be at the 25th bit. The program break is a pointer to the next free byte in the arena. When there is no space left in the arena for the requested allocation, the allocator requests more memory from the kernel.\nThe topsize entry in malloc_state stores the size of the top chunk, which means the amount of unallocated space in the arena. top is a pointer to the first byte in the unallocated arena.\nThe allocator keeps requesting memory from the kernel when it runs out, the total memory ever requested by the allocator instance is recorded by the max_footprint declaration.\nThe least_addr declaration points to the lowest memory address in the arena.\nfootprint_limit is the user-defined ceiling on how much memory the allocator may request.","bins#Bins":"A bin is a bucket for storing free chunks. Different buckets exist for different chunk size.\nBins are implemented using two data structures:\nLinked Lists (Singly and Circular Doubly) Bitwise Digital Trees When chunks are not in use, they are treated as nodes of either of these.\nBins are categorized as following:\nThis categorization of bins helps balancing rapid allocation, memory usage and fragmentation.\nNote: v2.7.0 of dlmalloc used fast bins, but they were removed in v2.8.0. The last version of dlmalloc is v2.8.6, as per this repository on GitHub.","designated-victim#Designated Victim":"Many programs do repeated allocations of the same similar sizes. Designated victim is a recently freed chunk which is a part of small-medium size.\nIf a malloc request matches the the size of designated victim (dvsize), it saves the allocator some work by reusing the chunk pointer by dv.","fits-strategy#Fits Strategy":"It refers to how the allocator chooses a free chunk from bins when multiple could work.\nFirst-fit: pick the first sufficiently large chunk you find. Fast, but can cause uneven fragmentation. Best-fit: search for the chunk closest in size to request. Reduces waste but costs more CPU (searching). Next-fit: like first-fit but resume search where you left off. Spreads allocations, less clustering. dlmalloc uses:\nexact fits in small bins. best-fit within size range for large bins (but not global best-fit, just best within that bin). This hybrid gives both speed and decent fragmentation control.","malloc_state#malloc_state":"The actual bins that manage free chunks are smallbins[] and treebins[] .\nUsing the two macros above, we can find the lengths of both the bins, i.e smallbins[66] and treebins[32].","rules-for-coalescing#Rules For Coalescing":"If the previous chunk is free, merge it with the current chunk. Use prev_foot to find the previous chunk’s size and adjust pointers. If the next chunk is free (top chunk excluded), merge it with the current chunk. Remove the next chunk from its bin before merging. If the next chunk is the top chunk, just extend the top chunk’s size instead of placing the chunk in bins. If coalescing has happened, update the size field of the resulting chunk and insert it in appropriate bin.\nIf the coalesced chunk is larger than dvsize, may replace the designated victim.","small-bins#Small Bins":"Since small bins are implemented using circular doubly-linked list. We have to maintain two pointers, i.e fd and bd for each bin.\nThe small bin at 1-index is used for unsorted bin. So there will be 32 small bins and 1 unsorted bin.\nSince array indices start from zero and we are counting bins from 1, for alignment purposes, the 0th element is sentinel and is not used.\nSmall bins manage fixed size chunks. Each small bin, smallbin_1 to smallbin_32 manage sizes in multiple of 8. Therefore, the smallbins array look something like this:\nsmallbins = [ 0, ubin, fd_8, bk_8, fd_16, bk_16, fd_24, bk_24, fd_32, bk_32, fd_40, bk_40, fd_48, bk_48, fd_56, bk_56, fd_64, bk_64, fd_72, bk_72, fd_80, bk_80, fd_88, bk_88, fd_96, bk_96, fd_104, bk_104, fd_112, bk_112, fd_120, bk_120, fd_128, bk_128, fd_136, bk_136, fd_144, bk_144, fd_152, bk_152, fd_160, bk_160, fd_168, bk_168, fd_176, bk_176, fd_184, bk_184, fd_192, bk_192, fd_200, bk_200, fd_208, bk_208, fd_216, bk_216, fd_224, bk_224, fd_232, bk_232, fd_240, bk_240, fd_248, bk_248, fd_256, bk_256 ] What do these entries mean?\nSmall bins are maintained using doubly-linked circular lists, so fd_8 represents the first node in a linked list that links all the free chunks of size 8 bytes together. And bd_8 represents the end of that same linked list. The malloc_chunk struct has 4 size_t elements, which weigh 16/32 bytes on 32-bit/64-bit systems. The least memory you can request is 1 bytes, which would round up the chunk size to 24/48 bytes on 32-bit/64-bit system. That means, the small bins linking chunks of 8 and 16 bytes makes no sense?\nYeah, that’s right. But dlmalloc keeps that overhead for clarity and alignment purposes. Those bins are empty. And we will see this practically very soon. When a chunk is freed, it goes into unsorted small bin. When multiple chunks are freed together, or there is no malloc request in between multiple frees, like this:\nfree(p); free(q); // or free(a); .. free(b); .. free(c); .. free(d); free chunks are inserted in the unsorted bin in LIFO order. So the last freed chunk is the first one. But popping doesn’t follow LIFO. If 2nd chunk is found to be appropriate in a list of 5, the allocator manages linking/unlinking itself. If the next malloc request finds nothing in the unsorted bin, every chunk is popped out and linked in the respective bins.","structures-in-account#Structures In Account":"We have 3 structs and some aliases to them for different use cases. The structs remains the same, only the naming changes so that it fits the context, that’s it.\nmalloc_chunk: used for small size free chunks (by small bins). struct malloc_chunk { size_t prev_foot; size_t head; struct malloc_chunk* fd struct malloc_chunk* bk; }; typedef struct malloc_chunk mchunk; typedef struct malloc_chunk* mchunkptr; typedef struct malloc_chunk* sbinptr; malloc_tree_chunk: used for large size free chunks (by tree bins). struct malloc_tree_chunk { // Usual metadata from ll-chunks size_t prev_foot; size_t head; struct malloc_tree_chunk* fd; struct malloc_tree_chunk* bk; // Bookkeeping for trees struct malloc_tree_chunk* child[2]; struct malloc_tree_chunk* parent; bindex_t index; }; typedef struct malloc_tree_chunk tchunk; typedef struct malloc_tree_chunk* tchunkptr; typedef struct malloc_tree_chunk* tbinptr; malloc_state: The master record which manages everything for an allocator instance. struct malloc_state { binmap_t smallmap; binmap_t treemap; size_t dvsize; size_t topsize; char* least_addr; mchunkptr dv; mchunkptr top; size_t trim_check; size_t release_checks; size_t magic; mchunkptr smallbins[(NSMALLBINS+1)*2]; tbinptr treebins[NTREEBINS]; size_t footprint; size_t max_footprint; size_t footprint_limit; flag_t mflags; msegment seg; void* extp; size_t exts; }; We have a few type definitions to ensure size consistency across systems.\ntypedef unsigned int bindex_t; typedef unsigned int binmap_t; typedef unsigned int flag_t; At last, we have a few macros which define some constant values.\n#define NSMALLBINS (32U) #define NTREEBINS (32U) 32U means, take the value 32 as an unsigned integer.","tree-bins#Tree Bins":"There are 32 tree bins in total. They manage chunks falling in a specific range of bytes. This range is obtained using power of 2.\nSmall bins manage size \u003c 256 bytes. Everything after that is managed by tree bins.\n256 is 2^8. So, the first range is 257-512 bytes (512 is 2^9). Similarly we have 513-1024, 1025-2048 bytes and so on.\nTree bins are implemented using bitwise digital trees. Every element in a tree bin is a pointer to the root node of the bitwise digital tree.\nLinked List are fairly simple but bitwise digital trees are not. To understand them and visualize them, we have to practically see how a tree bin is managed by dlmalloc, which we will do very soon."},"title":"Free Chunk Management"},"/gitbook/docs/all-roads-to-memory/dma/inspecting-dlmalloc-using-gdb/":{"data":{"":"It’s time to see dlmalloc in action.\nUse a VM to setup dlmalloc\nGnome Boxes is best for Linux\nLearn GDB"},"title":"Inspecting dlmalloc Using GDB"},"/gitbook/docs/all-roads-to-memory/dma/lets-get-chunky/":{"data":{"":"15 September 2025\nIn brief, the whole story of dlmalloc revolves around chunks.","clarity#Clarity":"DMA = “dynamic memory allocation”\nprocess = “an executing binary”\n“Chunkification” = not an actual term but I like to use it.","conclusion#Conclusion":"Use memory responsibly.\nWe saw memory allocation on surface and tried to picture it so that we can feel confident. And I am sure we are.\nBut how that allocation is actually managed by dlmalloc is still unknown. And to answer that, we have to get more chunky.\nBasically, now we have to explore the real structure of a in-use and free chunks, as perceived by dlmalloc and how the concept of bins is applied to manage chunks efficiently.\nUntil then, bye bye.","example-1#Example 1":"Suppose the process requests dynamic memory like this:\np = malloc(20); q = malloc(10); r = malloc(15); s = malloc(18); t = malloc(32); The first request is for 20 bytes and the allocator asks the kernel to release memory and it gets a total of 100 bytes. The arena is established now.\nThe allocator carves a chunk of 20 bytes and returns a pointer to it to the process. The first in-use chunk came into existence.\nFollowing the remaining four requests, a total of 95 bytes is allocated, which is shared by 5 in-use chunks and 5 bytes of unallocated/free memory.\nThis is how it will look in our heap art:\n┌────┐────┐────┐────┐────┐────┐────┐────┐────┐────┐ | p. | | | | | | | | | | └────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘ 1 2 3 4 5 6 7 8 9 10 ┌────┐────┐────┐────┐────┐────┐────┐────┐────┐────┐ | | | | | | | | | | .p | └────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘ 11 12 13 14 15 16 17 18 19 20 ┌────┐────┐────┐────┐────┐────┐────┐────┐────┐────┐ | q. | | | | | | | | | .q | └────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘ 21 22 23 24 25 26 27 28 29 30 ┌────┐────┐────┐────┐────┐────┐────┐────┐────┐────┐ | r. | | | | | | | | | | └────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘ 31 32 33 34 35 36 37 38 39 40 ┌────┐────┐────┐────┐────┐────┐────┐────┐────┐────┐ | | | | | .r | s. | | | | | └────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘ 41 42 43 44 45 46 47 48 49 50 ┌────┐────┐────┐────┐────┐────┐────┐────┐────┐────┐ | | | | | | | | | | | └────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘ 51 52 53 54 55 56 57 58 59 60 ┌────┐────┐────┐────┐────┐────┐────┐────┐────┐────┐ | | | .s | t. | | | | | | | └────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘ 61 62 63 64 65 66 67 68 69 70 ┌────┐────┐────┐────┐────┐────┐────┐────┐────┐────┐ | | | | | | | | | | | └────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘ 71 72 73 74 75 76 77 78 79 80 ┌────┐────┐────┐────┐────┐────┐────┐────┐────┐────┐ | | | | | | | | | | | └────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘ 81 82 83 84 85 86 87 88 89 90 ┌────┐────┐────┐────┐────┐────┐────┐────┐────┐────┐ | | | | | .t | // | // | // | // | // | └────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘ 91 92 93 94 95 96 97 98 99 100 Unallocated and free bytes are represented using // and all the blocks between p. to .p belong to one chunk which is pointed by p.\nNow the process is coming to an end and we are freeing every allocation. free(20); free(10); free(15); free(28); free(32); and the process exited. In this scenario, there were no free chunks and honestly, there was no need as well.\nThis example was pretty straightforward and had no space for the real chaos. So, let’s take another example which is slightly more real.","example-2#Example 2":"This time we will free memory in-between.\np = malloc(10); q = malloc(20); free(10); r = malloc(30); free(20); s = malloc(32); t = malloc(26); u = malloc(12); The total allocation size is 118 bytes but we will not need anything beyond 100 bytes as memory is being freed in the middle.\n┌────┐────┐────┐────┐────┐────┐────┐────┐────┐────┐ | p. | | | | | | | | | .p | └────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘ 1 2 3 4 5 6 7 8 9 10 ┌────┐────┐────┐────┐────┐────┐────┐────┐────┐────┐ | q. | | | | | | | | | | └────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘ 11 12 13 14 15 16 17 18 19 20 ┌────┐────┐────┐────┐────┐────┐────┐────┐────┐────┐ | | | | | | | | | | .q | └────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘ 21 22 23 24 25 26 27 28 29 30 . . Now we have to stop as there is a free instruction. On free(p) , the state of memory becomes this:\n┌────┐────┐────┐────┐────┐────┐────┐────┐────┐────┐ | // | // | // | // | // | // | // | // | // | // | └────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘ 1 2 3 4 5 6 7 8 9 10 ┌────┐────┐────┐────┐────┐────┐────┐────┐────┐────┐ | q. | | | | | | | | | | └────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘ 11 12 13 14 15 16 17 18 19 20 ┌────┐────┐────┐────┐────┐────┐────┐────┐────┐────┐ | | | | | | | | | | .q | └────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘ 21 22 23 24 25 26 27 28 29 30 . . And that’s how the first free chunk comes into existence. This free chunk is sized 10 bytes. Next comes r.\n┌────┐────┐────┐────┐────┐────┐────┐────┐────┐────┐ | // | // | // | // | // | // | // | // | // | // | └────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘ 1 2 3 4 5 6 7 8 9 10 ┌────┐────┐────┐────┐────┐────┐────┐────┐────┐────┐ | q. | | | | | | | | | | └────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘ 11 12 13 14 15 16 17 18 19 20 ┌────┐────┐────┐────┐────┐────┐────┐────┐────┐────┐ | | | | | | | | | | .q | └────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘ 21 22 23 24 25 26 27 28 29 30 ┌────┐────┐────┐────┐────┐────┐────┐────┐────┐────┐ | r. | | | | | | | | | | └────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘ 31 32 33 34 35 36 37 38 39 40 ┌────┐────┐────┐────┐────┐────┐────┐────┐────┐────┐ | | | | | | | | | | | └────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘ 41 42 43 44 45 46 47 48 49 50 ┌────┐────┐────┐────┐────┐────┐────┐────┐────┐────┐ | | | | | | | | | | .r | └────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘ 51 52 53 54 55 56 57 58 59 60 . . After free(q) , the state of memory becomes this:\n┌────┐────┐────┐────┐────┐────┐────┐────┐────┐────┐ | // | // | // | // | // | // | // | // | // | // | └────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘ 1 2 3 4 5 6 7 8 9 10 ┌────┐────┐────┐────┐────┐────┐────┐────┐────┐────┐ | // | // | // | // | // | // | // | // | // | // | └────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘ 11 12 13 14 15 16 17 18 19 20 ┌────┐────┐────┐────┐────┐────┐────┐────┐────┐────┐ | // | // | // | // | // | // | // | // | // | // | └────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘ 21 22 23 24 25 26 27 28 29 30 ┌────┐────┐────┐────┐────┐────┐────┐────┐────┐────┐ | r. | | | | | | | | | | └────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘ 31 32 33 34 35 36 37 38 39 40 ┌────┐────┐────┐────┐────┐────┐────┐────┐────┐────┐ | | | | | | | | | | | └────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘ 41 42 43 44 45 46 47 48 49 50 ┌────┐────┐────┐────┐────┐────┐────┐────┐────┐────┐ | | | | | | | | | | .r | └────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘ 51 52 53 54 55 56 57 58 59 60 . . We already had a free chunk of 10 bytes and now we have another 20 bytes. Is there any point in keeping these chunks different? Can we bring them together, like collapse them into one?\nThe rule is simple, allocated chunks can be adjacent to each other, but a free chunk is always surrounded by in-use chunks. So, when an in-use chunk adjacent to a free chunk is freed, the 2 free chunks are coalesced to form one single chunk. Now we have one free chunk of size 30 bytes and one in-use chunk of 30 bytes. Next comes s.\nWe already had a free chunk but that free chunk is sized 30 bytes and we need 32. So we have to allocate after r. ┌────┐────┐────┐────┐────┐────┐────┐────┐────┐────┐ | // | // | // | // | // | // | // | // | // | // | └────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘ 1 2 3 4 5 6 7 8 9 10 ┌────┐────┐────┐────┐────┐────┐────┐────┐────┐────┐ | // | // | // | // | // | // | // | // | // | // | └────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘ 11 12 13 14 15 16 17 18 19 20 ┌────┐────┐────┐────┐────┐────┐────┐────┐────┐────┐ | // | // | // | // | // | // | // | // | // | // | └────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘ 21 22 23 24 25 26 27 28 29 30 ┌────┐────┐────┐────┐────┐────┐────┐────┐────┐────┐ | r. | | | | | | | | | | └────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘ 31 32 33 34 35 36 37 38 39 40 ┌────┐────┐────┐────┐────┐────┐────┐────┐────┐────┐ | | | | | | | | | | | └────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘ 41 42 43 44 45 46 47 48 49 50 ┌────┐────┐────┐────┐────┐────┐────┐────┐────┐────┐ | | | | | | | | | | .r | └────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘ 51 52 53 54 55 56 57 58 59 60 ┌────┐────┐────┐────┐────┐────┐────┐────┐────┐────┐ | s. | | | | | | | | | | └────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘ 61 62 63 64 65 66 67 68 69 70 ┌────┐────┐────┐────┐────┐────┐────┐────┐────┐────┐ | | | | | | | | | | | └────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘ 71 72 73 74 75 76 77 78 79 80 ┌────┐────┐────┐────┐────┐────┐────┐────┐────┐────┐ | | | | | | | | | | | └────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘ 81 82 83 84 85 86 87 88 89 90 ┌────┐────┐────┐────┐────┐────┐────┐────┐────┐────┐ | | .s | // | // | // | // | // | // | // | // | └────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘ 91 92 93 94 95 96 97 98 99 100 Next we have malloc(26). The memory after s is insufficient so we have to request the kernel to allocated more memory. But there is a free chunk of size 30 bytes, we can use that chunk. But that chunk is more than what we need so we carve 26 bytes out of it for t and leave the rest as a free chunk. And the final state of our arena would be:\n┌────┐────┐────┐────┐────┐────┐────┐────┐────┐────┐ | t. | | | | | | | | | | └────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘ 1 2 3 4 5 6 7 8 9 10 ┌────┐────┐────┐────┐────┐────┐────┐────┐────┐────┐ | | | | | | | | | | | └────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘ 11 12 13 14 15 16 17 18 19 20 ┌────┐────┐────┐────┐────┐────┐────┐────┐────┐────┐ | | | | | | .t | // | // | // | // | └────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘ 21 22 23 24 25 26 27 28 29 30 ┌────┐────┐────┐────┐────┐────┐────┐────┐────┐────┐ | r. | | | | | | | | | | └────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘ 31 32 33 34 35 36 37 38 39 40 ┌────┐────┐────┐────┐────┐────┐────┐────┐────┐────┐ | | | | | | | | | | | └────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘ 41 42 43 44 45 46 47 48 49 50 ┌────┐────┐────┐────┐────┐────┐────┐────┐────┐────┐ | | | | | | | | | | .r | └────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘ 51 52 53 54 55 56 57 58 59 60 ┌────┐────┐────┐────┐────┐────┐────┐────┐────┐────┐ | s. | | | | | | | | | | └────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘ 61 62 63 64 65 66 67 68 69 70 ┌────┐────┐────┐────┐────┐────┐────┐────┐────┐────┐ | | | | | | | | | | | └────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘ 71 72 73 74 75 76 77 78 79 80 ┌────┐────┐────┐────┐────┐────┐────┐────┐────┐────┐ | | | | | | | | | | | └────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘ 81 82 83 84 85 86 87 88 89 90 ┌────┐────┐────┐────┐────┐────┐────┐────┐────┐────┐ | | .s | // | // | // | // | // | // | // | // | └────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘ 91 92 93 94 95 96 97 98 99 100 We have 3 in-use chunks, 1 free chunk and 1 unallocated memory (like free chunk only).\nAt last, we have to allocate 12 bytes for u.\nIf you notice, the total size of free/unallocated memory is 12 bytes, precisely what we need, but it is “fragmented”. This is what external fragmentation looks like. It wastes memory.","external-fragmentation#External Fragmentation":"This is a 15X16 grid, so 240 byte-addressable blocks in heap.\n┌────┐────┐────┐────┐────┐────┐────┐────┐────┐────┐────┐────┐────┐────┐────┐────┐ | .. | .. | .. | .. | .. | .. | .. | .. | | | | | .. | .. | .. | .. | └────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘ | .. | .. | .. | .. | | | | | | | | | | | | | └────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘ | | | | | | | .. | .. | .. | .. | .. | .. | .. | .. | | | └────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘ | | .. | .. | | | | | .. | .. | .. | .. | | | | | | └────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘ | | | | | | | .. | .. | .. | .. | .. | .. | .. | .. | .. | .. | └────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘ | | .. | .. | .. | .. | .. | .. | .. | .. | .. | .. | .. | .. | .. | .. | .. | └────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘ | | .. | .. | .. | .. | .. | .. | .. | .. | .. | .. | .. | .. | .. | .. | .. | └────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘ | | .. | .. | .. | .. | .. | .. | .. | .. | .. | .. | .. | .. | .. | .. | | └────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘ | | | | | .. | .. | .. | .. | .. | .. | | | | | | | └────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘ | | .. | .. | .. | .. | .. | .. | | | | | .. | .. | .. | .. | .. | └────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘ | | | | | | | .. | .. | .. | .. | .. | .. | | | | | └────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘ | | | | | | .. | .. | .. | .. | .. | .. | .. | | | | | └────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘ | | | | | | .. | .. | .. | .. | .. | .. | | | | | | └────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘ | | .. | .. | .. | .. | .. | .. | | | | | | | | | | └────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘ | .. | .. | .. | .. | .. | .. | | | | | | | | | | | └────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘ The dotted blocks are reserved while the rest are free. Roughly 108 bytes are lying freely. This is almost half of the total available bytes, 45%. Yet if I want 50 bytes contiguously, that’s not possible.\nNow scale it to 8 GiB of RAM or more. This is how memory is wasted\nThis is external fragmentation in miniature.\nAnd there are mechanisms to deal with it and this is what we are going to explore next.","how-chunks-are-formed#How chunks are formed?":"When a process start to exist, it rarely requires DMA.\nWhen malloc is called for the first time, the idea of DMA starts to exist for that process.\nWe know that a pointer in heap is returned for each malloc request. The kernel releases memory in pages, the allocator processes them as chunks and we get our requested bytes.\nThe smallest page is 1 page, assuming 4 KiB of size, the allocator would receive at least 4 KiB of memory from the kernel at minimum, which is 4096 bytes. This 4096 bytes is our arena, the total unallocated pool of memory, that the allocator is now going to manage. Note: 4096 bytes is a lot of memory so the allocator doesn’t need to request the kernel every time there is a malloc request. Only when the arena is not sufficient to support allocation is when the allocator reaches the kernel. Let’s take an example and visualize the theory.","the-playground#The Playground":"This is how our playground looks like.\n┌────┐────┐────┐────┐────┐────┐────┐────┐────┐────┐ | | | | | | | | | | | └────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘ 1 2 3 4 5 6 7 8 9 10 ┌────┐────┐────┐────┐────┐────┐────┐────┐────┐────┐ | | | | | | | | | | | └────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘ 11 12 13 14 15 16 17 18 19 20 ┌────┐────┐────┐────┐────┐────┐────┐────┐────┐────┐ | | | | | | | | | | | └────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘ 21 22 23 24 25 26 27 28 29 30 ┌────┐────┐────┐────┐────┐────┐────┐────┐────┐────┐ | | | | | | | | | | | └────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘ 31 32 33 34 35 36 37 38 39 40 ┌────┐────┐────┐────┐────┐────┐────┐────┐────┐────┐ | | | | | | | | | | | └────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘ 41 42 43 44 45 46 47 48 49 50 ┌────┐────┐────┐────┐────┐────┐────┐────┐────┐────┐ | | | | | | | | | | | └────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘ 51 52 53 54 55 56 57 58 59 60 ┌────┐────┐────┐────┐────┐────┐────┐────┐────┐────┐ | | | | | | | | | | | └────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘ 61 62 63 64 65 66 67 68 69 70 ┌────┐────┐────┐────┐────┐────┐────┐────┐────┐────┐ | | | | | | | | | | | └────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘ 71 72 73 74 75 76 77 78 79 80 ┌────┐────┐────┐────┐────┐────┐────┐────┐────┐────┐ | | | | | | | | | | | └────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘ 81 82 83 84 85 86 87 88 89 90 ┌────┐────┐────┐────┐────┐────┐────┐────┐────┐────┐ | | | | | | | | | | | └────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘ 91 92 93 94 95 96 97 98 99 100 I have used box-drawing characters to create this playground. You can check them out on Wikipedia.","visualizing-chunkification#Visualizing Chunkification":"Note 1: We are going to use ASCII Art to represent blocks in heap and 4096 bytes means 4096 blocks, which is too much and maybe an overkill. Therefore, for this experimentation, we are going to assume that the least the kernel can offer is “100 bytes”, not 4096 bytes. This way, we can control the influx of information without losing context.\nNote 2: This is just a simplified version of something that only exist in theory and there is no simple way to visualize it. The actual ground-level reality might differ because of “various rules”. But the idea always remains like this. “We are not studying the wrong way, we are studying the way it becomes easy to comprehend and sets up a foundation which can handle chaos later, much better.”"},"title":"Lets Get Chunky"},"/gitbook/docs/all-roads-to-memory/dma/structure-of-a-chunk/":{"data":{"":"16 September 2025\nWe’ll start by understanding the structure of a chunk. This is how a chunk looks like:\nstruct malloc_chunk { size_t prev_foot; size_t head; struct malloc_chunk* fd; struct malloc_chunk* bk; }; A chunk is just a piece of metadata.","conclusion#Conclusion":"size_t is the real MVP as it helps in making your code platform independent.\nEverything is confusing until you don’t understand it. Chunks is one of those things.\nNext we have to understand binning and how chunks are managed. Questions like:\nHow a free chunk is associated to a bin? What about coalesced free chunks? can be answered only when we understand binning. And that’s going to be our next exploration.\nUntil then, goodbye.","fd-and-bd#*fd and *bd":"These are only used by free chunks. They help us traverse forward and backward in the bin they are associated with.","final-looks#Final Looks":"","free-chunk#Free Chunk":"struct malloc_chunk { size_t prev_foot = \"DEPENDS ON PINUSE BIT\"; size_t head = \"8/16 + REQUESTED_BYTES + DWORD_PADDING\"; CINUSE=0; PINUSE=\"DEPENDS\"; struct malloc_chunk* fd = \"NEXT FREE CHUNK IN THE BIN\"; struct malloc_chunk* bk = \"PREVIOUS FREE CHUNK IN THE BIN\"; }; The allocator world considers this 8/16 math in head as “overhead”. But I don’t like to complicate that much so I didn’t use that.","in-use-chunk#In-use Chunk":"struct malloc_chunk { size_t prev_foot = \"DEPENDS ON PINUSE BIT\"; size_t head = \"8/16 + REQUESTED_BYTES + DWORD_PADDING\"; CINUSE=1; PINUSE=\"DEPENDS\"; struct malloc_chunk* fd = GARBAGE; struct malloc_chunk* bk = GARBAGE; };","size_t-head#size_t head":"Let’s talk about this declaration within the struct first because this probably the only declaration which is present both in free chunks and in-use chunks.\nAs head is of type size_t it will be 4 bytes on 32-bit and 8 bytes on 64-bit. So, size is sorted, I guess.\nWe need to do 2 things.\nWe know that a free chunk must be surrounded by in-use chunks only. This is only possible when you coalesce adjacent free chunks. To do that, we need to know whether the adjacent chunk is free or in-use. We also need to identify whether the current chunk is free or in-use. And we do that using flags bits. These are PINUSE and CINUSE bits.\nThe PINUSE bit is for “previous chunk type”.\n0 ⇒ previous chunk is free. 1 ⇒ previous chunk is in-use. The CINUSE bit is for “current chunk type”.\n0 ⇒ current chunk is free. 1 ⇒ current chunk is in-use. There are two ways to store these bits. Either we allocate separate ints for both, which would waste memory, or we use bit masking. And dlmalloc uses bit masking.\nYou may ask, wouldn’t that mess with the original size? No it won’t.\nRemember the stack pointer has to be double word aligned (16 for 64-bits and 8 for 32-bits) because there are SIMD instructions which expects that? A similar story is repeated here as well. The total size of the chunk has to be double-word aligned. The largest primitive data type in any architecture is double, which basically means a double-word type. If the chunk is only word-aligned, any double word request would mess up the whole calculation of the CPU. To keep things consistent and ensure that memory access for every type is managed efficiently, dlmalloc uses double-word aligned chunks. Double-word aligned means 8 bytes on 32-bit and 16-bytes on 64-bit.\nAny number which is a multiple of 8 or 16 is not going to use the lower 3 bits, i.e 0, 1, 2. These bits are always going to be free. You can do the math if unsure. So, why don’t we use them to mask pinuse and cinuse? The third bit is not used by dlmalloc but ptmalloc uses it. To retrieve the cinuse bit:\nsize_field \u0026 0x1 To retrieve the pinuse bit:\nsize_field \u0026 0x2 To retrieve the size, clear the lower 3 bits.\nsize_field \u0026 ~0x7 If something still feels off, remember that rule, memory interpretation is context dependent. The same group of 8 bits can be interpreted as an unsigned int, a signed int, an ASCII character, or maybe an emoji. So, bit masking doesn’t looses the original size. It just utilizes the bits which have become null function under the “alignment rule” situation.","size_t-prev_foot#size_t prev_foot":"Remember the pinuse bit?\nWhen it is 0, prev_foot stores the size of the previous free chunk. When it is 1, prev_foot is not managed.","the-confusion-of-size#The Confusion Of Size":"When we see a chunk as a struct, it’s literal size is always going to 16/32 bytes on 32-bit/64-bit. But when we talk about chunk as an allocation medium, it includes the size of both the chunk (as a metadata keeper) and the raw memory location to which a pointer is returned to the process.\nAnd this size is stored within the head declaration in the chunk itself.\nYou might be wondering where is the actual memory location. And it’s right to feel perplexed about it.\nWhat happens is that for every allocation, first comes a metadata chunk, followed by the actual memory location to which a pointer is returned to the process.\nSo for every allocation, it is more like:\n┌────┐────┐────┐────┐────┐────┐────┐────┐────┐────┐────┐────┐────┐────┐────┐────┐ | pf | he | fd | bk | p. | | | | | | .p | // | // | // | // | // | └────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘────┘ Remember, pointer p is not a part of the struct. And this is very loosely represented which we will try to manage with more realism later.\nThat means, every malloc request creates a chunk, where a chunk literally is just a metadata struct followed by the actual raw memory, but logically we consider them together.","what-is-size_t#What is size_t?":"size_t is an unsigned integer type defined by the C standard, which is guaranteed to be able to hold the size (in bytes) of the largest possible object (word size) in the architecture we are in.\nAt the end of the day, size_t is just a type definition alias for some unsigned integer type. In that case, why don’t we just use that instead? What’s the need for size_t ?\nEvery major kernel like Windows, Unix and Linux has a different ABI. Then there are platform specific ABIs. What looks easy outside is not really that simple inside.\nLeave the topic of ABIs for a second. We have multiple implementations of integer itself. Just open stdint.h . But when you malloc for any kind, it just works.\nDespite all these differences which we can’t comprehend as beginners, we still use the same malloc() on Windows, Linux and anywhere else. How is that made possible?\nWe use the same frontend but beneath that lies the complexity to keep malloc as one single frontend instead of\nmallocWin32(); mallocWin64() mallocUnix32(); mallocUnix64() and so on. So, we hide all that complication programmatically. The toolchain decides the most appropriate value and make it size_t so that we as programmers have no difficulty working across different systems.\nThat’s why when we are at 32-bit system, the size of a chunk becomes 16 bytes. And when we are at 64-bit system, it becomes 32 bytes automatically without any extra lines."},"title":"Structure Of A Chunk"},"/gitbook/docs/all-roads-to-memory/virtual-memory/":{"data":{"":"August 11, 2025","memory-size-unit#Memory Size Unit":"KB can be ambiguous as it may represent both the binary and the decimal representations, which vary greatly in terms of value.\n1 KB is 1000 bytes while 1 KiB is 1024 bytes. We will use the i ones because they are more relevant here.","page#Page":"A page is the smallest fixed-size chunk of memory that the CPU and the OS manages together.\nIn most modern x86-64 Linux systems: 1 page = 4 KiB (4096 bytes). There exist huge pages (2 MiB, 1 GiB) as well, but 4 KiB is the baseline.\nPage size is a hardware choice, not just an OS thing.\nEvery virtual address is a part of some page.\nPaging solves three problems:\nMemory isolation → each process has its own mapping from virtual pages to physical frames. Flexible allocation → you can give a process scattered physical memory but make it look contiguous. Protection → per-page permissions: read, write, execute. A Page is a portion of memory in the virtual address space. A Page frame is the portion of memory in the physical space that maps to a page in virtual space.\nBasically, page is a virtual memory term and page frame is a physical memory term. Page table is a data structure that the MMU uses to translate an address in virtual space to its corresponding address in the physical memory space.\nA page table entry (PTE) is a mapping record from a page to frame with permissions.\nMMU is a piece of hardware that uses page tables to translate addresses.\nA page fault occurs when the CPU tries to access a virtual address, but the page table entry for that address says:\nthe page is not present in RAM, or the access violates permissions. The CPU stops the instruction and hands control to the OS so it can handle the situation.","physical-memory#Physical Memory":"Byte-addressable: each address refers to 1-byte. Flat address space: goes from 0 to total_bytes - 1. Linux abstracts physical memory into page frames. Physical addresses aren’t directly visible (and accessible). The visible addresses are in virtual memory. Virtual addresses are mapped into physical address space by the Memory Management Unit (MMU).","premise#Premise":"Virtual memory is quite extensive, so diving in directly is foolish. Therefore, here is a tasting of each concept.","virtual-address-space#Virtual Address Space":"Address space is the set of all addresses available for a program to use.\nMMU with the OS manages to translate these addresses to physical memory.\nVirtual address space is split into user space and kernel space.\nUser space is where the program runs. Kernel space is where the OS runs.","virtual-memory#Virtual Memory":"Every process is allotted a virtual address space, which gives a fake sense of owning all the memory. Each process gets the same address layout, which gives predictability. The addresses generally visible are from VAS. MMU with the OS manages to translate these addresses to physical memory mappings. Processes can’t directly see each other’s memory. Virtual memory is divided into pages of 4 KiB (other configs also available, but 4 KiB is the most widely used). Each page in virtual memory maps to a page in physical memory."},"title":"Virtual Memory"},"/gitbook/docs/all-roads-to-memory/virtual-memory/address-translation/":{"data":{"":"This is the exact moment we were preparing for.\nAugust 12, 2025","address-translation#Address Translation":"This process can be divided into an if-else chart.\nWhen MMU receives a virtual address, first it checks the translation lookaside buffer.\nIf it finds an entry which maps the input with a physical address, it is considered TLB Hit and translation is done instantly. If no mapping is found, it is a TLB Miss. The CPU has to do a page walk to find the mapping and possibly cache it. Now the thing we were waiting for, page walk.","page-walk#Page Walk":"There is a special purpose register called CR3. This register keeps the base address of PML4 table.\nThe OS loads CR3 with the physical address of the PML4 for that process. When the MMU needs to translate a virtual address, it reads CR3 and gets the PML4 base physical address.\nSide by side, the virtual address is parsed to obtain the first 9-bits (from MSB) which represent the PML4 entry this virtual address belongs to.\nThe value obtained from these 9-bits is added to the base address of PML4 and the desired entry within the PML4 table is found. The desired entry is pointer to the PDPT table. Now we are at the base address of PDPT table.\nThe next 9-bits of the virtual address are parsed to obtain the right index/offset in the PDPT table. This value is added to the base address and we are at the correct entry now. This entry is a pointer to PDIT table. Now we are at the PDIT table.\nThe next 9-bits of the virtual address are parsed to obtain the right index/offset in the PDIT table. The value is added to the base address and we are at the correct entry now. This entry is a pointer to the plain page table this address belongs to. Now we are at the plain page table.\nThe next 9-bits of the virtual address are parsed to obtain the right index/offset in the page table. The value is added to the base address and we are at the correct entry now. This entry is the address to the actual physical frame this address is mapped to. The PTE entry is parsed and the bits 51 to 12 are extracted, which forms the page frame number.\nThe page frame number is the base address of the 4 KiB physical frame. Next, the last 12-bits of the virtual address are parsed to obtain the page offset.\nThe page offset is the actual byte in the group of 4096 bytes (4 KiB frame) that the virtual address is mapped to. The PFN and page offset are added to form the final physical address.\nAnd the page walk is done.\nAfter a successful page walk, the translation lookaside buffer is updated to contain this mapping for faster lookup.\nWhat if the page walk failed?\nA page fault;","translation-lookaside-buffer-tlb#Translation Lookaside Buffer (TLB)":"Translation Lookaside Buffer (TLB) is a specialized cache used by the CPU to speed up the process of translating virtual memory addresses to physical memory addresses.\nIt stores recently used address translations, allowing for quicker access if the same translation is needed again. Basically, it’s a cache for page table entries.\nIts primary function is to reduce the time it takes to access memory. A full 4-level page table walk would be very slow and come costly for every memory access.\nFor the time being, there is no need to dive further into it.\nLets understand address translation now.","what-is-mmu#What Is MMU?":"MMU stands for memory management unit, which is a hardware component in the CPU, that sits between.\nThe core executing instructions (which always works in virtual addresses), and The memory bus (which only understands physical addresses). Its entire reason to exist is to intercept every memory access and translate it from a virtual address to a physical address in real time, applying protection checks along the way.\nSo, MMU exists for one purpose, address translation.\nBefore we hop on address translation, we have to understand translation lookaside buffer."},"title":"Address Translation"},"/gitbook/docs/all-roads-to-memory/virtual-memory/paging/":{"data":{"":"August 11 and 12, 2025","4-level-paging#4-Level Paging":"In simple words, there are 4 levels and each level has a version of page table. These are structured in a way that as you move through these tables, you are reducing the sample space by multiple folds. Lets zero in and find out how it actually happens.\nWe will move from low to high because that is more logical and comprehensible.\nPlain page table. Page directory index table. Page directory pointer table. Page map level 4 table.","a-note#A Note":"Before you start to wonder how these 4 page tables work together and stuff, I want to say, sit tight and enjoy the journey. Right now we don’t know virtual addresses in-depth, which are essential to understand how all of this fits in.\nOnce we understand the virtual addressing system, we can hop on a process called page walk, which would piece everything we have understood so far together.\nAnd the next thing we are going to study is exactly that.","but-what-really-exist-in-these-tables#But what really exist in these tables?":"What we just had was a conceptual view of these page tables. But what do these page tables actually contain? That is important to understand, especially for the plain page table. Otherwise, it would lead to mental ruckus.\nIn simple words, all of them are pointer tables. And each address (pointer) is 64-bit, obviously.\nThe PML4 table contains 512 pointers to page directory pointer tables. The page directory pointer table contains 512 pointers to page directory index/offset tables. The page directory index table contains 512 pointers to plain page tables. A plain page table contains 512 pointers to physical page frames. The only important thing to understand here is the entries in the plain page tables.\nA page table is a gateway to physical page frames. A page table entry looks like this:\n*--------* *----------* *-------------* *-------------------* *---------------------* | NX-bit | | CPU bits | | OS-Reserved | | Page Frame Number | Flags \u0026\u0026 Control Bits | *--------* *----------* *-------------* *-------------------* *---------------------* 63 62 59 58 52 51 12 11 0 If you are deep into the trenches, and constantly thinking, you will spot that something is missing. Let me tell you that something. How everything fits in the bigger picture?","page-directory-index-table-pdit#Page Directory Index Table (PDIT)":"Each entry in this table is of type page table. Therefore, a page directory index table is a collection of 512 page tables, or, each entry in a page directory index table is a gateway to a page table.\nA single page table manages a total of 2097152 bytes, so, a page directory index table would manage a total of 512 * 2097152 bytes, which is 1073741824 bytes.","page-directory-pointer-table-pdpt#Page Directory Pointer Table (PDPT)":"Each entry in this table is of type page directory. Therefore, a page directory pointer table is a collection of page directories.\nIt has 512 entries, each pointing to a separate page directory.\nA page directory manages 1073741824 bytes. So, a page directory pointer table would manage a total of 512 * 1073741824 bytes, which is 549755813888 bytes.","page-map-level-4-pml4-table#Page Map Level 4 (PML4) Table":"Each entry in this table is of type PDPT.\nA PDPT manages 549755813888 bytes. So, a page map level 4 table would manage a total of 512 * 549755813888 bytes, which is 281474976710656 bytes.\nSo these bytes look daunting. Lets simplify them.\n1 KiB = 1024 bytes\n1 MiB = 1024 KiB = 1024 * 1024 bytes = 1048576 bytes\n2 MiB = (2 * 1048576) bytes = 2097152 bytes\n1 GiB = 1024 MiB = (1024*1024) KiB = (1024*1024*1024) bytes = 1073741824 bytes.\n549755813888/1073741824 = 512 GiB\n1 TiB = 1024 GiB = (1024*1024) MiB = (1024*1024*1024) KiB = (1024*1024*1024*1024) bytes = 1099511627776\n281474976710656/1099511627776 = 256 TiB\nTherefore:\nPage Tables Bytes Managed Simplified Size Plain Page Table 2097152 bytes 2 MiB Page Directory Table 1073741824 bytes 1 GiB Page Directory Index Table 549755813888 bytes 512 GiB Page Map Level 4 Table 281474976710656 bytes 256 TiB That’s enough for paging. Lets talk about virtual addresses.","page-table#Page Table":"A page table is a data structure that manages pages. That’s it.\nThese page tables are then used by the memory management unit to translate virtual addresses to physical addresses.\nModern systems are based on 64-bit architecture, which have 64-bit wide addressable length.\nVirtual Addresses are large and if a single flat page table was used, it would be huge. Managing such a page table would be a nightmare. To solve this problem, a hierarchical approach was implemented, which ensures that each access reduces the sample space of possibilities.\nThe closest example that explains this problem is linear search. If there is a sorted array of 1,000 elements, and our element lies at index 762, we have to traverse 763 entries before we find our match. But binary search reduces this exponentially. In just 9 iterations, we will find our match. Roughly 1% of linear search. That’s the same level of reduction we are talking about through paging. If you are wondering how binary search works, you can google it. In simple words, we divide the sample space in half and take the value at mid index. If the value at mid index is lesser than the target value, we have to search the upper half, otherwise, the lower half. We do this until the value at mid index becomes the target. This hierarchical approach is what we call as 4-level paging.","plain-page-table#Plain Page Table":"A page is a collection of individual bytes. At basic, we are dealing with 4 KiB pages. So, a page is a collection of 4096 addressable bytes in virtual memory.\nAs named, a page table should be a collection of pages, right? Not really.\nPage is just a conceptual term. It exist in theory only. What we really deal with is page frame in the physical memory. And, a page table is a collection of page frames. How many page frames, to be exact? The number is 512.\nThis number is obtained by dividing the page size by size of each entry, where size of each entry is given by the register width, which is 8 bytes on x64. So, we get, 4096/8, giving us 512 entries. And this mathematics is applicable to rest of the tables as well.\nTherefore, a plain page table is a collection of 512 page frames, or, each entry in a page table is a gateway to a page frame in the physical memory.\nA page is sized 4096 bytes, so, a plain page table manages a total of 512 * 4096 bytes, which is 2097152 bytes.","why-pages#Why Pages?":"Memory is byte-addressable. In 2025, most laptops comes with 8 GiB RAM at least. How many bytes does 8 GiB have?\n1 GiB = 1024 MiB 1 MiB = 1024 KiB 1 KiB = 1024 bytes Therefore, 1 GiB = 1024 * 1024 * 1024 bytes = 1073741824 bytes. And, 8 GiB would be 8 * 1073741824 = 8589934592 bytes or, ~8.6 billion bytes. If we have to keep track of every byte in a flat table, it would have ~8.6 billion entries. And this is for 8 GiB RAM stick. The number would become crazy for higher valued RAMs.\nSo, the solution? Group these bytes. And the group of these bytes is came to known as a page.\nGenerally, a page is sized 4 KiB in modern Linux systems, though Linux supports huge pages as well. But we need not to worry about them yet. Plus, if you ask why a page is 4 KiB only? Its historical and technical as well. But we can avoid that for now.\n4 KiB means 4 * 1024 bytes or 4096 bytes. That means, a page is a gateway to 4096 unique byte-addressable locations.\nRemember, the concept of pages exist in virtual memory only. The equivalent concept in physical memory is page frame, which we will talk about later."},"title":"Paging"},"/gitbook/docs/all-roads-to-memory/virtual-memory/virtual-addressing-system/":{"data":{"":"August 11 and 12, 2025","a-virtual-address#A Virtual Address":"Total number of bits we have are 64. But not all the 64-bits are required to manage addresses now. It would be huge to manage 2^64 addresses.\nSo, we stick to 48-bit virtual addresses. The rest of the 16-bits are sign extension of the 47th bit. We’ll explore what that means in a while.\nA virtual address is divided into several pieces. More precisely, the bits in the a virtual address are grouped together to represent different parts of the 4-level paging system that we have discussed previously.\nAt high level, a virtual address is structured like this:\n+--------------+ +--------------+ +--------------+ +------------+ +----------------------+ | PML4: 9-bits | | PDPT: 9-bits | | PDIT: 9-bits | | PT: 9-bits | | Page Offset: 12-bits | +--------------+ +--------------+ +--------------+ +------------+ +----------------------+ 47 39 38 30 29 21 20 12 11 0","address-range-split#Address Range Split":"The virtual address space is split into two halves for user space and kernel space.\n0x0000000000000000 → 0x00007FFFFFFFFFFF (Lower half, ~128 TiB) → User space 0xFFFF800000000000 → 0xFFFFFFFFFFFFFFFF (Upper half, ~128 TiB) → Kernel space The middle region (x00007FFFFFFFFFFF to 0xFFFF800000000000) is unused guard space. Note: The split is logical and exist only in virtual memory, except the hardware enforced rules.","analogy#Analogy":"Consider an office space with employees of different kinds. And there is a room for the boos.\nThe boss’s room is what kernel space is really is. Only privileged access is allowed and rest has to undergo a process to come there.\nThen there is general area which is accessible to everyone as long as they are an employee in the company. This is out user space.\nWhen you need to something that requires permission from the boss, you go through a standard process, which is exactly how the execution context changes from user space to kernel space when required.","hardware-enforced-privilege-levels#Hardware Enforced Privilege Levels":"Rings are hardware-enforced CPU privilege levels, which forms a core part of how modern processors (like x64) separates trusted code (kernel) from untrusted (user) code.\nCPUs implement multiple protection rings numbered 0 to 3, with:\nRing 0 = highest privilege (kernel mode) Ring 3 = lowest privilege (user mode) Rings 1 and 2 exist but are rarely used in mainstream Linux. System calls cause a CPU privilege level switch from Ring 3 → Ring 0.","some-good-to-know-things#Some Good-To-Know Things":"A virtual address is in big endian notation, so most significant bits are in left and least significant bits in the right.\nAs long as we are dealing with pen paper math, there is no need for bit shifts. But with programming, bit shifting becomes important for extracting values the right way and avoiding falling at edge cases.\nEach process gets a virtual address space, which has 256 TiB worth of addressable space, most of which is empty. Yes, the calculation we have done previously is applied individually to every single process.\nThe program never runs out of virtual address spaces. It only runs out of mappings in the physical memory. Note: Addressable space ≠ Usable space.","what-are-user-space-and-kernel-space-really#What are user space and kernel space really?":"In simple words, user space and kernel space are two logical distinctions within the virtual memory layout.\nThis logical distinction is achieved by access control (privileges) and protection rights, which are enforced both at the hardware level (CPU) and the software level (OS).\nUser space is the portion where unprivileged jobs are managed and kernel space is where privileged jobs are managed.\nA definition which is quite popular is that user space is where user mode applications run and kernel space is where the OS Kernel runs or privileged tasks are executed. This doesn’t sound accurate to me for one reason.\nA task is usually made up of multiple atomic jobs. And we can verify this with assembly. An action as simple as printing something to standard output can involve multiple steps. A task itself can’t be tagged as privileged or unprivileged. The atomic jobs that actually do something are the ones that can be actually tagged. For example, running VS Code is a user space action but within that are several thousands of actions, many of which aren’t possible without privileged access. Like writing code, which is an I/O operation inside a file, which is a privileged job. Therefore, it is a little ambiguous to say that “user space runs user mode applications and kernel space runs elevated tasks” or something like that. Saying that “user space executes unprivileged jobs and kerne space executes privileged jobs” is far more accurate in my opinion.","why-like-this#Why like this?":"All the 4 page tables have 512 entries, which require a minimum of 9-bits to represent. So, 9-bits are reserved for them.\nPage offset is the actual byte being addressed within the page. Since there are 4096 bytes in total, 12-bit are required at minimum to represent them.","why-this-distinction-exist#Why this distinction exist?":"There is no limit on what you can execute, which creates problems. There malwares and other program threatening the functioning of the hardware.\nThis distinction ensures that programs can be contained by default. Its a version of deny by default, allow by exception strategy. Anything is considered unsafe before it passes the kernel’s checks.\nAnd any attempt to access privileged area doesn’t get unnoticed. And if it is inappropriate, the system denies it.\nThere is a proper mechanism through which the execution mode switches from user space to kernel space, when required."},"title":"Virtual Addressing System"},"/gitbook/docs/all-roads-to-memory/virtual-memory/virtual-memory-layout/":{"data":{"":"August 12, 2025","big-picture#Big Picture":"High Address Top Of Virtual Address Space 0xFFFFFFFFFFFFFFFF *-----------------------------* End Of Kernel Space ↓ | | | Kernel Space | | | | Size: ~128 TiB | | | | Upper Half | | | 0xFFFF800000000000 *-----------------------------* Start Of Kernel Space ↑ | | | Unused / Guard Space | | | 0x0000800000000000 *-----------------------------* End of User Space ↓ | | | User Space | | | | Size: ~128 TiB | | | | Lower Half | | | 0x0000000000400000 *-----------------------------* Start Of User Space ↑ | | | Reserved / Unmapped | | | 0x0000000000000000 *-----------------------------* Bottom Of Virtual Address Space Low Address","user-space-layout#User Space Layout":"0x0000800000000000 *-----------------------------* End of User Space ↓ | Stack (grows downward) | *-----------------------------* | Memory-Mapped Region | | (shared libs, mmap, ....) | *-----------------------------* | Heap (grows upward) | *-----------------------------* | Static \u0026\u0026 Global Variables | | (.bss / .data) | *-----------------------------* | .text | 0x0000000000400000 *-----------------------------* Start Of User Space ↑ .data and .bss are packed together because they are functionally the same thing, just differ in initialization.","what-are-stack-and-heap-really#What are stack and heap really?":"In simple words, stack and heap are two approaches to manage memory. There are no specialized regions either in the physical memory which refer to stack or heap. They are just two ways to manage the same flat memory.","why-the-stack-grows-downward#Why the stack grows downward?":"When we learn stack as a data structure, we imagine it as a stack of plates. A stack of anything starts from bottom and approaches sky as the top.\nBut, when you learn assembly, you find that stack grows downward. And you keep scratching your head. I was no different.\nThere is a simple solution to this problem. Reverse the address space.\nRight now we are looking from top to bottom or higher addresses to lower addresses. Just flip the structure and you get an upward growing stack. Just remember that this doesn’t change the address management of stack. A push would still reduce the memory address mathematically and a pop would increase it. But, at least it solves the mental overhead of imagining stack growing downwards. Apart from this, there is a genuine question that why stack was put at the top of the user space. The reason that they shouldn’t collide is not applicable as the memory-mapped region will always come in-between.\nAs of now, I don’t have any answer, but if I find anything interesting, I will update this block.","why-the-stack-is-fast-and-heap-is-slow#Why the stack is fast and heap is slow?":"This question is not completely answerable as it is based on comparison.\nWe can explore why stack is fast because we are familiar with it. But we don’t know what heap is.\nAlthough we know why stack is fast because it is based on sequential allocation. But what makes heap slow is not known.\nWhen we will explore dynamic memory allocation, it will become clear why stack is fast and heap is slow.\nUntil then, we are now prepared to understand how all of this fits in the grand scheme of memory management with MMU."},"title":"Virtual Memory Layout"},"/gitbook/docs/debugging/":{"data":{"":"21 September 2025\nThe most popular debugger for us is the GNU Debugger or gdb. There are two way to learn it.\nYou pick a crash course that teaches it ASAP. You learn debugging as a concept and understand how gdb implements it. As you have guessed right, we are going for the second method. And I have two reasons for this decision:\nMy consciousness and learning methodology will not allow me to even open gdb, let alone memorizing the commands. Unless I understand the WH-family behind it and it no longer feels black magic, I can’t do anything. But that’s a personal reason and you can skip that. GDB is just one debugger. There are plenty more. The current landscape is such that you have to be proficient in multiple toolchains, and there are two ways to do it. One is sustainable, the other is not. The first one is memorizing and don’t give a damn about anything, the quickest way. The second way is to understand the underlying machinery thru foundational concepts, which is a transferable skill, which means that no matter which system, which architecture you are in, the rules are gonna be largely the same. The more you can prioritize on transferable skills, the more you are consciously choosing to pay upfront to avoid chaos later."},"title":"Debugging In Depth"},"/gitbook/docs/debugging/introduction/":{"data":{"":"22, 23 September 2025\nAt its core, debugging can be defined as a process of actively observing and controlling a program to understand what it’s doing, why it’s doing it, and how to manipulate it.\nA debugger is like a sentinel being who can observe everything about a process and change it against its will. It’s like S2 Loki (A Marvel Studious TV Series).\nAny debugger primarily does these three things:\nStop execution at a known point (breakpoints, signals, traps) Inspect program state (memory, registers, stack, variables) Modify execution if needed (change variable values, registers, or program counter) To understand debuggers, we have to understand what enables debugging. Otherwise, gdb will feel black magic."},"title":"Introduction To Debugging"},"/gitbook/docs/debugging/memory-is-byte-addressable/":{"data":{"":"26 September 2025\nThe first idea that we are going to verify is:\n“memory is a flat array of byte addressable blocks”.","command-background#Command Background":"GDB allows us to inspect individual memory locations via the examine command.\nWe can open its help page by doing:\n(gdb) help x The examine command has the following syntax:\n(gdb) x/FORMAT ADDRESS FORMAT specifies how we want to inspect memory. ADDRESS refers to the memory location we want to inspect. FORMAT is made up of three arguments.\nThe third argument specifies how much memory we want to inspect.","exploration-begins#Exploration Begins":"","setup#Setup":"Take this program.\n#include int main() { int x = 0x12345678; return 0; } Compile with debug information:\ngcc -g main.c -o binary Open inside gdb.\n$ gdb ./binary (gdb) Clear the window with CTRL+L ."},"title":"Memory Is Byte Addressable"},"/gitbook/docs/debugging/practical-gdb/":{"data":{"":"25 September 2025\nWithout wasting time, let’s jump right into experiments.","setup#Setup":"I started my low level journey on May 01, 2025. Today, it is September 25, 2025. Almost 5 months are about to complete. In these 5 months, I have understood lot of theoretical concepts. I tried to visualize them using ASCII and other mechanisms but I have not seen them practically.\nSo the idea is to make a list of all the concepts I have studied so far, write small programs to implement them, load them inside gdb and examine how that theory is implemented practically, to verify if what I read was right or not.\nThe list below includes everything."},"title":"Practical GDB"},"/gitbook/docs/debugging/the-gnu-debugger/":{"data":{"":"24, 25 September 2025","1-inferior#1. Inferior":"Normally, inferior means lower in rank or quality. Here, inferior refers to the debugee that gdb works on. It can be\nA live process started by gdb or attached later. A core dump. An executable file (only static information). GDB can debug multiple processes in a single session, so there can be multiple inferiors.\nIf you don’t have an inferior, there is nothing for gdb to process.\nIf you run info registers, gdb should give the state of the registers, but where there is no debugee process, how gdb is supposed to give the register state? That is why, an inferior is the first thing we should provide to gdb.\nThe inferior must be stopped to allow meaningful inspection of the execution state.\nIf the source is not compiled with debug symbols, gdb can’t give any semantic information about the source.\nFrom now on, we will refer the debugee process as inferior.","1-memory-image#1. Memory Image":"The process is seen as a set of mapped regions in the virtual address space. It includes everything we have studied in the ELF specification: text, data/bss, heap, stack, and shared libraries (as memory mapped regions).","2-execution-state#2. Execution State":"The execution state is what the CPU is doing at any point in time. It includes the state of registers, the syscall it is preparing for, flags, instruction pointer, frame information etc.\nGDB can walk thru all of that and give us an in-detail, low level overview of what’s happening in the process at any instant.\nGDB can modify the execution state as well to test how arbitrary or even precision-guided changes in the execution state drives the debugee in a different direction.\nThe execution state is often tied to a specific call stack. When you change the call stack, the execution state changes as well.\nWe have already explored process state in detail in the previous article, wpd-2.md","2-frame#2. Frame":"A single function activation in the call stack. Frames are numbered from 0 to n.\nframe 0: currently_executing() frame 1: called the currently_executing frame frame 2: called frame 1 . .","3-breakpoint#3. Breakpoint":"A “stop here” marker.\nWe can set a breakpoint on a source line and on an address.\nWe can set breakpoints directly or based on a condition.\nGDB processes that condition and decides when to set the breakpoint.","3-debug-information#3. Debug Information":"As mentioned in previous articles, debug information is semantic information which aids the debugging process. It’s not necessary if you are great with raw assembly and memory addresses.\nIf the source was compiled with -g option in gcc, we can observe exact source lines, function names, local variables etc.\nThis is how a debugee process is perceived by gdb.\nIf we hurry up and run commands randomly, we are not going to get anything useful because gdb expects us to behave in a certain way so that it can help us to the best of its capabilities.\nTo understand what gdb expects us, we have to map how gdb functions at high level.","4-watchpoint#4. Watchpoint":"A watchpoint observes for a change in the specified memory location and stops when the change happens.\nFor example - If we created a watchpoint on a variable and updated it later in the process, the moment it is updated, the execution would be halted.","5-catchpoint#5. Catchpoint":"A catchpoint observes for an event and when that happens, the execution is halted.\nGDB supports many events like exec, fork, shared libraries (load/unload), signal, syscalls (entry/exit), vfork, throw and rethrow (c++).","6-source-line#6. Source Line":"It refers to a human-friendly view that maps back to machine addresses.\nRemember, it is possible only when the source is compiled with debug information.","7-location#7. Location":"Location is any place you can stop the execution at. A line, an address, or a function.\nBreakpoints/watchpoints are always attached to a location.","8-core-dump#8. Core Dump":"A memory snapshot from a crashed process.\nIt is used extensively in memory analysis.\nNow we can explore the actual commands.","accessing-the-source-code#Accessing The Source Code":"Since GDB is a GNU software, it defaults to AT\u0026T syntax for assembly. But we can tell gdb to use intel syntax as well.\n(gdb) set disassembly-flavor intel","conclusion#Conclusion":"I mentioned this earlier as well that GDB is very extensive. It can do so much. Therefore, understanding how gdb does all of that is quite important because it primes you for future explorations.\nIf you have read my previous write ups, you might remember it, but if you don’t, this is for you.\nNo one is born with knowledge and understanding. You do the work and you build it. That’s the recipe.\nIn 2025, you have a proper GDB manual, spread across 994 pages. You have awesome reference cards. GDB itself comes with a built-in help .Make use of these resources. This is the age of AI, ask AI chatbots what is this?, why it is like this? They will help you.\nThat is how I have understood all of this. It’s not magic, it’s just work. So explore yourself, there is so much out there. The horizon expands as long as you want to see it.","execution-state-information#Execution State Information":"Requirement: An inferior which has been stopped for inspection.\nNote: All the commands here are frame-specific. When you change the current stack frame, the values will change. So remember that and save yourself headaches.","gdb-commands#GDB Commands":"Few things before the exciting part.\nGDB is very extensive. It can do a lot of things, so sticking to our purpose is very important. Otherwise, it’s a recipe for time and energy wastage. We can organize gdb commands based on some high level categories. Under these “high level categories”, we can further divide the commands based on their usability factor. The usability factor is simple. Some commands completely rely on debug information, other don’t. Later we’ll find the importance of this categorization.","gdbs-view-of-the-debugee#GDB\u0026rsquo;s View Of The Debugee":"The debugee process is divided into three parts.","managerial-commands#Managerial Commands":"All the i-suffixed commands operate on machine instruction. Their equivalent with no i in them operates on C source lines.\nTherefore, the i-suffixed ones work even when there are no debug symbols, because they don’t rely on them. This write up is already quite dense so we’ll leave it as is. In the next one, we will explore gdb fully practically, no theory.","mapping-gdb#Mapping GDB":"Mapping gdb is about priming ourselves with gdb’s terminology.","premise#Premise":"There is a lot that can be done with GDB. But using a bunch of commands and memorizing them is not my cup of tea. There are awesome reference cards on the internet, take this one: https://users.ece.utexas.edu/~adnan/gdb-refcard.pdf but memorization is not the point.\nTo really understand gdb, we have to understand how gdb perceives everything, what is gdb’s mental model.","stack-management#Stack Management":"","starting-a-gdb-session#Starting A GDB Session":"There are two ways to start gdb.\nWe provide the source (binary/core) to be debugged while starting gdb. We attach a running process or provide gdb a core or even a binary after the gdb session is started. When we do this:\n$ gdb ./source-binary We get a message along with a gdb prompt.\nGNU gdb ...... . . For help, type \"help\". Type \"apropos word\" to search for commands related to \"word\". (gdb) This means we are in a gdb session. But the source binary has not become a process yet.\nPassing the source binary name ensures that the source binary can be run as a child process for gdb so that there are no problems on the permission side, as discussed in the previous write up wpd-2.md\nWe will learn attaching to a running process later.","stopping-the-debugee#Stopping The Debugee":""},"title":"The GNU Debugger"},"/gitbook/docs/debugging/wpd-1/":{"data":{"":"22, 23 September 2025","breakpoint#Breakpoint":"INT3 is a one-byte (0xCC) instruction inserted by the debugger, which when executed CPU raises a breakpoint exception (#BP). The kernel delivers a SIGTRAP to the process.\nTo insert a breakpoint at an address, the debugger modifies the first byte of that instruction to 0xCC. When the CPU reaches that instruction, a break point exception is triggered.\nTo resume execution, the debugger restores the original byte at that instruction’s address. Since RIP has advanced by one (past the 0xCC), the debugger decrements RIP by 1 so that it points back to the intended instruction.\nIt is used to stop execution at a certain address in the process.","conclusion#Conclusion":"A debugger program sets up either an INT3 instruction to create a breakpoint at an address or it sets up the RFLAGS.TF bit to 1 to stop after each instruction.\nWhen a breakpoint is set on an instruction, an INT3 instruction executes immediately, which raises a #BP exception. The kernel responds to this by sending a SIGTRAP , which the debugger program catches via ptrace. The rest is taken care by ptrace.\nWhen RFLAGS.TF=1 , a #DB exception is raised after every instruction in the process. The kernel responds similarly by sending a SIGTRAP, which the debugger program catches via ptrace. The rest is taken care by ptrace itself.","exceptions#Exceptions":"An exception is a synchronous event that is generated when the processor detects one or more “predefined special conditions” while executing an instruction.\nTake 4/0, that’s going to raise divide by zero exception. This is only going to be raised when the CPU reaches that instruction, this is what synchronous means. The exception is not going to be raised arbitrarily but systematically. Exceptions is how the CPU stops normal execution and transfers control to the OS.\nThere are 3 classes of exceptions: faults, traps, and aborts.\nA synchronous exception detected before the instruction completes is called a fault. Ex: divide by zero and page fault. A synchronous exception after the instruction completes is called a trap. Ex: Breakpoint, overflow and single-step via trap flag (TF). An abort is a catastrophic exception, which typically cannot be handled normally. Usually indicates hardware failure or an unrecoverable condition.","interrupts#Interrupts":"An interrupt is an asynchronous event that is typically triggered by an I/O device. For example:\nCTRL + C to exit an infinite loop or a hanged terminal process. CTRL + D to exit python shell. When an interrupt or an exception is signaled, the processor halts the execution of the program and switches to a handler procedure that has been written specifically to handle the interrupt or exception condition.\nThe processor accesses the handler procedure through an entry in the interrupt descriptor table (IDT). When the handler has completed handling the interrupt or exception, program control is returned to the interrupted program.\nThe IA-32 Architecture defines 18 predefined interrupts and exceptions and 224 user defined interrupts.\nWhen the trap flag in RFLAGS is set, which is the 8th bit, the CPU generates a debug exception (#DB) after every instruction. This is the hardware single-step mode.","premise#Premise":"It looks like debuggers are doing some sort of black magic, which is not the case. To remove that black magic, we have to understand what powers debuggers.\nPrimarily there are 2 things that power a debugging program.\nTrap instructions, CPU flags, signals and exceptions are the basis of this black magic. ptrace syscall is the dedicated API that allows a debugger to act as a sentinel being and observe other processes. It allows the debugger to make use of traps and signals to do the black magic. But there is one more thing, called symbol and debug information.\nThis is not strictly required but it makes debugging intuitive and less mentally draining. Without debug information, you’d just have raw assembly and memory addresses. If you have no problem seeing raw assembly and memory addresses, this part is just add-on for you. But to make the process easier, debug information is a necessary thing. By the way, this is a part of ELF and DWARF specs.","references#References":"Intel 64 and 32 bit Manual","trap-flag#Trap Flag":"Trap flag or RFLAGS.TF is the 8th bit in the CPU flags register.\nWhen this bit is set to 1, the CPU raises a debug exception (#DB) after every instruction in the process. Kernel delivers SIGTRAP to the debugger.\nSingle-step means execute one instruction and then stop. It is achieved by setting RFLAGS.TF=1."},"title":"What Powers Debuggers? — 1"},"/gitbook/docs/debugging/wpd-2/":{"data":{"":"23 September 2025","1-triggering-event#1. Triggering Event":"The debugee is running fine and there is no need for a debugger to intervene. But for a debugger to intervene in a process, it needs to stop the process. To stop the debugee, the debugger needs a triggering event.\nTriggering event is anything that signals the kernel to halt the execution of the debugee process.\nSo far, we have read about breakpoint #BP and single-step #DB. But there is more to it.\nFor example: syscall entry/exit When the kernel detects that event, it suspends the debugee process.","2-notification-to-the-debugger#2. Notification To The Debugger":"When the debugee is suspended, the debugger is notified about the activation of the event that the debugger told the kernel to stop at.\nThe debugger process is notified via waitpid()/waitid() that the debugee has stopped and the reason behind it.","3-transfer-of-control#3. Transfer Of Control":"After notifying the debugger, the debugger process gains the ability to inspect/modify the state of the debugee process via ptrace. This state is often referred to as process/execution context.\nEverything is mediated by the kernel.","4-resuming-the-debugee-process#4. Resuming The Debugee Process":"When the debugger process is done with its work, it can notify the kernel about resuming the debugee process.\nThe debugee is now resumed by the kernel until another event triggers.","conclusion#Conclusion":"This is what that enables debugging.\nNow we are primed to understand how the GNU Debugger works.","event-monitoring#Event Monitoring":"Breakpoints and Debug exceptions are just two events. There are multiple events like fork, execve, syscall entry/exit, which the debugger can monitor.\nThe kernel mediates all reads/writes, so the debugger cannot bypass protections directly.\nThis event monitoring is what tools like strace do.","execution-control#Execution Control":"A debugger needs access to the debugee’s memory to set breakpoints and modify instructions.\nThis temporarily bypasses normal memory protections, but all access is mediated by the kernel via ptrace.","inspecting-the-state-of-registers#Inspecting the state of registers":"General purpose registers can reveal the state of current execution context.\nCalling convention like System V ABI on Linux 64-bit have syscall convention which mandates certain registers to be used in a specific way for cross-compatibility.\nInspecting rax for example can tell us which syscall the debugee is preparing for. Inspecting rsi, rdi can tell us what are the arguments to that syscall. Inspecting rip can inform about the next instruction.\nrsp can be used to inspect local variables and overall stack state.","modifying-registers#Modifying registers":"Flags register is great to inspect the result of various computations like OF ZF SF PF etc.\nWe also have trap flag on 8th-bit which can be modified to raise debug exception #DB after each instruction.","premise#Premise":"We have a process that we want to debug/trace. We call it debugee/tracee process.\nTo debug a process, we have a program called debugger, which itself becomes a process upon running, which is called, debugger/tracer process.\nNote: We are going to stick to the debugger/debugee terminology.\nSince we have to bypass a lot of restrictions to do debugging, all the requests of the debugger process are mediated by the kernel. The arm of the kernel that does all of it is called ptrace.\nptrace is a syscall interface that lets one process (the debugger) observe and control another process (the debugee).","process-context#Process Context":"Process context refers to the state of a process at any given instance. Process context includes multiple things, some important ones are:\nGeneral purpose registers Flags register Segment registers (read-only) Floating point registers (SSE/AVX) Virtual address space (.text/.data/.bss/heap/stack) Signals Process metadata (pid, ppid, uid, gid)","rules-for-debugging#Rules For Debugging":"A process can deny to be debugged.\nOnly the processes with the same user ID (UID) can trace each other. If you try to attach to a process owned by another user, the kernel denies it.\nRoot can trace any process.\nModern Linux restricts tracing even further using the Linux Security Module YAMA: /proc/sys/kernel/yama/ptrace_scope.\n0 → trace anything allowed by UID. 1 → only direct parent can trace. 2 → no tracing allowed (even by parent). This prevents arbitrary processes from attaching to random programs.\nA child is always traceable by its parent. This is why we usually use gdb to execute the program, so that gdb can have the privilege to do debugging.\nA process can voluntarily call ptrace(PTRACE_TRACEME) to allow its parent to debug it. If it doesn’t, a child cannot be traced unless PTRACE_ATTACH is used by a permitted debugger (and the kernel verifies the security policies).\nThese are some of the rules that protects ptrace from abuse.","the-mental-model#The Mental Model":"There are 4 steps in the debugging model.","what-can-a-debugger-do#What can a debugger do?":"Based on the process context, a debugger can inspect, modify, and control the execution flow of the debugee.\nA debugger can do a lot of things, most of which might be beyond the scope of this exploration, but here are the most important ones, which a debugger does almost all the times.","where-do-signals-fit#Where do signals fit?":"Suppose our process accessed an invalid memory location. A page fault occurs as there is no page mapping between the physical and virtual memory. That’s basically a segfault and the kernel has to send the process a SIGSEGV.\nWhen there is no debugger, the kernel directly sends the SIGSEGV signal to the process and the process handles it.\nWhen there is a debugger process waiting for a segfault event, the debugee is suspended when it occurs and the debugger is notified using waitpid()/waitid() that “the debugee has been suspended because this event occurred”.\nSince the debugee is suspended, it can’t receive the signal until the debugger resumes it."},"title":"What Powers Debuggers? — 2"},"/gitbook/docs/elf-parser-project/":{"data":{"":"Hey, what’s up?","why-this-projectx20#Why This Project? ":"After finishing the static analysis of the Hello World binary (with some parts still left), I needed a break. But I don’t want to stop learning.\nWhile I was studying the hello world binary, I felt something was missing: a clear, C-style view of the entire ELF structure. I wanted to see actual structs with spec-defined types, populated with values parsed directly from a real binary.\nI have found a project called pax-utils, from GNU Project, which has a utility named dumpelf. And i decided that I will explore ELF spec myself by extracting raw bytes from an elf file and dump the values in a C-style format.\nI began this project on 24/07/2025 with one goal in mind. I have to build a tool which can parse a Hello World ELF binary on x86_64. That’s it.\nI used readelf as a reference throughout this process as it is too easy to get lost in raw bytes without a guide.\nThe language would be C.\nHere is the project repository, GitHub."},"title":"ELF Parser Project"},"/gitbook/docs/elf-parser-project/header-files/":{"data":{"":"","conclusion#Conclusion":"Its a simple way to reduce redundancy and organize the code. That’s it.","why#Why?":".c files are source files and .h files are header files.\nThe idea is to keep declarations and definitions separate. Why? For a variety of reasons.\nCleanliness and Directness. When something is used by multiple files, it is better to keep it in one file and reference that file instead of duplicating it, reducing chaos and redundancy. Modular code. The files which require those definitions just have to include the header file, avoiding the need to include the complete source file. Compiler efficiency. The most important one. We know that when multiple files are passed to gcc , it generates object files for each source file and the linker program joins them together to form the final binary.\nIf the source file changes in the future, that’s where this system shines.\nIf the other files have included the declaration file only, they are not required to be recompiled. Only the corresponding source file needs recompilation. This leads to compiler efficiency. If the other files have included the source (full definition) itself, every other such file needs to be recompiled before linking. You may ask why including header files doesn’t demand full recompilation but including source files does. And the answer to this question lies in the build process.\ninclude preprocessing directive copies the file as it is, in the file it is called. Header files themselves don’t generate any object code. They are an interface to how we access glibc, the standard C library, which is one-file that becomes libc.so . All these different header files are used to categorize the declarations used for different purposes. And they are expanded into the source file. The linker joins the object files with the shared object library and we are done. Have a look at the /build-steps directory.\nIf you open the elfdump-mappings.i file, along with dump_structure/mappings.c and dump_structure/mappings.h, you will find that it includes both of these files along with the the declarations from the inttypes.h file. Line 296 on wards. And these extended files are then compiled to object codes."},"title":"What are .h files?"},"/gitbook/docs/elf-parser-project/idea-and-structure/":{"data":{"":"","few-things-about-the-project#Few Things About The Project":"It is not perfect. I am not trying to compete with projects like binutils , which hosts readelf or pax-utils , which hosts dumpelf or any other project. It is purely to understand the elf specification not by reading docs but by implementing something myself. It is verbose by design because I am not familiar with things that reduce code but at the cost of readability. I wanted something straightforward. It is great for educational purposes, where beginners can read the code and understand it themselves, where things obscure to them are not used to do the thing.","project-design#Project Design":"The whole project can be divided into two parts.\nExtracting raw bytes from the ELF. Interpreting those raw bytes and creating a c-style dump of it.","structure#Structure":"elf_parser └─ build-steps ... ... └─ core_api └─ parser.c └─ parser.h └─ dump_structure └─ dump.c └─ dump.h └─ mappings.c └─ mappings.h └─ reference └─ hello_elf └─ hello_world.c └─ readelf_output └─ elf_spec.h └─ main.c","timeline#Timeline":"Started on July 24, 2025\nFinished writing core API on July 31, 2025\nFinished writing c-style dumps on August 02, 2025\nStarting documentation on August 03, 2025\nImprovements ahead."},"title":"Idea \u0026 Structure"},"/gitbook/docs/elf-parser-project/magic-verification/":{"data":{"":"","conclusion#Conclusion":"And we have checked if the file passed to our program is an ELF or not.\nNext we will parse the file headers.","does-this-magic-number-hold-any-meaning#Does this magic number hold any meaning?":"Yes. 0x7f = DEL, 0x45 = E, 0x4c = L and 0x46 = F","problem-statement#Problem Statement":"The first step in parsing an ELF is to verify if the file passed in the argument is an ELF or not.\nThis is done by verifying the magic bytes (or numbers) present as the first thing in all kinds of binary files.","reading-the-magic#Reading The Magic":"To mark a file as a valid ELF, the first 4 bytes must be 0x7F 0x45 0x4C 0x46 . We will use fread() to read those bytes.\nIt is provided by the C standard I/O library (stdio.h). The signature of fread() is as follows.\n// General Signature fread(dest_ptr, size_each_element, n_ele, file_ptr); fread requires 4 arguments.\ndest_ptr is where the raw bytes would be stored after extraction. n_elements is the total number of elements we are extracting. size_each_element is self-explanatory. file_ptr has access to the file’s raw bytes. This is how we are going to read the file. In simple words, read N number of elements, each of size S, from the file pointer and store them in the destination pointer.\n// From Manual Entry size_t fread(void ptr[restrict .size * .nmemb], size_t size, size_t nmemb, FILE *restrict stream); Lets discuss what void ptr[restrict .size * .nmemb] means. nmemb represents number of memory blocks to read. size represents the size of each memory block. void *restrict stream means the pointer can point to data of any type. restrict is a type qualifier introduced in the C99 standard. It tells the compiler that, for the lifetime of this pointer, no other pointer will be used to access the object it points to. ptr[restrict .size * .nmemb] indicates that ptr is a pointer to a block of memory with a minimum size of size * nmemb bytes. Those who have taken C tutorials can spot something here. So, its worth addressing.\nDepending on the time you have watched the File I/O tutorials, you may have seen a variety of functions to read a file. They may include fscanf, fgets etc. Those tutorials focused on text files. We are dealing with binary files, which need different handling. If you are known to assembly, you know that memory is a flat-array of bytes at low level. We have to interpret those bytes the right way to obtain the intended meaning. Either we do it ourselves or outsource it to some API. fread is that API.","return-value#Return Value":"If successful, it returns the number of items read. And this forms the basis for error handling.\nIf you wonder why fread needs size and count of entries, instead of just byte count, remember, C is statically-typed and fread allows you to abstract away the complexity of parsing and interpreting raw bytes according to some data type. If you want to deal with raw bytes directly, use the UNIX system call API read .\nIf you are still unsure about it, and want to dive deep into it, I’ve written a short detour here.\nFor more information on fread, visit its man page.\nman fread man7 online","verifying-the-magic-bytes#Verifying The Magic Bytes":"{% code title=“parser.c” %}\nint verify_elf(FILE* f_obj){ unsigned char magic_bytes[4]; if (fread(\u0026magic_bytes, 1, 4, f_obj) != 4) { fprintf(stderr, \"Error: `fread()`: Unable to read ELF magic bytes.\\n\"); fclose(f_obj); return -1; } if (magic_bytes[0] != 0x7f || magic_bytes[1] != 'E' || magic_bytes[2] != 'L' || magic_bytes[3] != 'F'){ fprintf(stderr, \"Error: Unexpected magic bytes returned.\\n Expected: `0x7F, E, L, F`\\n Found: %02X, %02X, %02X, %02X\\n\", magic_bytes[0], magic_bytes[1], magic_bytes[2], magic_bytes[3]); fclose(f_obj); return -1; } return 0; } {% endcode %}","what-are-magic-numbers#What are magic numbers?":"Magic numbers can take multiple forms in computer programming. In our case, it is a constant stream of characters, used to identify a file format.\nFrom the static analysis of the hello world binary, we know that file headers are the first thing in an ELF. The output for readelf started like this:\nELF Header: Magic: 7f 45 4c 46 02 01 01 00 00 00 00 00 00 00 00 00 The first 4 hexadecimal numbers represent the magic number.\nBut readelf refers all the pairs as magic?\nOnly the first 4 pairs of hexadecimal values form the magic number. Rest are the values in the e_ident[] array, which is a part of the file headers. Remember, each pair is hexadecimal so they are 1 byte in size.","why-0x7f-#Why `0x7f` ?":"A random text file may start with 45 4c 46 bytes, but it is very unlikely for a text file to start with 7f 45 4c 46 . That is why a random character is used.","why-fprintf-not-printf#Why `fprintf` not `printf`?":"Lets make it clear. C does have abstraction. But those abstractions are usually a collection of very low level stuff.\nprintf defaults to printing at standard out (stdout). fprintf allows you to set the direction for the output stream. Therefore, the below two are the same.\nprintf(\"Hi\\n\"); fprintf(stdout, \"Hi\\n\"); Even these are just wrapper APIs. The actual heavy lifters are v* prefixed printfs. Checkout the library functions manual for printf.\nman 3 printf man7 online fprintf lets us send error messages to stderr instead of mixing them with normal output.\nWe are passing a reference of the magic_number array as fread expects a pointer.","why-unsigned-char#Why unsigned char?":"char is 1-byte, so it is appropriate to store magic bytes.\nBut char can be signed (-128 to 127) or unsigned (0-255). We have to make sure that it is unsigned because the magic bytes for an ELF are unsigned."},"title":"Magic Verification"},"/gitbook/docs/elf-parser-project/what-are-the-available-apis/":{"data":{"":"This file includes all the internal APIs that does the heavy lifting.\ncore_api/parser.h#ifndef PARSER_H #define PARSER_H #include \u003cstdio.h\u003e #include \u003celf.h\u003e typedef struct { Elf64_Ehdr* ehdr; // File Header Elf64_Phdr* phdrs; // Program Headers Elf64_Shdr* shdrs; // Section Headers char* r_shstrtab; // flat section header string table dump int r_shstr_count; // └─ count of total bytes char** f_shstrtab; // formatted dump of section header string table int f_shstr_count; // └─ count of total individual entries char* r_strtab; // flat string table dump int r_str_count; // └─ count of total bytes char** f_strtab; // formatted dump of string table int f_str_count; // └─ count of total individual entries Elf64_Sym* symtab; // Symbol Table int symtab_count; // └─ entry count Elf64_Sym* dynsym; // Dynamic Symbol Table int dynsym_count; // └─ entry count Elf64_Rela* reladyn; // .rela.dyn Table int reladyn_count; // └─ entry count Elf64_Rela* relaplt; // .rela.plt Table int relaplt_count; // └─ entry count char* r_dynstr; // flat .dynstr dump int r_dstr_count; // └─ count of total bytes char** f_dynstr; // formatted dump of .dynstr table int f_dstr_count; // └─ count of total individual entries Elf64_Dyn* dynamic; // Dynamic Section int dyn_ent; // └─ entry count } ElfFile; int verify_elf(FILE* f_obj); int parse_ehdr(FILE* f_obj, ElfFile* AccessFile); int parse_phdrs(FILE* f_obj, ElfFile* AccessFile); int parse_shdrs(FILE* f_obj, ElfFile* AccessFile); int parse_shstrtab(FILE* f_obj, ElfFile* AccessFile); int parse_strtab(FILE* f_obj, ElfFile* AccessFile); int parse_symtab(FILE* f_obj, ElfFile* AccessFile); int parse_dynsym(FILE* f_obj, ElfFile* AccessELF); int parse_relocations(FILE* f_obj, ElfFile* AccessELF); int parse_dynstr(FILE* f_obj, ElfFile* AccessELF); int parse_dynamic(FILE* f_obj, ElfFile* AccessELF); int deallocator(ElfFile* AccessELF); #endif The data type structs, like Elf64_Ehdr are borrowed from elf.h header file.\nThe ElfFile is the actual API that would be out for external use.\nThe idea is that the main controller would open the ELF file to be parsed and it will pass the file pointer to each of these APIs, along with a ElfFile struct. Every API will populate its pointer and it can be used later.\nThe idea behind ElfFile struct is that it will be the one stop solution for all the extracted parts from the ELF. It really simplifies the access to different parts of the ELF just by one struct.\nIn the end, deallocator is called to free up the memory."},"title":"What are the available APIs?"},"/gitbook/docs/extras/":{"data":{"extras#Extras":"Extras"},"title":"_index"},"/gitbook/docs/extras/garbage-collection/":{"data":{"what-is-garbage-collection#What Is Garbage Collection?":"What Is Garbage Collection?"},"title":"garbage-collection"},"/gitbook/docs/extras/how-package-management-works/":{"data":{"how-package-management-works#How Package Management Works?":"How Package Management Works?apt, npm, yarn, pip\nsources\nrepositories\ngpg keys\nchannels\ncontrib main non-free"},"title":"how-package-management-works"},"/gitbook/docs/extras/inline-assembly/":{"data":{"inline-assembly#Inline Assembly":"Inline Assembly"},"title":"inline-assembly"},"/gitbook/docs/extras/processes/":{"data":{"a-real-example#A Real Example":"We are going to run an ELF binary made from this C code and see how Linux does all the magic.\n#include #include // sleep() int main(void){ printf(\"Hello, World!\"); sleep(400); } To get the final ELF binary:\ngcc main.c -o main_elf","entry-point-management--program-execution#Entry point Management \u0026amp;\u0026amp; Program Execution":"The dynamic linker jumps to _start in our binary (provided by crt1.o).\n_start sets up the runtime.\nThen it calls __libc_start_main(), a libc function that initializes more stuff and finally calls the main() .\nNow the program runs.\nprintf() is a call into libc. sleep() sleeps the process for 400 seconds using nanosleep syscall.","exec-family-of-functions#`exec` Family Of Functions":"execve syscall has multiple wrappers in the form of C library functions. The questions is, which one to use when?\nexecve(path, argv, envp) is the raw signature of this syscall.\nchar *argv[] = {\"/path/to/binary\", .., .., NULL}; char *envp[] = {\"variable=value\", \"var2=val2\", NULL} execve(args[0], args, envp);","functions#Functions":"execv(path, argv): inherits caller’s environment. execl(path, arg0, arg1, ..., NULL): Same as execv, except that the arguments are directly passed as varargs, rather than an array. execvp(file, argv): searches $PATH variable for the binary. Good to run programs like a shell would do, like ls or ./exe execlp(file, arg0, ...., argN, NULL): combines execl + $PATH execle(path, arg0, ..., NULL, envp): varargs + custom env And that’s how we finish establishing a baseline understanding in Linux processes.\nThank You.","handle-dynamic-linking#Handle Dynamic Linking":"Since the PHT has INTERP header, this tells the kernel to run this interpreter lib64/ld-linux-x86-64.so.2 before jumping to the entry point in the binary.\nThe kernel now loads the dynamic linker/loader (ld-linux-x86-64.so.2) into the memory.\nThe dynamic linker is the first code that will run in this process.\nNow the kernel set up the stack, rip to ld-linux’s entry point and returns the control to user space.\nAnd the child process is finally alive.\nld-linux now:\nParses the .dynamic section (readelf -S main_elf) Finds the required shared libraries, like libc.so.6 Loads them into the memory using mmap(). Applies relocations to the code. Finally, jumps to our ELF binary’s real entry point (not main, but _start).","introduction-to-processes#Introduction To Processes":"A process is an instance of a running program. Every time you execute a command or run a program, the Linux kernel creates a process to run it.\nEvery process gets a virtual address space (VAS), which is mostly made up of the program image.\nA virtual memory address (VMA) is the same as the name of a street and the physical memory address (PMA) is the actual GPS coordinates. Two cities can have a “main street” but their coordinates would be unique. Every process gets its own isolated address space, thanks to virtual memory. So your program might access 0x400000, but that doesn’t mean that’s the physical address. The OS + hardware’s MMU (Memory Management Unit) maps VMA to PMA. Without the MMU, all processes would share the same memory space — like roommates with no walls. A bug in one process could overwrite another process’s memory. With MMU, each process gets its own private virtual address space. It’s still messy on ground level but easier for management. It’s a nice abstraction.","introduction-to-processes-in-linux#Introduction To Processes In Linux":"Introduction To Processes In Linux","key-properties-of-a-process#Key Properties Of A Process":"Property Description PID (Process ID) Unique identifier of a process. PPID (Parent Process ID) Unique identifier of the process that created the PID process UID (User ID) Who owns the process State Running, sleeping, zombie etc…. Memory Info RAM consumption Executable code and data What it’s running and working with","lets-manage-an-actual-process-using-c#Lets Manage An Actual Process Using C":"Downloadable source code of the program can be found at here.\n// Standard I/O: printf() and perror() #include // General Utils: exit() #include // POSIX OS API: fork(), execvp(), getpid(), getppid() #include // Defines data types used in system calls: pid_t, the data type for process IDs #include // Provides macros and functions for waiting on child processes: waitpid(), WIFEXITED(), WEXITSTATUS() #include int main() { pid_t pid; printf(\"Calling Process `p_proc`:\\n\"); printf(\" PPID: %d\\n\", getppid()); printf(\" PID : %d\\n\", getpid()); printf(\"---------------------------\\n\"); // 1. Process creation (fork) printf(\"Calling fork.....\\n\"); pid = fork(); if (pid == -1) { perror(\"fork failed\"); printf(\"`p_proc`: return value from fork(): %d\\n\", pid); exit(EXIT_FAILURE); } if (pid == 0) { printf(\"Cloned Process `c_proc`:\\n\"); printf(\" PPID: %d\\n\", getppid()); printf(\" PID : %d\\n\", getpid()); printf(\"Return value from fork() to `c_proc`: %d\\n\", pid); printf(\"---------------------------\\n\"); // 2. Image replacement using exec char *args[] = {\"./main_elf\", NULL}; if (execvp(args[0], args) == -1) { perror(\"exec failed\"); exit(EXIT_FAILURE); } } else { // Parent process int status; waitpid(pid, \u0026status, 0); // Wait for child to finish if (WIFEXITED(status)) { printf(\"Child exited with status %d\\n\", WEXITSTATUS(status)); printf(\"---------------------------\\n\"); printf(\"Return value from fork(): to `p_proc` %d\\n\", pid); } else { printf(\"Child did not exit normally.\\n\"); } } return 0; } Lets understand this program.\npid_t is a type definition, defined inPOSIX to hold process IDs. It allows the kernel and user-space programs to use a consistent and portable data type for managing process IDs.\nThe pid variable is very important. Understanding this small and harmless looking part is very important. The confusion is paired with fork, so we’ll learn it there. fork() function is used to clone the calling process. Tell me, where would fork send its return value?\nTo the calling process? or, To the cloned process? The answer is, both. And this is where the question how the parent process maintains its state arises from. It returns\n0 to the cloned process if the calling process is cloned successfully and the process ID of the cloned process to the calling process (which is now the parent process). -1 to the parent process, if an error occurred and cloning didn’t succeed. Remember, fork() makes a near-clone of the calling process. Only certain things are different.\nThe functions getpid() and getppid() are used to obtain the child process ID and the parent process ID, respectively.\nThese functions are relative to the process that has invoked them. This is why getpid() before forking the process returned the process ID of the current process, which became the parent process after forking. The exec() family of functions replaces the current process image with a new process image. Under the hood, they all use the mighty execve syscall.\nThe char *argv[] argument is an array of pointers to null-terminated strings that represent the argument list available to the new program.\nThe first argument, by convention, should point to the filename associated with the file being executed. The array of pointers must be terminated by a null pointer.\nexecvp() is wrapper build upon execve syscall. Internally, it is just:\nint execvp(const char *file, char *const argv[]); The first argument is a pointer to the binary which is to be executed and the second argument is an array to the arguments provided to the binary.\nWhy not execvp(argv) directly?\nRemember, sys.argv[0] is reserved to the filename in python. $0 is reserved for the script name in bash. The same principle is followed here. The exec() family of functions returns only when an error has occurred, which is -1, which is why we are running the binary through execvp and matching if the return value is -1, to indicate failure or success.\nThe waitpid() function is like async() function in JavaScript, which waits for a longer process to finish and then adjusts the results appropriately, without stopping the current thread.\nAfter that some cleanup happens and we are done.\nNow, lets understand the flow of execution, that’s the most important thing here.\nLets name the process of the calling C program as p_proc and the process of the binary it is calling internally as c_proc . The calling process is cloned. We know that process are independent in their execution context, which means that p_proc and c_proc will be running independently. If we print something just after cloning the process, there is no guarantee if the first print came from the parent or the child because the child has a copy of the file descriptors and it depends on scheduling algorithms that which on goes first. Both the processes continue executing the same code from the point where fork() returned. Lets look at the execution of p_proc first. The pid variable for the calling process would have a random 4-5 digit unsigned value, which is definitely not equal to -1. Therefore, it never goes in the first if block. Also, it is not 0. So, it never goes in the second if block as well. Remaining else block. Here, it will find waitpid(), which will tell it to wait until the cloned process ends up. If you comment this part, this means, the parent didn’t wait until the child finished. Such a process is called zombie process. Such processes are adopted by init. You’ll only see Hello, World! and no sleep(10) effect. But wait. After 10 seconds, you’ll see that too, but in a new prompt. Here the parent process finished. Lets focus on the child now. The c_proc receives 0 in its pid variable. Thus, it qualifies to go inside the second if block. And everything happens as stated before in execve section. But remember, both the processes are executing independently. But because of waitpid(), the parent waits for the child to finish and cleans up everything. That’s how it works.","prefix-guide#Prefix Guide":"v: vector: refers to an array/vector of arguments l: list: refers to a list of arguments, passed as varargs p: path: tells the function to search $PATH for the executable e: environment: lets you explicitly pass the environment","process-creation#Process Creation":"It is a 4 step “process”.\nEach process may require other helper processes.\nFor example, when I did pstree, I found that VS Code is not a standalone process. It has a lot of sub-processes. Like the terminal opened within VS Code comes under it, not as a fully independent process with no parent. The same is with Firefox and everything else. Hierarchical structure is not just a design philosophy, but a very logical decision.\nIt is a proper way to manage which process spawned which sub-process. We fork the calling process (parent process), keep the identifying metadata as it is and replace the old process image with the new one. This ensures that the child process has proper links to the parent process, without any hustle. Every process is independent in its execution. But it is answerable to the parent process so that it knows if the sub-process exited successfully or it acquired any problem.\nJust like children, who are although independent but they are always answerable to their parents.","process-hierarchy#Process Hierarchy":"Processes follow a tree like structure in linux.\nThe first process is init or systemd, depending on the system, which is started by the kernel. It’s PID is 1, and PPID is 0. Every process is created by another process (its parent process). Every process is a descendant of the init process. Processes follow a parent-child relationship. Although each process is independent in nature, and the execution of a sub-process doesn’t hinder the execution of the parent process, still, the child process is linked with the parent process and it has to report its exit status. To view the tree like structure of processes, use pstree.\nWhy does process hierarchy exists? Why can’t processes be truly independent?\nTo understand this, we need to understand how processes are created.","step1---call-the-binary#Step1 - Call The Binary":"To call or execute the binary (main_elf), we need a shell (or terminal).\nI have opened my shell, which is zsh.\n$ echo $SHELL /usr/bin/zsh zsh itself is a “running” process.\n$ ps PID TTY TIME CMD 41027 pts/0 00:00:01 zsh 49852 pts/0 00:00:00 ps Run the binary in background so that we can retain the shell session.\n$ ./main_elf \u0026 [1] 52184 ps ~\n$ ps PID TTY TIME CMD 41027 pts/0 00:00:02 zsh 52184 pts/0 00:00:00 main_elf 52325 pts/0 00:00:00 ps Since main_elf is executed within zsh, zsh must be the parent of main_elf? Lets verify this.\n$ ps -T -o pid,ppid,cmd PID PPID CMD 41027 40797 /usr/bin/zsh -i 59461 41027 ./main_elf 59559 41027 ps -T -o pid,ppid,cmd -T shows processes for the current terminal session. -o helps in custom formatting. This proves that the zsh process was forked and the child process (main_elf) born from it.\nUntil now, we can say that a base template for the child process is created.\nNow we have to find the evidence for the process image replacement.","step2---correct-replace-the-process-image-in-the-child-process-fork#Step2 - Correct (Replace) The Process Image In The Child Process (Fork)":"A fork is a near-clone of the parent process. But the child process is a different program than the parent. Therefore, the process image must have been changed.\nstrace is a Linux utility which helps in tracing all the syscalls a process has executed.\nIf we run our program with strace, like this:\nstrace ./main_elf we can find a long list of output, which starts from execve.\nWhat is execve?\nIn simple words, execve is a syscall which executes a binary. In real terms, execve is a syscall which executes a binary passed in the pathname argument by replacing the process image of the current process (not the child process, the current process). execve is designed to be paired with fork in order to fit Linux’s hierarchical process structure. This is the signature of the execve syscall, int execve(const char *pathname, char *const _Nullable argv[], char *const _Nullable envp[]); pathname is the name of the binary. argv[] is a NULL terminated array of arguments passed to the binary. envp[] is a NULL terminated array of environment variables required in the process image. What is a process image?\nJust imagine how crazy and chaotic it would get to manage millions of process inside a single RAM, with all demanding various services including stack and heap. This is why an abstraction known as virtual address space (VAS) exist. Every process is executed in an isolated environment called virtual address space. A process image is the complete in-memory layout of a program after it has been loaded into memory by the OS. It is the answer to the question, “What the process looks like in the RAM?” A process image is the memory representation of a program at runtime. It includes code, data, stack, heap, environment, memory-mapped regions, loaded libraries etc…. It is created by the kernel during execve(), based on the ELF layout. This is the whole process that execve syscall carries out.\nThe kernel opens the binary (main_elf) using virtual file system (VFS).\nIt reads the ELF Header (first 64 bytes) to confirm that it is an ELF file, and find the e_type, e_entry and Program Headers Table (PHT) for carrying out its job. What is VFS and Why the kernel is using it?\nIt’s an abstraction layer inside the Linux kernel that provides a uniform interface to access all kinds of file systems — regardless of their actual formats or physical devices. There exist multiple file systems, like ext4, btrfs, zfs, hfs, ntfs, fat32 and so on…. If there is no VFS, the kernel has to learn to speak in all the different file systems. VFS knows how to talk to different file systems and provide the kernel with a consistent interface. The kernel loads the binary into memory by reading the PHT.\nIt maps each segment defined in the PHT into memory regions using mmap() (memory map) syscall and sets the permissions (R, W, X) accordingly.\nThis is the PHT for our ELF:\n$ readelf -l main_elf Elf file type is DYN (Position-Independent Executable file) Entry point 0x1060 There are 14 program headers, starting at offset 64 Program Headers: Type Offset VirtAddr PhysAddr FileSiz MemSiz Flags Align PHDR 0x0000000000000040 0x0000000000000040 0x0000000000000040 0x0000000000000310 0x0000000000000310 R 0x8 INTERP 0x0000000000000394 0x0000000000000394 0x0000000000000394 0x000000000000001c 0x000000000000001c R 0x1 [Requesting program interpreter: /lib64/ld-linux-x86-64.so.2] LOAD 0x0000000000000000 0x0000000000000000 0x0000000000000000 0x0000000000000660 0x0000000000000660 R 0x1000 LOAD 0x0000000000001000 0x0000000000001000 0x0000000000001000 0x0000000000000179 0x0000000000000179 R E 0x1000 LOAD 0x0000000000002000 0x0000000000002000 0x0000000000002000 0x000000000000010c 0x000000000000010c R 0x1000 LOAD 0x0000000000002dd0 0x0000000000003dd0 0x0000000000003dd0 0x0000000000000250 0x0000000000000258 RW 0x1000 DYNAMIC 0x0000000000002de0 0x0000000000003de0 0x0000000000003de0 0x00000000000001e0 0x00000000000001e0 RW 0x8 NOTE 0x0000000000000350 0x0000000000000350 0x0000000000000350 0x0000000000000020 0x0000000000000020 R 0x8 NOTE 0x0000000000000370 0x0000000000000370 0x0000000000000370 0x0000000000000024 0x0000000000000024 R 0x4 NOTE 0x00000000000020ec 0x00000000000020ec 0x00000000000020ec 0x0000000000000020 0x0000000000000020 R 0x4 GNU_PROPERTY 0x0000000000000350 0x0000000000000350 0x0000000000000350 0x0000000000000020 0x0000000000000020 R 0x8 GNU_EH_FRAME 0x0000000000002014 0x0000000000002014 0x0000000000002014 0x000000000000002c 0x000000000000002c R 0x4 GNU_STACK 0x0000000000000000 0x0000000000000000 0x0000000000000000 0x0000000000000000 0x0000000000000000 RW 0x10 GNU_RELRO 0x0000000000002dd0 0x0000000000003dd0 0x0000000000003dd0 0x0000000000000230 0x0000000000000230 R 0x1 Section to Segment mapping: Segment Sections... 00 01 .interp 02 .note.gnu.property .note.gnu.build-id .interp .gnu.hash .dynsym .dynstr .gnu.version .gnu.version_r .rela.dyn .rela.plt 03 .init .plt .plt.got .text .fini 04 .rodata .eh_frame_hdr .eh_frame .note.ABI-tag 05 .init_array .fini_array .dynamic .got .got.plt .data .bss 06 .dynamic 07 .note.gnu.property 08 .note.gnu.build-id 09 .note.ABI-tag 10 .note.gnu.property 11 .eh_frame_hdr 12 13 .init_array .fini_array .dynamic .got Note: It is slightly formatted so that we can see it clearly.\nProgram headers table describes how the operating system should load the ELF binary into the memory. It maps parts of the binary file into memory regions with specific permissions and purposes.\nA simple decode of this cryptic table is:\nType: PHDR\nOffset: 0x40 VirtAddr:0x40 Size: 0x310 Flags: R This describes where in the memory the program headers themselves are located.\nThe loader reads this to get all other segment info.\nType: INTERP\nOffset: 0x394 VirtAddr: 0x394 Size: 0x1c Flags: R [Requesting program interpreter: /lib64/ld-linux-x86-64.so.2] Specifies the dynamic linker to load and run this PIE executable.\nThis linker will resolve symbols and apply relocations before main() runs.\nType: LOAD Segments, The actual loadable code/data in the binary\n# LOAD 1 Offset: 0x0 VirtAddr: 0x0 FileSiz: 0x660 Flags: R The R flag shows that this is a read only section, contains the initial part of ELF.\n# LOAD 2 Offset: 0x1000 VirtAddr: 0x1000 Size: 0x179 Flags: R E This is a read + executable section. This implies that it contains the actual code segment\n# LOAD 3 Offset: 0x2000 VirtAddr: 0x2000 Size: 0x10c Flags: R Another readonly segment, which may contain constants and other ro-data.\n# LOAD 4 Offset: 0x2dd0 VirtAddr: 0x3dd0 Size: File=0x250, Mem=0x258 Flags: RW This section is both readable and writeable. This is where .data, .bss, etc…. are stored.\nType: DYNAMIC\nOffset: 0x2de0 VirtAddr: 0x3de0 Size: 0x1e0 Flags: RW Contains relocations, library names, symbol tables, etc…., which are used by the dynamic linker to perform symbol resolution and relocation at runtime.\nType: Note Segments and GNU_PROPERTY, for metadata handling. Nothing explosive.\nType: GNU_EH_FRAME, exception handling frame, used by debuggers and during crashes.\nType: GNU_STACK, specifies stack permissions.\nType GNU_RELRO, a region that is read-only after relocation.\nThe kernel finds the LOAD segments and maps them into memory.\nAll the offsets are relative to the location where the binary would be actually loaded.\nNow our program is loaded in the memory, but we’re not executing yet.","step3---end-of-the-program#Step3 - End Of The Program":"After main() ends, control goes back to __libc_start_main(), which handles the final cleanup and calls exit().\nThe kernel:\nCleans up the process resources. Returns the exit code to parent (zsh)."},"title":"processes"},"/gitbook/docs/extras/user-input/":{"data":{"displaying-the-input#Displaying The Input":"Setup rax for write syscall, 1.\nSet the file descriptor to 1, for stdout.\nLoad the buffer to write from in rsi.\nSet the number of bytes to write in rdx.\nThe question is, how we are going to know the length of our input? Because 100 is the maximum number of bytes that can be read, not necessarily the bytes we have read in total.","how-to-find-the-number-of-bytes-being-read#How to find the number of bytes being read?":"If we open the man page for read syscall, we can find this signature:\nssize_t read(int fd, void buf[.count], size_t count); If you are still unsure, lets look at the RETURN VALUE section.\nOn success, the number of bytes read is returned (zero indicates end of file), and the file position is advanced by this number. Now it is confirmed that the number of bytes read is returned, but where? As this is a C wrapper on the actual syscall!\nIf you go back to the calling convention article, you can find that rax is where the result of a syscall is returned.\nWe can also verify this by visiting the System V ABI documentation. Visit this GitLab repo, x86-64 psABI. Search for “Download latest PDF” and open the link. Check Appendix A, AMD64 Linux Kernel Conventions on page 146. Point number 5 reads as: Returning from the syscall, register %rax contains the result of the system-call. A value in the range between -4095 and -1 indicates an error, it is -errno That’s why we are setting up rdx before setting the accumulator for write syscall.\nInvoke the syscall, print to console.\nExit syscall. And we are done.","reserve-space#Reserve Space":"buffer is a user-defined label which is reserving number of bytes for stdin.\n.skip is a GAS directive used to reserve uninitialized space by skipping N-bytes.","setup-read-syscall#Setup Read Syscall":"The accumulator (rax) is set for read syscall, which is 0.\nThe file descriptor (rdi) is set to 0, which is for stdin.\nNow we need the runtime address of the buffer label in the source index register (rsi). To obtain this, we use lea instruction, which stands for load effective address.\nSet rdx with the number of bytes to read (arg 3).\nInvoke the syscall and read from console.\nWith that in mind, read syscall would look like: read(fd, buffer, bytes)","user-input-in-assembly#User Input In Assembly":"User Input In AssemblyTo take user input, we use read syscall.\n.intel_syntax noprefix .section .bss buffer: .skip 100 .section .text .global _start _start: # Step 1: Take user-input mov rax, 0 # sys_read mov rdi, 0 # stdin lea rsi, buffer # buffer to read into mov rdx, 100 # bytes to read syscall # Step 2: Display the input mov rdx, rax # number of bytes read (from previous syscall) mov rax, 1 # sys_write mov rdi, 1 # stdout lea rsi, buffer # buffer to write from syscall # Exit mov rax, 60 # sys_exit xor rdi, rdi # status 0 syscall","what-is-lea-and-why-is-lea#What is \u003ccode\u003elea\u003c/code\u003e and Why is \u003ccode\u003elea\u003c/code\u003e?":"It stands for “load effective address”.\nIt computes the address of a memory operand and loads it into a register, but it never access the value at that memory address.\nWhy we haven’t used offset?\nRemember assembly-time v/s runtime constraints? That’s the reason. offset is an assembler directive. It replaces the label with a virtual address or offset. It doesn’t resemble the actual runtime address of that label (symbol). lea is a CPU instruction which specializes in finding the runtime memory address of a label. Lets talk about an undefined behavior here.\nRight now we don’t know how memory is managed, so we don’t know what an offset, virtual address or anything else actually mean. Sometimes, just using offset with mov can perfectly work. But, its not guaranteed. This undefined behavior exists when that offset or virtual address is mapped as it is in the actual memory, which in today’s world is almost impossible if you use production-grade principles. ASLR exists to eliminate such possibilities. ASLR stands for address space layout randomization. But we need not to know about it. Just keep this in mind that offset might work but it is not right.","why-stdin-goes-in-bss-and-not-in-data-section#Why `stdin` goes in `.bss` and not in `.data` section?":".bss section is the place for uninitialized data.\nThe allocation in .bss is zero-initialized at runtime. Why can’t we just zero initialize the memory locations ourselves in the .data section only?\n.data section holds static and global variables, which are already initialized. This directly affects the size of the binary.\nWhen we allocate an array of size 100 bytes, zero-initialized in .data section, those 100 bytes are basically excess space, because they aren’t used right away. We have to populate them before using.\nThose 100 bytes could also have been allocated directly at runtime, reducing the size of the overall binary? This is the whole idea behind the existence of .bss and why stdin goes in .bss not .data."},"title":"user-input"},"/gitbook/docs/extras/virtualization/":{"data":{"virtualization#Virtualization":"Virtualizationhow to virtualize efficiently\nproblems in mainstream solutions\nplug and play GNOME Boxes\nkvm+qemu the best way\ncomparison table (ease of use, customization, memory, usage, unforeseen breaks, most stable, plug and play behavior, “set it, forget it” level"},"title":"virtualization"},"/gitbook/docs/low-level-architecture/":{"data":{"":"Understanding low level systems is about understanding how things work at assembly level.\nWe are not going to study assembly alone. It is going to be a mixture of C and assembly.\nWe will discuss how C constructs exist in assembly. How abstractions in C are translated to assembly.\nI don’t expect a mastery in C, but normal understanding of programming is expected. Although it is not really required because we scratch the surface before diving in anyways. So, yeah.\nEnjoy."},"title":"Low Level Architecture"},"/gitbook/docs/low-level-architecture/control-flow/":{"data":{"":"Polished on 8 September 2025 (First written in May 2025)","comparison#Comparison":"cmp compares two operands by subtracting the second operand from the first operand.\ncmp op1, op2 =\u003e op1 - op2 The result is used to set certain CPU flags. There is a special purpose register called RFLAGS in x64 and EFLAGS in x86 which holds these flags. And other architectures have similar mechanisms.\nNote: cmp is just one instruction that changed RFLAGS; there are other operations that set them as well. Although RFLAGS is a 64-bit wide register, the total CPU flags aren’t 64. Most of the bits are reserved by the CPU for internal things. The most common ones include: CF ZF OF PF AF SF TF . Based on these CPU flags the conditional jump statements make an assumption about the result of comparison.","introduction#Introduction":"Control flow helps in breaking the linearity of execution in assembly.\nThese three high level constructs in C are all implemented using jump statements.\nIf else ladder: if, else if and else. Iteration: for, while and do-while loops. Switch case. Synopsis\nUnconditional jumps are the jumps that are certain to happen. Nothing can stop them. jmp mnemonic is used for this. Conditional jumps are based on a conditions. The cmp mnemonic helps in comparing two entities. The comparison instruction affects certain CPU Flags which the jump statements use to make their decision of jumping. The jump is made to a code symbol.","signed-comparison#Signed Comparison":"","unsigned-comparison#Unsigned Comparison":"Since unsigned integers uses full set of bits available to store the magnitude of the value, we don’t require other flags."},"title":"Control Flow"},"/gitbook/docs/low-level-architecture/control-flow/examples/":{"data":{"conclusion#Conclusion":"This is how control flow looks like.","example-1-if-else#Example 1: if-else":"#include int main(void) { int a = 5, b = 6; if (a \u003e b){ printf(\"Yes, 5 \u003e 6.\\n\"); } else{ printf(\"No, 5 \u003c 6.\\n\"); } } Assembly:\n.text .section\t.rodata .LC0: .string\t\"Yes, 5 \u003e 6.\" .LC1: .string\t\"No, 5 \u003c 6.\" .text .globl\tmain .type\tmain, @function main: push\trbp mov\trbp, rsp sub\trsp, 16 mov\tDWORD PTR -4[rbp], 5 ; a mov\tDWORD PTR -8[rbp], 6 ; b mov\teax, DWORD PTR -4[rbp] cmp\teax, DWORD PTR -8[rbp] ; if (a \u003e b) jle\t.L2 ; if block lea\trax, .LC0[rip] mov\trdi, rax call\tputs@PLT jmp\t.L3 ; else block .L2: lea\trax, .LC1[rip] mov\trdi, rax call\tputs@PLT ; return .L3: mov\teax, 0 leave ret","example-10-do-while-loop#Example 10: do while loop":"#include int main(void) { int i = 0; do { printf(\"i: %d\\n\", i); i++ ; } while (i \u003c 5); } Assembly:\n.text .section\t.rodata .LC0: .string\t\"i: %d\\n\" .text .globl\tmain .type\tmain, @function main: push\trbp mov\trbp, rsp sub\trsp, 16 mov\tDWORD PTR -4[rbp], 0 .L2: mov\teax, DWORD PTR -4[rbp] mov\tesi, eax lea\trax, .LC0[rip] mov\trdi, rax mov\teax, 0 call\tprintf@PLT add\tDWORD PTR -4[rbp], 1 cmp\tDWORD PTR -4[rbp], 4 ; while (i \u003c 5) check jle\t.L2 ; return mov\teax, 0 leave ret","example-2-if--else-if--else#Example 2: if- else if -else":"#include int main(void) { int a = 5, b = 6; if (a == b){ printf(\"Yes, a \u003e b.\\n\"); } else if (a == b){ printf(\"Actually, a == b.\\n\"); } else{ printf(\"No, a \u003c b.\\n\"); } } Assembly:\n.text .section\t.rodata .LC0: .string\t\"Yes, a \u003e b.\" .LC1: .string\t\"Actually, a == b.\" .LC2: .string\t\"No, a \u003c b.\" .text .globl\tmain .type\tmain, @function main: push\trbp mov\trbp, rsp sub\trsp, 16 mov\tDWORD PTR -4[rbp], 5\t; a mov\tDWORD PTR -8[rbp], 6\t; b mov\teax, DWORD PTR -4[rbp] cmp\teax, DWORD PTR -8[rbp]\t; if (a \u003e b) jne\t.L2 ; if block lea\trax, .LC0[rip] mov\trdi, rax call\tputs@PLT jmp\t.L3 ; else if block .L2: mov\teax, DWORD PTR -4[rbp] cmp\teax, DWORD PTR -8[rbp]\t; if (a == b) jne\t.L4 lea\trax, .LC1[rip] mov\trdi, rax call\tputs@PLT jmp\t.L3 ; else block .L4: lea\trax, .LC2[rip] mov\trdi, rax call\tputs@PLT ; return .L3: mov\teax, 0 leave ret","example-3-switch-case#Example 3: Switch Case":"#include int main(void) { int choice = 5; switch(choice){ case 0: printf(\"Yes, equal to zero.\\n\"); break; case 5: printf(\"No, not 1.\\n\"); break; default: printf(\"No case matched. It is 5.\\n\"); } } Assembly:\n.text .section\t.rodata .LC0: .string\t\"Yes, equal to zero.\" .LC1: .string\t\"No, not 1.\" .LC2: .string\t\"No case matched. It is 5.\" .text .globl\tmain .type\tmain, @function main: push\trbp mov\trbp, rsp sub\trsp, 16 mov\tDWORD PTR -4[rbp], 5 ; choice cmp\tDWORD PTR -4[rbp], 0 ; case 0 je\t.L2 cmp\tDWORD PTR -4[rbp], 5 ; case 1 je\t.L3 jmp\t.L7 ; default ; case 0 .L2: lea\trax, .LC0[rip] mov\trdi, rax call\tputs@PLT jmp\t.L5 ; case 1 .L3: lea\trax, .LC1[rip] mov\trdi, rax call\tputs@PLT jmp\t.L5 ; default .L7: lea\trax, .LC2[rip] mov\trdi, rax call\tputs@PLT ; return .L5: mov\teax, 0 leave ret Notice that for default case we are using unconditional jump.\nAlthough .L7 is not strictly required but the semantics of switch cases require you to create a separate label for each. The compiler may optimize it at higher levels.","example-4-importance-of-break#Example 4: Importance of break;":"This code doesn’t use break; in case 5. We know that if break is not present, the cases after it are executed without any check. Let’s see.\n#include int main(void) { int choice = 5; switch(choice){ case 0: printf(\"Yes, equal to zero.\\n\"); break; case 5: printf(\"No, not 1.\\n\"); default: printf(\"No case matched. It is 5.\\n\"); } } Assembly:\n.text .section\t.rodata .LC0: .string\t\"Yes, equal to zero.\" .LC1: .string\t\"No, not 1.\" .LC2: .string\t\"No case matched. It is 5.\" .text .globl\tmain .type\tmain, @function main: push\trbp mov\trbp, rsp sub\trsp, 16 mov\tDWORD PTR -4[rbp], 5 cmp\tDWORD PTR -4[rbp], 0 ; case 0 je\t.L2 cmp\tDWORD PTR -4[rbp], 5 ; case 5 je\t.L3 jmp\t.L4 ; case 0 .L2: lea\trax, .LC0[rip] mov\trdi, rax call\tputs@PLT jmp\t.L5 ; case 5 .L3: lea\trax, .LC1[rip] mov\trdi, rax call\tputs@PLT\t; Notice, no jmp to .L5 ; case default .L4: lea\trax, .LC2[rip] mov\trdi, rax call\tputs@PLT ; return .L5: mov\teax, 0 leave ret This code lacks an unconditional jump to .L5, the return label, which is why the case after it is executed without any check as the check is performed before.","example-5-for-loop#Example 5: for loop":"#include int main(void) { for (int i = 0; i \u003c 5; i++){ printf(\"i: %d\\n\", i); } } Assembly:\n.text .section\t.rodata .LC0: .string\t\"i: %d\\n\" .text .globl\tmain .type\tmain, @function main: push\trbp mov\trbp, rsp sub\trsp, 16 mov\tDWORD PTR -4[rbp], 0 ; i = 0 jmp\t.L2 .L3: mov\teax, DWORD PTR -4[rbp] ; load i mov\tesi, eax ; esi = i (arg 2) lea\trax, .LC0[rip] mov\trdi, rax ; edi = addr(str) mov\teax, 0 call\tprintf@PLT add\tDWORD PTR -4[rbp], 1 ; update local instance of i ; body .L2: cmp\tDWORD PTR -4[rbp], 4 ; check i \u003c 4 jle\t.L3 ; otherwise, return mov\teax, 0 leave ret Notice that .L2 is present after .L3 , so .L2 need not to make any jump to it. That’s compiler optimization at bare minimum.\nEven if you declare i outside of loop, nothing will change.","example-6-infinite-for-loop#Example 6: Infinite for loop":"#include int main(void) { int i = 0; for (;;){ printf(\"i: %d\\n\", i); } } Assembly:\n.text .section\t.rodata .LC0: .string\t\"i: %d\\n\" .text .globl\tmain .type\tmain, @function main: push\trbp mov\trbp, rsp sub\trsp, 16 mov\tDWORD PTR -4[rbp], 0 .L2: mov\teax, DWORD PTR -4[rbp] mov\tesi, eax lea\trax, .LC0[rip] mov\trdi, rax mov\teax, 0 call\tprintf@PLT jmp\t.L2 Unless the source has an explicit break condition, it will run until it eats up all the resources.","example-7-while-loop#Example 7: while loop":"Although this is an infinite while loop, but it will not run.\n#include int main(void) { int i = 0; while (i){ printf(\"i: %d\\n\", i); } } Assembly:\n.text .section\t.rodata .LC0: .string\t\"i: %d\\n\" .text .globl\tmain .type\tmain, @function main: push\trbp mov\trbp, rsp sub\trsp, 16 mov\tDWORD PTR -4[rbp], 0 jmp\t.L2 .L3: mov\teax, DWORD PTR -4[rbp] mov\tesi, eax lea\trax, .LC0[rip] mov\trdi, rax mov\teax, 0 call\tprintf@PLT .L2: cmp\tDWORD PTR -4[rbp], 0 jne\t.L3 mov\teax, 0 leave ret Notice the condition in .L2. It jumps to .L3 when the comparison is not equal to zero.","example-8-infinite-while-loop#Example 8: Infinite while loop":"#include int main(void) { int i = 1; while (i){ printf(\"i: %d\\n\", i); } } Assembly:\n.text .section\t.rodata .LC0: .string\t\"i: %d\\n\" .text .globl\tmain .type\tmain, @function main: push\trbp mov\trbp, rsp sub\trsp, 16 mov\tDWORD PTR -4[rbp], 1 jmp\t.L2 .L3: mov\teax, DWORD PTR -4[rbp] mov\tesi, eax lea\trax, .LC0[rip] mov\trdi, rax mov\teax, 0 call\tprintf@PLT .L2: cmp\tDWORD PTR -4[rbp], 0 jne\t.L3 mov\teax, 0 leave ret If you notice, infinite while loop and infinite for loop matches instruction by instruction except the .L2 label in while loop, which basically checks for true/false.","example-9-finite-while-loop#Example 9: Finite while loop":"#include int main(void) { int i = 0; while (i \u003c 5){ printf(\"i: %d\\n\", i); } } Assembly:\n.text .section\t.rodata .LC0: .string\t\"i: %d\\n\" .text .globl\tmain .type\tmain, @function main: push\trbp mov\trbp, rsp sub\trsp, 16 mov\tDWORD PTR -4[rbp], 0 jmp\t.L2 .L3: mov\teax, DWORD PTR -4[rbp] mov\tesi, eax lea\trax, .LC0[rip] mov\trdi, rax mov\teax, 0 call\tprintf@PLT add\tDWORD PTR -4[rbp], 1 .L2: cmp\tDWORD PTR -4[rbp], 4 jle\t.L3 ; return mov\teax, 0 leave ret Again, it is exactly same as finite for loop.\nThis proves that for and while is just syntactic sugar.","examples#Examples":"ExamplesLets talk practically how if, else if and else, for loop, while loop and do while loop exist."},"title":"examples"},"/gitbook/docs/low-level-architecture/floats/":{"data":{"":"The history of floats, the issues, the solutions, historical solutions and modern solutions, x87 FPU stack, SSE/AVX, ymm, zmm regs"},"title":"The World Of Floats"},"/gitbook/docs/low-level-architecture/format-specifier/":{"data":{"":"what they mean to c and asm.\nsyntax of writing a format specifier: like 0x%016llx"},"title":"Format Specifier"},"/gitbook/docs/low-level-architecture/functions/":{"data":{"":"Function and stack might seem unrelated when you work at high level. At low level, these are in a very inseparable relationship. You can’t study one without other.\nIf stack is an idea, function is its best implementation. If function is an idea, it can’t exist in its best form without stack. To understand their relationship, we have to move slow enough that we don’t gloss over anything and fast enough that we don’t dry ourselves up. Here is the progression:\nIntroduction introduces to the idea of functions and stack at assembly level. Although it dives deep into theory, it will not feel like that because I have included real-life examples, interactive questions, gradual flow and introduction, ASCII art and a small “theoretical-practical” in the end. Recursion: introduces us to stack discipline and how stack frames are actually stacked. Here we implement the theory we have studied so far. Fully practical. Parameter Passing improves our understanding of reference and returns by one step. It lays the foundation without which you can’t understand how complex value are returned. Fully practical. How returns are managed? is the final boss. Fully practical. For the time being, practical means that we are going to study an implementation of a set of concepts that feel superficial. We can definitely use GDB to inspect the real stack in the memory but to me, it is not the right way to understand.\nTo me, the right approach is that you study the theory, you implement it on paper (in terms of loose ideas) and then you move towards ultimate practicality. Right now, we are exploring the first two parts in this process. But very soon we will explore the third one as well."},"title":"Functions And Stack"},"/gitbook/docs/low-level-architecture/functions/parameter-passing/":{"data":{"assembly-comparison#Assembly Comparison":"This is call by value.\n#include void sq(int n){ int s = n*n; printf(\"%d\\n\", s); } int main(){ sq(5); } And there is nothing new.\n.LC0: .string\t\"%d\\n\" sq: push\trbp mov\trbp, rsp sub\trsp, 16 mov\tDWORD PTR -4[rbp], edi ; 5 mov\teax, DWORD PTR -4[rbp] imul\teax, eax ; 25 mov\tDWORD PTR -4[rbp], eax ; update local n mov\teax, DWORD PTR -4[rbp] mov\tesi, eax lea\trax, .LC0[rip] mov\trdi, rax mov\teax, 0 call\tprintf@PLT nop leave ret main: push\trbp mov\trbp, rsp sub\trsp, 16 mov\tDWORD PTR -4[rbp], 5 mov\teax, DWORD PTR -4[rbp] mov\tedi, eax call\tsq mov\teax, DWORD PTR -4[rbp] mov\tesi, eax lea\trax, .LC0[rip] mov\trdi, rax mov\teax, 0 call\tprintf@PLT mov\teax, 0 leave ret This is call by reference.\n#include void sq(int* n){ *n = (*n)*(*n); printf(\"%d\\n\", *n); } int main(){ int num = 5; sq(\u0026num); printf(\"%d\\n\", num); } This is the assembly.\n.LC0: .string\t\"%d\\n\" sq: push\trbp mov\trbp, rsp sub\trsp, 16 mov\tQWORD PTR -8[rbp], rdi ; address of num passed from main mov\trax, QWORD PTR -8[rbp] mov\tedx, DWORD PTR [rax] mov\trax, QWORD PTR -8[rbp] mov\teax, DWORD PTR [rax] imul\tedx, eax ; 5 * 5 mov\trax, QWORD PTR -8[rbp] ; load the address of -8[rbp] mov\tDWORD PTR [rax], edx ; mov the updated value of n (n*n) mov\trax, QWORD PTR -8[rbp] mov\teax, DWORD PTR [rax] mov\tesi, eax lea\trax, .LC0[rip] mov\trdi, rax mov\teax, 0 call\tprintf@PLT nop leave ret main: push\trbp mov\trbp, rsp sub\trsp, 16 mov\tDWORD PTR -4[rbp], 5 lea\trax, -4[rbp] ; We are loading the address of 4[rbp], not what is at -4[rbp] mov\trdi, rax call\tsq mov\teax, DWORD PTR -4[rbp] mov\tesi, eax lea\trax, .LC0[rip] mov\trdi, rax mov\teax, 0 call\tprintf@PLT mov\teax, 0 leave ret And the fun begins here. Let’s start with the main symbol.\nIn call by value assembly, we load the value at stack memory:\nmov eax, DWORD PTR -4[rbp] In call by reference, we are computing the address where 5 is in the stack memory:\nlea rax, -4[rbp] In call by reference, the compiler uses 64-bit registers for the pointer, because addresses on a 64-bit system are 8 bytes. The integer itself is 4 bytes, so we still use 32-bit registers for arithmetic.\nThe rest is the same.\nLet’s shift our focus on the sq symbol now.\nIn call by value, we are moving a 4-byte value at -4[rbp], which is 5.\nmov DWORD PTR -4[rbp], edi In call by reference, we are moving a 8-byte value, and we know that an integer is not sized “8-bytes” by default on Linux. This again reinforces the fact that this is a pointer to/address of 5, not 5 itself.\nmov QWORD PTR -8[rbp], rdi The call by value code simply loaded the local instance of n in eax , multiplied with itself and updated the local instance with new value.\nmov eax, DWORD PTR -4[rbp] imul eax, eax mov DWORD PTR -4[rbp], eax This is quite complicated for the call by reference program.\nFirst we load 8-bytes starting from -8[rbp].\nmov rax, QWORD PTR -8[rbp] Next we dereference the address to obtain the actual value (5). Since it is a 32-bit value, we are using DWORD to move it in edx .\nmov edx, DWORD PTR [rax] We repeat the same process to hold 5 in another register for multiplication.\nmov rax, QWORD PTR -8[rbp] mov eax, DWORD PTR [rax] We are using different registers here to avoid overwriting values that are still required for computation.\nNow we have to update the existing instance of stack with 25. In call by value, it was again quite simple.\nmov DWORD PTR -4[rbp], eax In call by reference, first we have to load the 8-bytes of address in rax:\nmov rax, QWORD PTR -8[rbp] Then we dereference it and mov 25 there.\nAfter this we print the value.\nAnd that’s how call by reference works.","but-what-is-the-utility-of-call-by-reference#But what is the utility of call by reference?":"That’s the only way stack frames can interact.\nThat’s the only way stack frames can manage complex data.\nPointers are the only mechanism that lets a function access memory outside its own frame","parameter-passing#Parameter Passing":"Parameter Passing5 September 2025","parameter-passing-1#Parameter Passing":"Functions can receive arguments from the caller. These arguments can be passed in two ways.\nCall by value -\u003e a copy of the actual value is passed. Call by reference -\u003e the memory address of the value is passed, allowing the function to modify the original variable. Let’s take an example. We have a number and we want to increment it by 10.\n#include void inc1(int n){ printf(\"Inside inc1\\n\"); printf(\" Before increment: %d\\n\", n); n += 10; printf(\" After increment: %d\\n\", n); } void inc2(int *m){ printf(\"Inside inc2\\n\"); printf(\" Before increment: %d\\n\", *m); *m += 10; printf(\" After increment: %d\\n\", *m); } int main(){ int n = 2; printf(\"In main\\n\"); printf(\" Before increment: %d\\n\", n); inc1(n); printf(\"In main\\n\"); printf(\" After increment: %d\\n\", n); printf(\"\\n--------\\n\\n\"); int m = 4; printf(\"In main\\n\"); printf(\" Before increment: %d\\n\", m); inc2(\u0026m); printf(\"In main\\n\"); printf(\" After increment: %d\\n\", m); } When we normally pass a value, a copy of it is passed. When we pass the reference of a value, the memory address at which it is stored is passed, which is why the change persists after function call.\nSwapping two numbers is very famous in this space.","parameters-and-arguments#Parameters and Arguments":"Parameters are variables defined in a function definition that act as placeholders for values the function will receive.\nArguments are the actual values supplied to a function when it is called.\nFor example:\n#include int square(int n){ return n*n; } int main(){ square(5); } Here, n is parameter and 5 is the argument."},"title":"parameter-passing"},"/gitbook/docs/low-level-architecture/functions/recursion/":{"data":{"3808-frame#3808 Frame":"The top stack frame is rbp=3808 , and is rsp=3792 here. Let’s look at assembly.\nIf (n==0) , we set eax=1 (which is the return value) and jump to .L3 .\nThe leave instruction resets the stack pointer by using rbp mov rsp, 3808 and pops the old base pointer (located in rsp) into rbp, which changes the current base pointer to the previous stack frame. mov rbp, [3808] ; [3808] = 3840 add rsp, 8 ; rsp = 3816 Now rsp=3816 and rbp=3840 .\nWhen we do pop rip, it is: mov rip, [3816] add rsp, 8 ; rsp = 3824 dereferencing 3816 gives the address of imul eax, DWORD PTR -4[rbp] instruction in the previous stack frame (3840).\nAnd we have successfully returned to the previous stack frame, the one with rbp=3840 .\nState of pointers: rsp=3824 and rbp=3840 .","3840-frame#3840 Frame":"Now we are inside the rbp=3840 stack frame.\nHere, n=1. So, .L2 was executed, which sets up the next recursion call.\nThe next recursion call was rbp=3808, which successfully returned 1 in eax .\nNow we are at:\nimul eax, DWORD PTR -4[rbp] For this stack frame, rbp=3840. -4[3840] would go to 3836 which stores a local copy of n received by this procedure’s frame, which is 1 here.\nSo, the instruction translates to:\nimul eax, 1 and eax is already 1, so the result in eax would be 1.\nAfter this, .L3 is called.\n; leave mov rsp, 3840 mov rbp, [3840] ; [3840] = 3872 add rsp, 8 ; rsp = 3848 ; return mov rip, [3848] add rsp, 8 ; rsp = 3856 And we have successfully returned to the previous stack frame, the one with rbp=3872 .\nState of pointers: rsp=3856 and rbp=3872 .","3872-frame#3872 Frame":"Now we are inside the rbp=3872 stack frame.\nHere, n=2. So, .L2 was executed, which sets up the next recursion call.\nThe next recursion call was rbp=3840 , which successfully returned 1 in eax.\nNow we are at:\nimul eax, DWORD PTR -4[rbp] For this stack frame, rbp=3872 . -4[3872] would go to 3868 , which stores a local copy of n received by this procedure’s frame, which is 2 here.\nSo, the instruction translates to:\nimul eax, 2 eax is 1, so the result in eax would be 2.\nAfter this, .L3 is called.\n; leave mov rsp, 3872 mov rbp, [3872] ; [3872] = 3904 add rsp, 8 ; rsp = 3880 ; ret mov rip, [3880] add rsp, 8 ; rsp = 3888 And we have successfully returned to the previous stack frame, the one with rbp=3904 .\nState of pointers: rsp=3888 and rbp=3904 .","3904-frame#3904 Frame":"Now we are inside the rbp=3904 stack frame.\nHere, n=3. So, .L2 was executed, which sets up the next recursion call.\nThe next recursion call was rbp=3872 , which successfully returned 2 in eax.\nNow we are at:\nimul eax, DWORD PTR -4[rbp] For this stack frame, rbp=3904 , -4[3904] would go to 3900 , which stores a local copy of n received by this procedure’s frame, which is 3 here.\nSo, the instruction translates to:\nimul eax, 3 eax is 2, so the result in eax would be 6.\nAfter this, .L3 is called.\n; leave mov rsp, 3904 mov rbp, [3904] ; [3904] = 3936 add rsp, 8 ; rsp = 3912 ; ret mov rip, [3912] add rsp, 8 ; rsp = 3920 And we have successfully returned to the previous stack frame, the one with rbp=3936 .\nState of pointers: rsp=3920 and rbp=3936 .","3936-frame#3936 Frame":"Now we are inside the rbp=3936 stack frame.\nHere, n=4. So, .L2 was executed, which sets up the next recursion call.\nThe next recursion call was rbp=3904 , which successfully returned 6 in eax.\nNow we are at:\nimul eax, DWORD PTR -4[rbp] For this stack frame, rbp=3936 , -4[3936] would go to 3932 , which stores a local copy of n received by this procedure’s frame. The value of n is 4 here.\nSo, the instruction translates to:\nimul eax, 4 eax is 6, so the result in eax would be 24.\nAfter this, .L3 is called.\n; leave mov rsp, 3936 mov rbp, [3936] ; [3936] = 3968 add rsp, 8 ; rsp = 3944 ; ret mov rip, [3944] add rsp, 8 ; rsp = 3952 And we have successfully returned to the previous stack frame, the one with rbp=3968 .\nState of pointers: rsp=3952 and rbp=3968 .","3968-frame#3968 Frame":"Now we are inside the rbp=3968 stack frame.\nHere, n=5. So, .L2 was executed, which sets up the next recursion call.\nThe next recursion call was rbp=3936 , which successfully returned 24 in eax.\nNow we are at:\nimul eax, DWORD PTR -4[rbp] For this stack frame, rbp=3968 , -4[3968] would go to 3964 , which stores a local copy of n received by this procedure’s , which is 5 here.\nSo, the instruction translates to:\nimul eax, 5 eax is 24, so the result in eax would be 120.\nAfter this, .L3 is called.\n; leave mov rsp, 3968 mov rbp, [3968] ; [3968] = 4000 add rsp, 8 ; rsp = 3976 ; ret mov rip, [3976] add rsp, 8 ; rsp = 3984 And we have successfully returned to the previous stack frame, the one with rbp=4000 .\nState of pointers: rsp=3984 and rbp=4000 .\nNow we are inside the rbp=4000 stack frame.\nThis is where we started from. From here, we return to the almighty gods of C, the startup code, which handles the exit and remaining cleanup. Sigh. It was crazy, isn’t it? Conclusion is still remaining.","conclusion#Conclusion":"The only conclusion that is worth reading is that, it’s a hoax that low level systems are complex and impossible to understand without the help of some C God. It’s a hoax that you can’t draw theory, visualize theory. Any idea that restricts you from doing the work to build deep understanding is just a hoax. It takes time and it takes energy, but the output is worth every bit of effort.\nIt took me a whole day to build this understanding and stack art, roughly ~7h accumulated.\nAnd the result of that is that I will never be confused about stack discipline. I am sure that stack is not done yet. There is a lot to explore. But I am also ready to do that, the hard way, the boring way, the repetitive way.","definition#Definition":"A recursive function is a function that calls itself during its execution to solve a problem by breaking it down into smaller, simpler instances of the same problem.\nIt requires a recursive step where the function calls itself with a modified input and a base case, which is the stopping condition that prevents infinite calls.\nThese recursive case and base case are the things that makes a recursion either easy to understand or very complex. For this reason, we are using factorial, because it is very straightforward.\nProblems like Tower of Hanoi and Fibonacci series can be solved with recursion but they are a little complex to understand. But we can definitely touch that later. Recursion is the ideal next step to understand:\nhow stack frames are “stacked”? how stack frame returns? how arguments are managed across calls?","factorial#Factorial":"To calculate the factorial of a number, we use this formula:\nn! = n * (n - 1) * (n - 2) * .... * (n - (n-1)) where n is a positive integer and the factorial of 0 is 1. For example - 5! = 120, which is calculated as:\n5 * (5 - 1) * (5 - 2) * (5 - 3) * (5 - 4) 5 * 4 * 3 * 2 * 1 120 A loop based program would be:\nint factorial(int n){ if (n == 0){ return 1; } int f = 1; for (int i = 1; i \u003c= n; i++){ f *= i; } return f; } To do this with recursion, we need a base condition and recursive condition.\nBase condition: 0! = 1. Recursive condition: n * func(n - 1) The idea is that each recursive call reduces the value of n until it becomes 0. When it becomes zero, return is triggered. And the final return computes n! .\nThus, recursion can be implemented as:\n#include int rec_fact(int n){ if (n == 0){ return 1; } return (n * rec_fact(n - 1)); } int main(){ int n = 5; rec_fact(n); } This the assembly.\nrec_fact: push rbp mov rbp, rsp sub rsp, 16 mov DWORD PTR -4[rbp], edi\t; n received as func arg is moved to stack cmp DWORD PTR -4[rbp], 0\t; (n == 0) check jne .L2\t; if not, prepare for next call mov eax, 1 jmp .L3\t; if yes, we've hit the base case ; return n * rec_fact(n - 1) .L2: mov eax, DWORD PTR -4[rbp]\t; load n in eax sub eax, 1\t; eax = eax - 1 OR ( n - 1) mov edi, eax\t; setup arg1 = eax call rec_fact\t; call again imul eax, DWORD PTR -4[rbp]\t; when the function call hit the base case, and there is a return, multiply the return (rax) with n ; return .L3: leave ret main: push rbp mov rbp, rsp sub rsp, 16 mov DWORD PTR -4[rbp], 5 mov eax, DWORD PTR -4[rbp] mov edi, eax call rec_fact mov eax, 0 leave ret Procedures receive the first 6 arguments in registers, where the first one goes into edi . But in a continuous recursion, edi is constantly in use, which makes it unreliable to keep the original value of n.\nWe can use other registers? We can manage the caller/callee discipline ourselves? We can, but we’ve to deal with two problems. We can only implement this in pure assembly. There is no limit to how many arguments a function can receive, which makes relying on registers a mess. We are compiling C into assembly. When we are translating one language into another one, we would prefer standard rules which remain consistent across all the cases. That’s the reason behind creating a local copy of n on stack. This keeps the original value intact, stack frames clean and predictable and no management hell. The assembly is straightforward, so we will skip that. Lets do a dry run and understand the state of stack, because, we already know what the code is doing.","recursion-recursive-function#Recursion (Recursive Function)":"Recursion (Recursive Function)04, 05 September 2025","return-management#Return Management":"This is a compressed view of stack.\n| Stack Frame | rbp | n | *-------------*------*---* | main | 4000 | 5 | \u003c- Bottom | rec_fact | 3968 | 5 | | rec_fact | 3936 | 4 | | rec_fact | 3904 | 3 | | rec_fact | 3872 | 2 | | rec_fact | 3840 | 1 | | rec_fact | 3808 | 0 | \u003c- Top If you notice, each frame is exactly 32-bytes in size; 16 for locals, 8 for return address and 8 for old rbp .","stack-layout#Stack Layout":"We can talk theory all the day, but how one interprets that theory changes everything. And the best way to ensure that we are on the same page is by visualizing the stack.\nNote: This visual representation of stack might not be very accurate, but it explains things in a way that ensures that all of use are interpreting the theory the right way.\nWith this ASCII Art, we can draw the theory. That’s the main thing.\nAll the addresses are in decimal, no hex is used as it creates an overhead of calculation. The addresses are kept deliberately small so that subtraction of bytes is easier to calculate.\n4008: rsp *------------------* 4000 -\u003e | old rbp on stack | (push rbp) *------------------* new rbp = 4000 Stack Frame: main() *------------------* 3996 -\u003e | edi (n = 5) | -4[rbp] *------------------* 3992 -\u003e | | -8[rbp] *------------------* 3988 -\u003e | | -12[rbp] *------------------* 3984 -\u003e | | -16[rbp] *------------------* 3976 -\u003e | addr(mov eax, 0) | *------------------* 3968 -\u003e | main() rbp (4000)| *------------------* new rbp = 3968 Stack Frame: rec_fact() *------------------* 3964 -\u003e | edi (n = 5) | -4[rbp] *------------------* 3960 -\u003e | | -8[rbp] *------------------* 3956 -\u003e | | -12[rbp] *------------------* 3952 -\u003e | | -16[rbp] *------------------* 3944 -\u003e | addr(imul eax, 5)| *------------------* 3936 -\u003e | old rbp (3968) | *------------------* new rbp = 3936 Stack Frame: rec_fact() *------------------* 3932 -\u003e | edi (n = 4) | -4[rbp] *------------------* 3928 -\u003e | | -8[rbp] *------------------* 3924 -\u003e | | -12[rbp] *------------------* 3920 -\u003e | | -16[rbp] *------------------* 3912 -\u003e | addr(imul eax, 4)| *------------------* 3904 -\u003e | old rbp (3936) | *------------------* new rbp = 3904 Stack Frame: rec_fact() *------------------* 3900 -\u003e | edi (n = 3) | -4[rbp] *------------------* 3896 -\u003e | | -8[rbp] *------------------* 3892 -\u003e | | -12[rbp] *------------------* 3888 -\u003e | | -16[rbp] *------------------* 3880 -\u003e | addr(imul eax, 3)| *------------------* 3872 -\u003e | old rbp (3904) | *------------------* new rbp = 3872 Stack Frame: rec_fact() *------------------* 3868 -\u003e | edi (n = 2) | -4[rbp] *------------------* 3864 -\u003e | | -8[rbp] *------------------* 3860 -\u003e | | -12[rbp] *------------------* 3856 -\u003e | | -16[rbp] *------------------* 3848 -\u003e | addr(imul eax, 2)| *------------------* 3840 -\u003e | old rbp (3872) | *------------------* new rbp = 3840 Stack Frame: rec_fact() *------------------* 3836 -\u003e | edi (n = 1) | -4[rbp] *------------------* 3832 -\u003e | | -8[rbp] *------------------* 3828 -\u003e | | -12[rbp] *------------------* 3824 -\u003e | | -16[rbp] *------------------* 3816 -\u003e | addr(imul eax, 1)| *------------------* 3808 -\u003e | old rbp (3840) | *------------------* new rbp = 3808 Stack Frame: rec_fact() *------------------* 3804 -\u003e | edi (n = 0) | -4[rbp] *------------------* 3800 -\u003e | | -8[rbp] *------------------* 3796 -\u003e | | -12[rbp] *------------------* 3792 -\u003e | | -16[rbp] *------------------* If you notice, the addresses feel inconsistent. The difference is oscillating between 4 and 8. That’s because a direct push is a shorthand for subtracting 8 bytes and moving a value at that memory. When we reserve 16 bytes separately, and they bytes are used to store an integer, they are 4-byte aligned for efficient memory access, as an integer is normally 4-bytes in size. This ASCII Art has stopped at (n = 0) as we have reached the base condition. Now the frames will remove one-by-one. Let’s see how that works."},"title":"recursion"},"/gitbook/docs/low-level-architecture/functions/return/":{"data":{"12-byte-struct#12-byte Struct":"#include #include struct Point { int x; int y; int z;}; struct Point make_pair(int a, int b, int c) { struct Point p; p.x = a; p.y = b; p.z = c; return p; } int main() { struct Point p = make_pair(2, 3, 4); printf(\"sizeof struct `p`: %d\\n\", sizeof(p)); } This is the assembly.\nmake_pair: push\trbp mov\trbp, rsp ; make a local copy of args (2, 3, 4) mov\tDWORD PTR -36[rbp], edi mov\tDWORD PTR -40[rbp], esi mov\tDWORD PTR -44[rbp], edx ; Copy them again mov\teax, DWORD PTR -36[rbp] mov\tDWORD PTR -24[rbp], eax mov\teax, DWORD PTR -40[rbp] mov\tDWORD PTR -20[rbp], eax mov\teax, DWORD PTR -44[rbp] mov\tDWORD PTR -16[rbp], eax ; Copy them again which is used in return mov\trax, QWORD PTR -24[rbp] mov\tQWORD PTR -12[rbp], rax mov\teax, DWORD PTR -16[rbp] mov\tDWORD PTR -4[rbp], eax mov\trax, QWORD PTR -12[rbp] ; (2, 3) in rax mov\tecx, DWORD PTR -4[rbp] ; 4 in rdx mov\trdx, rcx pop\trbp ret .LC0: .string\t\"sizeof struct `p`: %d\\n\" main: push\trbp mov\trbp, rsp sub\trsp, 16 mov\tedx, 4 mov\tesi, 3 mov\tedi, 2 call\tmake_pair ; Unpack mov\tQWORD PTR -12[rbp], rax mov\teax, DWORD PTR -4[rbp] and\teax, 0 or\teax, edx mov\tDWORD PTR -4[rbp], eax mov\tesi, 12 lea\trax, .LC0[rip] mov\trdi, rax mov\teax, 0 call\tprintf@PLT mov\teax, 0 leave ret This time, rax alone can’t help as it has already contained the two ints. So we use rdx to return the third int. The extra copying behavior is due to -00.\nThis would be the state of make_pair stack frame.\nrbp -4[rbp] \u003c-\u003e 4 -8[rbp] \u003c-\u003e -12[rbp] \u003c-\u003e 2, 3 -16[rbp] \u003c-\u003e 4 -20[rbp] \u003c-\u003e 3 -24[rbp] \u003c-\u003e 2 -28[rbp] \u003c-\u003e -32[rbp] \u003c-\u003e -36[rbp] \u003c-\u003e 2 -40[rbp] \u003c-\u003e 3 -44[rbp] \u003c-\u003e 4","16-byte-struct#16-byte Struct":"#include #include struct Point { int x; int y; int z; int s;}; struct Point make_pair(int a, int b, int c, int d) { struct Point p; p.x = a; p.y = b; p.z = c; p.s = d; return p; } int main() { struct Point p = make_pair(2, 3, 4, 5); printf(\"sizeof struct `p`: %d\\n\", sizeof(p)); } This is the assembly:\nmake_pair: push\trbp mov\trbp, rsp ; local copy of args (2, 3, 4, 5) mov\tDWORD PTR -20[rbp], edi mov\tDWORD PTR -24[rbp], esi mov\tDWORD PTR -28[rbp], edx mov\tDWORD PTR -32[rbp], ecx ; copy them for return mov\teax, DWORD PTR -20[rbp] mov\tDWORD PTR -16[rbp], eax mov\teax, DWORD PTR -24[rbp] mov\tDWORD PTR -12[rbp], eax mov\teax, DWORD PTR -28[rbp] mov\tDWORD PTR -8[rbp], eax mov\teax, DWORD PTR -32[rbp] mov\tDWORD PTR -4[rbp], eax ; export two 8-byte pointers mov\trax, QWORD PTR -16[rbp] mov\trdx, QWORD PTR -8[rbp] pop\trbp ret .LC0: .string\t\"sizeof struct `p`: %d\\n\" main: push\trbp mov\trbp, rsp sub\trsp, 16 mov\tecx, 5 mov\tedx, 4 mov\tesi, 3 mov\tedi, 2 call\tmake_pair mov\tQWORD PTR -16[rbp], rax mov\tQWORD PTR -8[rbp], rdx mov\tesi, 16 lea\trax, .LC0[rip] mov\trdi, rax mov\teax, 0 call\tprintf@PLT mov\teax, 0 leave ret Now we are smart enough to recognize that export two 8-byte pointers and unpack them in main is the strategy for 16-byte structs. Perfect.\nTime for final boss, structs ≥ 16 bytes.","8-byte-struct#8-byte Struct":"#include struct Pair { int x; int y; }; struct Pair make_pair(int a, int b) { struct Pair p; p.x = a; p.y = b; return p; } int main() { struct Pair q = make_pair(2, 3); return q.x + q.y; } Feels like we have implemented a class in C huh?\nThis is the assembly:\nmake_pair: push\trbp mov\trbp, rsp ; create a local copy of args (2, 3) mov\tDWORD PTR -20[rbp], edi mov\tDWORD PTR -24[rbp], esi ; Load the local copy at a different place for operation ; as the primary local copy is kept untouched unless specified mov\teax, DWORD PTR -20[rbp] mov\tDWORD PTR -8[rbp], eax mov\teax, DWORD PTR -24[rbp] mov\tDWORD PTR -4[rbp], eax ; The most important line ; We are loading 8-bytes in rax starting from -8[rbp] to -1[rbp] mov\trax, QWORD PTR -8[rbp] pop\trbp ret main: push\trbp mov\trbp, rsp sub\trsp, 16 mov\tesi, 3 mov\tedi, 2 call\tmake_pair ; Unpacking the 8-bytes into two separate 4-byte integers mov\tQWORD PTR -8[rbp], rax mov\tedx, DWORD PTR -8[rbp] ; -8, -7, -6, -5 represent 2 mov\teax, DWORD PTR -4[rbp] ; -4, -3, -2, -1 represent 3 ; return add\teax, edx leave ret This would be the state of make_pair stack frame:\nrbp -4[rbp] \u003c-\u003e 3 -8[rbp] \u003c-\u003e 2 -12[rbp] \u003c-\u003e -16[rbp] \u003c-\u003e -20[rbp] \u003c-\u003e 2 -24[rbp] \u003c-\u003e 3 The trick is that we load both the values in one single register. The question is how? Remember, interpretation rules memory access. As long as you interpret the right way, even garbage is gold.\nLets modify this program to see what is returned in rax .\n#include #include struct Pair { int x; int y; }; union PairBits { struct Pair p; unsigned long long bits; }; struct Pair make_pair(int a, int b) { struct Pair tmp = {a, b}; return tmp; } void print_binary64(uint64_t val) { for (int i = 63; i \u003e= 0; i--) { putchar((val \u003e\u003e i) \u0026 1 ? '1' : '0'); if (i % 8 == 0) putchar(' '); // group by bytes } putchar('\\n'); } int main() { union PairBits u; u.p = make_pair(2, 3); printf(\"sizeof union `u`: %d\\n\", sizeof(u)); printf(\"rax (hex) = 0x%016llx\\n\", u.bits); printf(\"rax (bin) = \"); print_binary64(u.bits); printf(\"q.x = %d, q.y = %d\\n\", u.p.x, u.p.y); } Lets understand this program first.\nWe can’t capture rax directly because it exist in only. But there are two way ways to do it:\nUsing union. Inline assembly. We are implementing the union way because we are not known to inline assembly yet.\nThe idea is simple, all the members in a union share the same memory starting at offset 0. So, u.p struct and u.bits are just two aliases for the same memory and the sizeof printf confirms that.\nLet’s talk about 0x%016llx .\n0x : literally prints 0x in front. % : start of format specifier. 0 : pad with zeroes instead of space. 16 : total width of output is 16 characters. Every hex bit represents 4 binary bits so 64 binary bits require 16 hex digits. ll : length modifier: long long (for 64-bit). x : print in hexadecimal lowercase. Since there is no builtin way to print binary bits, we created our own.\nLet’s run the program, we get:\n$ gcc main.c $ ./a.out sizeof union `u`: 8 rax (hex) = 0x0000000300000002 rax (bin) = 00000000 00000000 00000000 00000011 00000000 00000000 00000000 00000010 q.x = 2, q.y = 3 This is the proof that the compiler packed both the members of the struct in rax only.","conclusion#Conclusion":"What did we learn?\n-O0 is best to study the fundamentals, the bookish behavior. It is not for shipping the final product. In any of the cases, just change -O0 to 1 or 2 and you’ll start to notice how dangerous the assembly gets. Dangerous in terms of optimization. It shows that a group of assembly instructions can be replaced by one single assembly instruction as well. A lot of times, the most “complex looking things” boils down to simple hacks. And we are done with returning complex data.","how-returns-are-managed#How returns are managed?":"How returns are managed?5, 6 September 2025","premise#Premise":"Returns can be classified into 3 types:\nPrimitives (int, char, float, double): the return is in register (rax for int and char, xmm0 for floats) Arrays: they can’t be returned. Period. You pass a reference to a modifiable memory from the caller itself. To return a local declaration, you make it block static. Structures/Unions: this is where the problem is.","returning-structsunions--16-bytes#Returning Structs/Unions \u0026gt; 16 Bytes":"For structs greater than 16 bytes in size, the caller must allocate space for the return object and pass a hidden pointer to it as the first argument.\nThe callee writes the struct into that space and returns (with rax typically holding that pointer back again).\nBasically, we came back to square one. The caller has to pass a pointer, either pass it directly or indirectly.\nLike every single time, the caller would reserve space on stack, what changes is that this time, the caller will pass address to the start of memory reserve for stack in caller’s stack frame. The callee will use that pointer to populate the caller’s stack frame directly. And the callee returns the same address again. That’s it.\nThis is the source:\n#include #include struct Point { int x; int y; int z; int r; int s;}; struct Point make_pair(int a, int b, int c, int d, int e) { struct Point p; p.x = a; p.y = b; p.z = c; p.r = d; p.s = e; return p; } int main() { struct Point p = make_pair(2, 3, 4, 5, 6); printf(\"sizeof struct `p`: %d\\n\", sizeof(p)); } And the assembly:\nmake_pair: push\trbp mov\trbp, rsp ; local copy of arguments (2, 3, 4, 5, 6) mov\tQWORD PTR -40[rbp], rdi mov\tDWORD PTR -44[rbp], esi mov\tDWORD PTR -48[rbp], edx mov\tDWORD PTR -52[rbp], ecx mov\tDWORD PTR -56[rbp], r8d mov\tDWORD PTR -60[rbp], r9d ; another copy for return mgmt mov\teax, DWORD PTR -44[rbp] mov\tDWORD PTR -32[rbp], eax mov\teax, DWORD PTR -48[rbp] mov\tDWORD PTR -28[rbp], eax mov\teax, DWORD PTR -52[rbp] mov\tDWORD PTR -24[rbp], eax mov\teax, DWORD PTR -56[rbp] mov\tDWORD PTR -20[rbp], eax mov\teax, DWORD PTR -60[rbp] mov\tDWORD PTR -16[rbp], eax ; Updating the memory in main's stack frame (via rdi) mov\trcx, QWORD PTR -40[rbp]\t; save the address in caller's stack in rcx for easy access mov\trax, QWORD PTR -32[rbp]\t; load 8-bit pointers to 2 and 4 mov\trdx, QWORD PTR -24[rbp]\t; load 8-bit pointers to 2 and 4 mov\tQWORD PTR [rcx], rax ; dereference and populate mov\tQWORD PTR 8[rcx], rdx\t; dereference and populate mov\teax, DWORD PTR -16[rbp]\t; load 6 mov\tDWORD PTR 16[rcx], eax\t; copy 6 mov\trax, QWORD PTR -40[rbp]\t; prepare rax for return pop\trbp ret .LC0: .string\t\"sizeof struct `p`: %d\\n\" main: push\trbp mov\trbp, rsp sub\trsp, 32 lea\trax, -32[rbp]\t; address of struct on main's stack frame mov\tr9d, 6 mov\tr8d, 5 mov\tecx, 4 mov\tedx, 3 mov\tesi, 2 mov\trdi, rax ; the address is passed in rdi as the 1st arg call\tmake_pair ; printf mov\tesi, 20 lea\trax, .LC0[rip] mov\trdi, rax mov\teax, 0 call\tprintf@PLT mov\teax, 0 leave ret The state of stack:\nmake_pair: rbp -4[rbp] \u003c-\u003e -8[rbp] \u003c-\u003e -12[rbp] \u003c-\u003e -16[rbp] \u003c-\u003e 6 -20[rbp] \u003c-\u003e 5 -24[rbp] \u003c-\u003e 4 -28[rbp] \u003c-\u003e 3 -32[rbp] \u003c-\u003e 2 -36[rbp] \u003c-\u003e -40[rbp] \u003c-\u003e 3960 -44[rbp] \u003c-\u003e 2 -48[rbp] \u003c-\u003e 3 -52[rbp] \u003c-\u003e 4 -56[rbp] \u003c-\u003e 5 -60[rbp] \u003c-\u003e 6 main: 3992 \u003c-\u003e rbp 3988 \u003c-\u003e -4[rbp] \u003c-\u003e 3984 \u003c-\u003e -8[rbp] \u003c-\u003e 3980 \u003c-\u003e -12[rbp] \u003c-\u003e 3976 \u003c-\u003e -16[rbp] \u003c-\u003e 6 3972 \u003c-\u003e -20[rbp] \u003c-\u003e 5 3968 \u003c-\u003e -24[rbp] \u003c-\u003e 4 3964 \u003c-\u003e -28[rbp] \u003c-\u003e 3 3960 \u003c-\u003e -32[rbp] \u003c-\u003e 2","returning-structuresunions--16-bytes#Returning Structures/Unions \u0026lt;= 16 Bytes":"System V ABI decides return strategy based on size and fields.\nIf the return size is \u003c= 16 bytes, it is returned in registers. If the return size is \u003e 16 bytes, it is complicated."},"title":"return"},"/gitbook/docs/low-level-architecture/functions/stack/":{"data":{"anatomy-of-a-procedure#Anatomy Of A Procedure":"A procedure is composed of four core components:\nHeader (label) is the name of the function. Prologue (entry setup) represents the clever use of stack. Body represents the function body. Epilogue (cleanup and return)","body#Body":"Next comes the function body. It is made up of two parts.\nReservation on stack. Everything else (instructions and static allocation). Reservation on stack is a little tricky. This is where the concept of padding becomes important.\nStack pointer movement is word-aligned. Meaning, rsp always moves in units of the machine’s word size, which is 64-bit for us.\nBut, there are some special instructions (SIMD) which require the stack to be 16-bytes aligned.\nWhy? Checkout simd.md, but it is not required here, so you can do that later. So far, the stack is aligned. But now we have to reserve space for locals.\nTo keep rsp 16-bytes aligned, the total allocation on stack must be divisible by 16. As long as the total allocation on stack is 16 divisible, no padding is required. If that’s not the case, the allocation is rounded up to the next 16-divisible digit. If 100 bytes of locals were required, 112 bytes are reserved. The 12 bytes are for 16-bytes alignment.\nThere are leaf functions which are functions which don’t call any other functions inside them. For leaf functions, a concept called red zone exists in x64 System V ABI.\nRed zone is a small area of memory on the stack that a function can use for temporary storage without explicitly moving the stack pointer. The red zone is 128 bytes immediately below the rsp (stack pointer). The red zone is guaranteed to be safe, nothing will write there unexpectedly.","conclusion#Conclusion":"This sets up the foundation for the next exploration. Now we know how functions exist at basic. From here we can study:\nFunction arguments. How assembly sees function parameters. For clarity, parameters are declared in definition and arguments are passed in actual call. Although they are treated synonymous, they aren’t, at least at low level. Single return value. Returning a complex type (array, structure, union and pointer) Recursion Calling mechanisms: call by value and call by reference. Always keep in mind, low level systems aren’t black magic. They are complex but can be understood if approached the right way.\nBy the way, this article is polished multiple times, which proves that I didn’t learned all this in one run, nor even in consecutive runs. Although the head offers a count of dates, you can always check the GitHub. I am mentioning this because I don’t want you mistreat your overwhelm. It is genuine. I too have faced that.","epilogue#Epilogue":"It is about cleanup and return.\nleave ret Stack does not need a deallocation process. To free the memory, we just make it inaccessible and it is quite efficient. How does that work?\nTechnically, reducing rsp doesn’t clear the memory. The values are still there. The point is that there are so many processes running on a machine constantly. You mark a memory inaccessible, ends the program and another process quickly overwrites the stack memory. When we execute the leave instruction,\nIt restores the rsp by moving rbp into it. Remember, what is rbp pointing at? The old base pointer. When we do pop rbp it restores the old base pointer in rbp by popping rsp , moving its value into rbp and adding 8-bytes to rsp to point at the return address. At last we execute the ret instruction, which pops the return address into rip. And we are back into the old stack frame or old function context.","how-is-a-stack-frame-structured#How is a stack frame structured?":"Lets revise how user space memory is laid out. For more information, checkout virtual-memory-layout.md\nUser Space Memory Layout *--------------------------* | High Memory (~128 TiB) | | *-----------------* | | | Stack (↓) | | | *-----------------* | | | mmap region | | | *-----------------* | | | Free Space | | | *-----------------* | | | Heap (↑) | | | *-----------------* | | | Data (data/bss) | | | *-----------------* | | | Code | | | *-----------------* | | Low Memory (0..0) | *--------------------------* This is the general layout of a stack frame.\n*---------------------* | Function Arugments | \u003c-- [rbp+16], [rbp+24], .... | (beyond 6) | *---------------------* | Return Address | | (next ins in prev.) | \u003c-- [rbp+8] *---------------------* | Old Base Ptr Saved | \u003c-- [rbp]: old base pointer \u0026\u0026 rbp: new base pointer *---------------------* | Local Variables | \u003c-- [rbp-8], [rbp-16], .... *---------------------* | Empty Space | | (for alignment) | | (as required) | *---------------------* \u003c-- rsp The first 6 arguments go in registers, we know that. Checkout calling-conventions.md","how-procedures-are-set-up#How procedures are set up?":"Call to procedure Function prologue Body Function epilogue","how-stack-fits-in#How stack fits in?":"Close your eyes and imagine a stack of plates.\nThe last plate, which is on the top, is always taken out first. When more plates come after getting washed, they come on the top of the existing stack. Plates can’t be taken out from middle. Isn’t this the same as order and lifecycle of function calls?\nUntil the callee is not finished, caller can’t execute further. Until the plate on top is not taken, the bottom ones can’t be taken out. Now imagine a stack of office files, organized from high priority (top) to low priority (bottom), each containing a number of pages.\nEach file has a case study and each case has its own data collection and outcomes. Similarly, each function gets its own universe where its declarations and nested calls reside. Close your eyes one last time and imagine adding and removing plates from the stack.\nCan you see stack smoothly lining up with our priorities?\nThere is main which calls printf. A stack frame is created and the control is transferred to it. When printf is finished, the control naturally returns to main , without any extra managerial logic. When scanf is called, another stack frame is created and the control is transferred to it. And when it finishes, the control returns back main. Each stack frame is isolated, so the previous frame or the next frame can’t mess with it. I’ve deliberately kept it explicit and verbose so that we can have a broader overview of stack as a methodology, not just a data structure. I hope it was worth it.\nLet’s see how functions actually exist in assembly.","how-stack-qualifies-for-function-management#How stack qualifies for function management?":"Primarily there are 2 parts to function management.\nManaging the scope of a function. Managing the scope of nested function calls. Everyone has some experience with C. You create main function and call printf from stdio.h to print Hello, World!.\nprintf itself is a function, which uses other internal functions to perform the task it is meant to. We would not appreciate one function accessing the variables declared inside another function, right? But we want functions to access a limited part of other functions.\nThat’s the problem of scope. When we call printf from main , we would not appreciate main finishing before printf. We want it to wait.\nThat’s the problem of lifecycle. When one function calls another function, we want the callee function to return to the caller function, so that it knows it has finished and can continue its execution. Maybe we want to return something to the caller function as well.\nThat’s the problem of return context and return values.","introducing-procedures#Introducing Procedures":"In simple words, a procedure is just a code symbol with jump statements and some “clever” usage of stack.\nA procedure is a named, reusable block of code that performs a specific task, can accept input (arguments), has proper memory-management and returns a result.","introduction#Introduction":"IntroductionAugust 15 and 16, 2025\nSeptember 3, 2025\nEveryone knows stack as a data structure, and functions as reusable code blocks. So we are not going to repeat that theory.","management-pointers#Management Pointers":"The clever use of stack is about implementing stack frames and return context, which requires some general purpose registers, reserved for some specific purposes in the System V ABI.","prologue#Prologue":"As per System V ABI, rsp is guaranteed to be 16-bytes aligned.\nBefore calling a procedure, rsp is 16-bytes aligned. The call instruction is a shorthand for two instructions.\nPush the return address on stack, which is basically the next instruction in the current (caller’s) stack frame. This makes rsp misaligned by 8-bytes. Jump on the procedure’s header (label). After call , we push the base pointer of the caller’s stack frame on stack, this is used to return to the caller function. This instruction makes the rsp 16-bytes aligned again.\nAfter this, we setup the base pointer for the current stack frame. This is a little confusing so we will move a little slowly here.\nA push instruction is a shorthand for “reserve 8 bytes, then populate them”.\nWhen we push return address, the rsp now points to the memory where return address is stored.\nSimilarly, when we push rbp on stack, the rsp now points to the memory where rbp is stored.\nWe have pushed rbp on stack but the original value of rbp is still in rbp, right? When we do:\nmov rbp, rsp We are updating the base pointer. The new base or the stack frame starts from here.\nTake this:\nIf the rsp was 0d4000 before call, pushing the return address would subtract it to 0d3992. 0d3992 stores the return address. When we push rbp, the rsp is subtracted by 8 again and rbp goes on 0d3984. The stack also points at 0d3984 now. When we do mov rbp, rsp, rbp now stores 0d3984 . When you ask what is 0d3984, it would be the new base pointer. When you ask what is at 0d3984, it would be the old base pointer. I hope it is clear. These two instructions form the function prologue.\npush rbp mov rbp, rsp","shorthand-operations#Shorthand Operations":"A call instruction calls a procedure, which is shorthand for pushing the address of next instruction (rip) to stack and jumping to the procedure’s label, like this:\npush rip jmp label Although this is not exactly how it is done, but we can consider it like this on surface.\npush is a shorthand for:\nsub rsp, 8 mov [rsp], reg/imm pop is a shorthand for:\nmov reg, [rsp] add rsp, 8 leave restores the previous stack frame, which is a shorthand for:\nmov rsp, rbp pop rbp ret is a shorthand to take the return address from stack and put it into rip:\npop rip","stack-frame#Stack Frame":"A stack frame is chunk of stack that belongs to a single procedure call.\nWhen a function calls another function, a new stack frame is created and the instruction pointer register (rip) is adjusted by the CPU to point to the instruction in the new procedure.\nWhile the upper stack frame exists, the lower one can’t execute itself. Once the stack frame at top is done with its execution and it is killed, rip is adjusted again to continue where it has left.","the-basic-idea-behind-stack#The basic idea behind stack":"We know that memory is a flat-array. Stack approaches that memory sequentially.\nStack as a memory management technique works exactly like a stack of plates.\nThe first plate is at the bottom and every other plate comes above it (push). The last plate is the top one. When we take out plates, it happens from the top, not bottom (pop). You may question that a stack of plates grow upwards while the stack in memory grows downwards.\nActually, the addresses grow downwards with each push on stack. This feels counterintuitive because we don’t know how memory is structured. Checkout the process memory layout article for more information. It explains it the best. The whole addressable memory is not managed with stack. There are multiple techniques for multiple purposes.","the-ultimate-question-why-functions-in-c-return-only-one-value#The Ultimate Question: Why functions In C return only one value?":"I always have to use heap to return values. Why C is not like Python or JavaScript where your function can return multiple values?\nA simple value can be returned in a register. Multiple values require multiple registers, which would need complex ABI rules. Although the existing ABI conventions are no simpler but that’s all I have found.\nAlthough we can return a complex data structure like struct, but again, that changes the situation.","what-about-return-value#What about return value?":"As per the ABI, the return goes in the accumulator (rax).\nWhen you write raw assembly, you can technically return multiple things as long as you keep ABI rules in check. But when you are writing C, only one return value is possible.","what-makes-something-a-function#What makes something a function?":"A function has its own variable declarations and nested function calls. A function can receive arguments. A function has a return context. A function can return values to the caller. If something can explain functions at best, it’s code reusability.\nWe know that labels combined with jump statements help us achieve control flow.\nThe problem with jump statements is that they are absolute in nature.\nThere is no return context. If I have to return to the caller label, which called the callee label, the jump would be absolute. Meaning, I would return to the start of the caller label, not where the caller label had called that second label. This lack of context limits code reusability, which is paramount to functions.\nNow the question is, how can we create labels such that they can emulate a function?\nAnd the answer is stack."},"title":"stack"},"/gitbook/docs/low-level-architecture/mem-alloc/":{"data":{"":"","block-scope#Block Scope":"Any declaration inside a { } block is block scoped. Example: functions, if-else and loops.\nThe default storage class for block scoped declarations is auto.\nIt kinda resembles with local scope but that is too shallow because a block itself can contain other blocks. Examples:\nA function is a block that can contain an if block, which can contain another series of conditional blocks. A function can contain a for loop, which may contain nested if-else blocks. All in all, local scope is not a resilient mental model for low level work. That’s my opinion.","external-linkage#External Linkage":"The name refers to the same object across all the translation units.\nWhen a declaration is placed outside of any function, by default, it has the extern storage class, which makes a declaration accessible to the entire project (all the .c files).\nint PI = 3.14; int circumference(int r) { return 2 * PI * r; }","file-scope#\u003cstrong\u003eFile Scope\u003c/strong\u003e":"A declaration which is globally accessible within one translation unit only is considered to be a file scoped declaration.\nFrom build process, we know that the first step in processing a C source code is to expand preprocessing directives, where the toolchain (gcc, clang etc) just copies the headers files recursively until there is none.\nThe result of this process is what a translation unit is. Therefore, a TU is not something magical, just a sugar coating. For example, pie is a file scoped declaration.\n#include static float pie = 3.14; int main(void); Note: Don’t think about this code. The next write up would explore it in enough depth.","final-mental-model#Final Mental Model":"","internal-linkage#Internal Linkage":"In this translation unit, anyone referring to X would be referring to the X declared in file scope.\nstatic int PI = 3.14; int circumference(int r) { return 2 * PI * r; } int area(int r){ return PI*r*r; } Here, PI has internal linkage. Both the functions access the same object when referencing PI.","no-linkage#No Linkage?":"The name refers to an entity only in its scope. Example: any declaration inside a function scope.\nIf you use readelf and inspect the ELF structure of a binary, either you are going to find GLOBAL linkage or LOCAL linkage, which are external and internal linkages respectively. There is nothing as NO LINKAGE. And this is the reason why understanding a block scope static is so confusing. We are going to explore that in the next write up.","premise#Premise":"Memory for storage can be divided into 3:\nStack: the most straightforward thing you’ll ever come across. Static: .data/.bss Heap: for dynamic allocation. As long as we are talking about stack and static memory, we have to consult storage classes to find the appropriate place for allocation.","program-scope#Program Scope":"When a declaration exist until the program exist in memory and is available to be referenced by every translation unit, the declaration is program scoped.\nExample:\n#include float pie = 3.14; int main(void); Note: Again, don’t focus on the code.\nThe concept of scope is language enforced, which is why you will find different implementations of “scope”. What scope implies in C isn’t the same with Python because both the languages work differently.\nHow scopes behave in C is largely limited to C only and when you cross that boundary, the scenario changes. And we will see this practically as well.\nAt assembly level, the “concept of linkage” seems to be more prominent. Lets explore that.","storage-classes#Storage Classes":"Storage classes guide the compiler to manage static memory allocation.\nPrimarily there are 4 things associated with a variable.\nLocation: Where in the memory the variable would be stored? Our options: stack, registers, .data and .bss. Lifetime: How long the variable should exist (or be accessible)? Scope: Where that variable can be accessed from? Default State: Whether the variable has a default initial value when uninitialized or a garbage value? Storage class answers all of this.\nEvery variable has a storage class associated with it, but usually it is not visible.","the-concept-of-linkage#The Concept Of Linkage":"If I use the same identifier name in multiple scopes or translation units, do they refer to the same object, or different ones?\nThis is defined by linkage.","the-concept-of-scope#The Concept Of Scope":"Local and global scopes are too shallow to understand scope.\nIn simple words, scope defines where a declaration of an identifier can be referenced from (used).\nPrimarily there are 3 scopes.\nBlock Scope File Scope Global scope"},"title":"Memory Allocation"},"/gitbook/docs/low-level-architecture/mem-alloc/array/":{"data":{"array-decay#Array Decay":"Also know as array-to-pointer decay, is an automatic internal process that converts the array into a pointer to its first element. This conversion leads to the loss of array’s original size and dimension information.\nIt is essential for passing arrays to a function. Not just arrays but other complex types like structures and unions also decay into pointers when passed to function as an argument. Take this example:\n#include void takeArray(int arr[]){} void takeArrayPtr(int* arr){} int main(){ int arr[] = {41, 23, 94, 55}; takeArray(arr); } This is the assembly for both the functions.\ntakeArray: push\trbp mov\trbp, rsp mov\tQWORD PTR -8[rbp], rdi nop pop\trbp ret takeArrayPtr: push\trbp mov\trbp, rsp mov\tQWORD PTR -8[rbp], rdi nop pop\trbp ret main: push\trbp mov\trbp, rsp sub\trsp, 16 mov\tDWORD PTR -16[rbp], 41 mov\tDWORD PTR -12[rbp], 23 mov\tDWORD PTR -8[rbp], 94 mov\tDWORD PTR -4[rbp], 55 lea\trax, -16[rbp] mov\trdi, rax call\ttakeArray mov\teax, 0 leave ret There is no difference.","arrays#Arrays":"Arrays1, 2, 5 September 2025","conclusion#Conclusion":"Arrays are contiguous memory blocks.\nWhat changes is how the compiler allocates, aligns, and initializes them — influenced by constants, optimization flags, and ABI rules.\nOptimizations vary, but the core intent stays constant: manage memory predictably and efficiently.\nThank you.","example#Example":"#include int main(void){ int n; // Not known at compile-time scanf(\"%d\", \u0026n); int arr[n]; printf(\"%d\", arr[0]); } Normally integer is sized 4-bytes, so the total requirement is given by n*4 bytes.\nNow we have to calculate padding. Lets see how much padding is required for n ∈ {1....8}\nThis shows that the value for padding for a 4-byte integer belongs to {0, 4, 8, 12} . Plus, when the total bytes required are greater than the closest 16-divisible digit, we take the next 16-divisible digit.\nFrom this information, we can create a simple program to calculate the total bytes required.\n#include int main(void){ int n; printf(\"Enter n: \"); scanf(\"%d\", \u0026n); int l, u; int bytes = n * sizeof(int); if (bytes % 16 == 0){ printf(\"Voila... It is a multiple of 16!\\n\"); return 0; } else if (bytes % 16 \u003e 8){ u = bytes + ((bytes % 16) - 8); l = u - 16; } else{ l = bytes - ((bytes % 16)); u = l + 16; } printf(\"u: %d\\n\", u); printf(\"l: %d\\n\", l); printf(\"\\n Allocate %d bytes on stack.\\n\", u); } This program “efficiently” calculates how much n*sizeof(int) defers from a multiple of 16. And that’s basically the “intent” behind variable length allocation. We have to calculate how far we are from the next multiple of 16. Once we get this value, we can allocate space on stack.\nBy the way, this is just one way to do it. Let’s see how the compiler does it.\n.text .section\t.rodata .LC0: .string\t\"%d\" .text .globl\tmain .type\tmain, @function main: ; init push\trbp mov\trbp, rsp push\trbx sub\trsp, 40 mov\trax, rsp mov\trbx, rax\t; save rsp in rbx ; scanf(\"%d\", \u0026n) lea\trax, -36[rbp]\t; \u0026n mov\trsi, rax\t; arg2 = \u0026n lea\trax, .LC0[rip]\t; \"%d\" mov\trdi, rax\t; arg1 = \"%d\" mov\teax, 0 call\t__isoc99_scanf@PLT ; calculate total bytes required mov\teax, DWORD PTR -36[rbp] movsx\trdx, eax sub\trdx, 1 mov\tQWORD PTR -24[rbp], rdx cdqe lea\trdx, 0[0+rax*4] ; calculate padding required for 16-bytes alignment of rsp mov\teax, 16 sub\trax, 1 add\trax, rdx mov\tecx, 16 mov\tedx, 0 div\trcx imul\trax, rax, 16 ; reserve space for array sub\trsp, rax ; Make the base address of array (arr[0]) 4-byte aligned, if not mov\trax, rsp add\trax, 3 shr\trax, 2 sal\trax, 2 ; printf() mov\tQWORD PTR -32[rbp], rax mov\trax, QWORD PTR -32[rbp] mov\teax, DWORD PTR [rax] mov\tesi, eax\t; \u0026arr[0] lea\trax, .LC0[rip]\t; arg2 mov\trdi, rax\t; arg1 = arr[0] mov\teax, 0 call\tprintf@PLT ; restore rsp and return mov\trsp, rbx mov\teax, 0 mov\trbx, QWORD PTR -8[rbp] leave ret The compiler has employed another strategy to do this calculation.\nAdd 15 to the bytes required, we get (n + 15). Divide this by 16 and focus on quotient, we have to do (n+15)//16 . Multiply the quotient with 16 and you get the total bytes required. For example, take n = 5.\nn*sizeof(int) = 5*4 = 20 20+15 = 35 35//16 = 2 2*16 = 32 If you have trouble making sense of this, remember the ceil() and floor() functions.\nceil rounds up to the next integer while floor rounds down to the previous integer. ceil(5.6) would give 6 while floor(5.6) would give 5. If you notice, we are rounding in terms of 1. The algorithm above does the same thing except it rounds integers to the next multiple of 16.","example-1-declaration-only#Example 1: Declaration Only":"#include int main(void){ int arr[5]; } main: push\trbp mov\trbp, rsp mov\teax, 0 pop\trbp ret As expected, there is no reservation on stack.","example-2---declare-and-use-no-init#Example 2 - Declare and Use (No init)":"If you print the elements, you will find garbage values.","example-3---declaration--initialization#Example 3 - Declaration + Initialization":"#include int main(void){ int arr[5] = {1, 2, 3, 4, 5}; printf(\"Hello\\n\"); } We need 5*4 = 20 bytes on stack and the closest 16-bytes aligned round off for 20 is 32 so 32 bytes will be reserved on stack.\nmain: push\trbp mov\trbp, rsp sub\trsp, 32 mov\tDWORD PTR -32[rbp], 1 mov\tDWORD PTR -28[rbp], 2 mov\tDWORD PTR -24[rbp], 3 mov\tDWORD PTR -20[rbp], 4 mov\tDWORD PTR -16[rbp], 5 Note: If you do not mention 5 explicitly, the assembly generated is no different, which proves that the calculation for size allocation is done at compilation level.","example-4---empty-initialization#Example 4 - Empty Initialization":"#include int main(void){ int arr[5] = {}; } This is the assembly.\nmain: push\trbp mov\trbp, rsp sub\trsp, 32 pxor\txmm0, xmm0 movaps\tXMMWORD PTR -32[rbp], xmm0 movaps\tXMMWORD PTR -16[rbp], xmm0 The 3 instructions above are used to zero-initialize the 5 elements at runtime.\nWe could have used simply mov instructions or even xor to do the same thing, why these instructions then? First of all, we can definitely do that. If we use -mno-sse -mno-sse2 -mno-avx flags with gcc , we can see that the compiler now uses mov.\nmain: push\trbp mov\trbp, rsp sub\trsp, 32 mov\tQWORD PTR -32[rbp], 0 mov\tQWORD PTR -24[rbp], 0 mov\tQWORD PTR -16[rbp], 0 mov\tQWORD PTR -8[rbp], 0 If we change from arr[5] to arr[100] and keep these flags, we’d expect too many mov instructions. Let’s see.\nmain: push\trbp mov\trbp, rsp sub\trsp, 400 lea\trdx, -400[rbp] mov\teax, 0 mov\tecx, 50 mov\trdi, rdx rep stosq This was completely unexpected, isn’t it? And the best part is yet to come. I tried to compile the arr[5] code again and now I have a slightly different assembly. The number of mov instructions reduced.\nmain: push\trbp mov\trbp, rsp sub\trsp, 32 mov\tQWORD PTR -20[rbp], 0 mov\tQWORD PTR -12[rbp], 0 mov\tDWORD PTR -4[rbp], 0 Now the question is, what’s happening here?\nModern compilers are optimization monsters. They have evolved for decades and now they have too many tricks under their sleeves. You close one door and another is opened. Compilers search for the most efficient way to do something, and that depends on so many parameters. This is why there is always a possibility that two identical systems in an identical environment with the same compiler can generate a different assembly. The extent of difference also varies. The generated assembly has no guarantee to be the same, but the intent remains the same, always. That’s why I didn’t jumped on explaining what are pxor and movaps. I wanted to prove the point that understanding the intent is a far better strategy than understanding every optimization that the compiler can make to do the same thing. There is no end to it.\nSo, what’s the intent here?\nThe intent is to zero-initialize the array, efficiently. SIMD instructions are one way to do that. They zero multiple integers in parallel, reducing instruction count and improving throughput. Here, pxor clears the register, and movaps writes aligned 128-bit blocks. For now, it’s enough to know that SIMD zeroes multiple elements in parallel for efficiency. A deeper SIMD deep-dive deserves its own write-up. So, when we do empty initialization, the array is zero-initialized.\nNote: When we use an uninitialized array, the elements have garbage value, by default, in auto class, obviously.","example-5---incomplete-initialization#Example 5 - Incomplete Initialization":"We are declaring an array of 5 elements but we are not initializing all the 5 elements.\n#include int main(void){ int arr[5] = {1, 2, 3}; printf(\"Hello\\n\"); } If you printf the elements, you will find that 3rd and 4th positions are zeroed out. Let’s have a look at assembly.\nmain: push\trbp mov\trbp, rsp sub\trsp, 32 ; Zero-initialize the array pxor\txmm0, xmm0 movaps\tXMMWORD PTR -32[rbp], xmm0 movd\tDWORD PTR -16[rbp], xmm0 ; Initialize the positions from starting mov\tDWORD PTR -32[rbp], 1 mov\tDWORD PTR -28[rbp], 2 mov\tDWORD PTR -24[rbp], 3 An uninitialized array has garbage values but an initialized one should have values properly.\nWhen you initialize the array completely along the length, there is no need to zero before. Here we are doing partial initialization, which is why we need to zero-initialize the array so that each element has a proper value, then we are initializing the initial from starting positions with specified values.","premise#Premise":"Arrays are just consecutive blocks of memory interpreted together as a collection.\nWe use printf to make main a non-leaf function so that we don’t get confused with red zone.","questions-time#Questions Time":"Q1. What’s the purpose of saving rsp in rbx ? And why we are pushing rbx on stack?\nrbx is a callee-saved register. If the callee function want to use rbx, it has to preserve the state of rbx and return rbx in the same state to the caller function. That’s why it is pushed on stack. We are using it to preserve the state of rsp after reserving 40 bytes. Later it used in cleanup. Q2. Why inconsistent use of registers? When you need sign-extended value, why you are using eax? just use rax directly?\nCompiler optimization. Q3. How stack is cleaned up after usage?\nIt doesn’t. There are millions of processes and they use it very fast and dump it. And the next process overwrites the stack memory. Just reduce rsp and you are done. We just have to make memory inaccessible. That’s it. This is the reason why we sometimes get exactly what we expect but the next moment it vanishes because the stack memory mistakenly had that exact value from a previous process but soon someone else override it. An undefined behavior, basically.","starters#Starters":"We are already familiar with these concepts, so we just have to reinforce them for arrays.","static--extern-vla#Static \u0026amp;\u0026amp; Extern VLA":"Both are possible but require compile-time constant declaration. Because storage with static duration must be determined fully at compile time as memory layout is fixed before runtime.\nSimply put,\n// Outisde Functions int n = 5; int arr1[n]; static int arr2[n]; func(){ int m = 5; static int arr3[m]; } all of these are invalid.\nThe valid ones are these:\n// Outisde Functions const int n = 5; int arr1[n]; static int arr2[n]; func(){ const int m = 5; static int arr3[m]; }","steps-in-vla#Steps In VLA":"Allocate space for things defined at compile-time. Ensure n is populated, either at compile-time or runtime. Calculate the bytes required by type arr[n] declaration. Calculate the padding required for 16-bytes alignment for rsp. Allocate space for the array. Align the base address of the array, arr[0] to be 4-bytes.","variable-length-allocation#Variable Length Allocation":"This is the most important one here.\nIf you do:\nint n = 5; // Known at compile-time int arr[n]; int m; // Not known at compile-time scanf(\"%d\", \u0026m); int arr[m]; Both are identified as variable length allocation, even though n is known at compile time, the compiler triggers code for VLA.\nWhat makes VLA different is that you have to calculate the total bytes required to be allocated on stack.\nAs of now (September 2, 2025) I don’t know why the compiler emitted code for VLA when n is known at compile-time. Although the idea remains the same in both the cases, I’d suggest to keep the “not known at compile-time” case in reference because it would not make any sense in the other one. Since n is not known beforehand, we can’t allocate stack in one single instruction. We follow a structured process to allocate stack."},"title":"array"},"/gitbook/docs/low-level-architecture/mem-alloc/data-types/":{"data":{"conclusion#Conclusion":"This is all about primitive data types.\nThere are complex data types like enum, structure and union, which we will talk about later.","data-types#Data Types":"Data TypesAugust 18, 2025","general-knowledge#General Knowledge":"","how-data-types-are-translated#How Data Types Are Translated?":"A char is 1-byte so it aligns perfectly with the idea of byte-addressable memory. The problem comes with int float double etc.\nA single byte can represent 256 combination of numbers.\nSigned: -128 to +127 Unsigned: 0 to 255 A group of 4 bytes means 32-bits together, which can represent a large sum of values.\nIf we have to store 6, how it is stored?\nA single byte is more than enough to represent 6. It would be 00000110. What about other 3 bytes? Just zero them and you get a 32-bit wide value for 6. Something like this: 00000000000000000000000000000110 . What does this imply?\nThis implies that a general int requires 32-bits or 4-bytes of space. So, a variable declaration of type int would require 4-bytes to be reserved.\nBut memory is byte addressable. That means, a continuous block of 4-bytes is required to store an integer. And you have to interpret those 4-byte together in order to interpret them rightly. Same with float and double.\nWhat about arrays?\nAn array is a buffer in real sense. You have a group of bytes that are distinct enough that they exist as individual units but they are a part of one single entity.\nFor example, take an array of 10 elements.\nIn simple words, it is a group of 10 logical units where each logical unit is a sequence of bytes interpreted as one single entity. An array of 10 integers is a contiguous block of memory split into 10 logical units, where each unit is 4 bytes. Each 4-byte unit, when interpreted together as an int, they represent a single number. Therefore, the size of an array is the size of the type of data it contains multiplied by the number of elements.\nSo, what are data types?\nA data type is a contract between the programmer and the compiler that defines how a sequence of bits in memory should be interpreted and what operations are valid on it. This is why interpretation is so important. A group of bytes can be represented in a variety of ways and each way changes the meaning. For example:\n16 bytes can be interpreted as 16 distinct characters, 4 integers or 2 doubles. And these 16/4/2 can exist as distinct variables or a part of an array of the same type.","how-much-do-they-weigh#How Much Do They Weigh?":"Size is architecture specific and we are talking about 64-bit only.\nchar is generally 1-byte across all architectures and implementations while int is implementation depended. Although the common size of int is 4-bytes on 64-bit but there is a complete header file called inttypes.h which provides so many int.","major-data-types-in-c#Major Data Types In C":"Note: void is only meaningful in functions and pointers."},"title":"data-types"},"/gitbook/docs/low-level-architecture/mem-alloc/multidimensional-array/":{"data":{"accessing-elements#Accessing Elements":"This one is a little tricky as there are two ways to access the elements:\nRow-major is the standard way and cache-friendly way to access elements. It is the default one. Column-major is the non-standard way and non-cache-friendly way to access elements. The talk about “standard access mode” is also language dependent.\nRow-major is visually easy and aligns with how we interpret matrices in general, while column-major has its own use cases, primarily in scientific computing and advance mathematics. Many mainstream languages default to row-major for that reason, like C/C++, Java, Python. On the other hand, languages like Fortran, MATLAB and Julia prioritizes column-major. This is the code for accessing elements in both ways:\n#include int main(void){ // 2D Array int arr[2][3] = { {1, 2, 3}, {4, 5, 6} }; // Row-Major for (int i = 0; i \u003c 2; i++){ for (int j = 0; j \u003c 3; j++){ printf(\"%d, \", arr[i][j]); } printf(\"\\n\"); } printf(\"\\n\"); // Column-Major for (int i = 0; i \u003c 3; i++){ for (int j = 0; j \u003c 2; j++){ printf(\"%d, \", arr[j][i]); } printf(\"\\n\"); } printf(\"\\n\"); } In terms of normal mathematics, it is just matrix transformation.\nNote: Elements are stored row-major only. We can’t change that.","address-calculation#Address Calculation":"For 1D arrays, the formula to calculate the address of any position was:\naddr_Of_i = base_addr + sizeof(type)*i For example, if an array has 10 integers, the base address is 10000, the address of 7th element (0-based indexing) would be: 10000 + 4*6 = 10024 We can verify if the formula works by laying down all the addresses. 10000 10004 10008 10012 10016 10020 10024 10028 10032 10036 0 1 2 3 4 5 6 7 8 9 For multidimensional arrays, the formula to calculate the address of a position is different for both row-major and column-major.\nLets derive the formula for row-major order.\nint arr[2][2] = { {1, 2}, {3, 4} }; 10000 10004 10008 10012 ROW 0 0 1 1 COLUMN 0 1 0 1 To jump to the next row, we have to traverse the total number of elements in one row multiplied by the size of their type, i.e n(cols)*sizeof(int) To jump to the next element within the same row, we have to add the column offset.\nTo traverse every element in 0th row, we have to do:\nbase_addr + col_off To jump to the next row, we have to go past every element in the current row, which is given by the number of columns and then add the offset for elements in the next row.\nbase_addr + n(cols)*4 + col_off*4 To jump to the ith row, we have to go past (i-1) rows, where each row would have n(cols) elements, that means, we have to go past i*cols*4 to reach the next row. Then we add the column offset multiplied by size. So, the final formula becomes:\nbase_addr + (ith_row * n(cols) * 4) + (column_offset + 4) In simple terms,\naddr_of_i = ( (i * N_COLS) + j ) * 4 Lets test this formula.\n#include #define ROWS 2 #define COLS 3 int main(void){ int arr[ROWS][COLS] = { {1, 2, 3}, {4, 5, 6} }; int* base = \u0026arr[0][0]; for (int i = 0; i \u003c ROWS; i++){ for (int j = 0; j \u003c COLS; j++){ printf(\"%d, \", *(base + (i*3 + j)) ); } } } We are not multiplying by 4 because the pointer is of type int and dereferencing scales to the size of the type automatically.","general-trivia#General Trivia":"Multidimensional arrays are like matrices in mathematics. For example:\n// array[N_ROWS][N_COLUMNS] // A 2D Array or 2X2 Matrix int arr1[2][2] = { {1, 2}; {3, 4} }; Note: You can’t leave both ROWS and COLUMNS empty. Specifying the number of columns is necessary.\nAt assembly level, they are no different than a normal array. This is the assembly for the above code.\nsub\trsp, 16 mov\tDWORD PTR -16[rbp], 1 mov\tDWORD PTR -12[rbp], 2 mov\tDWORD PTR -8[rbp], 3 mov\tDWORD PTR -4[rbp], 4 From this, we can infer that elements are stored row-wise, i.e arr[0][0], arr[0][1], arr[1][0], arr[1][1] .\nThe size for stack allocation is calculates using this formula: n(rows)*n(columns)*sizeof(type) .\nFor the above example, that would be 2*2*4 = 16 bytes. We can also verify the formula by individually calculating the size. There are 4 integers, each of size 4, so 16 bytes are required. So, no block magic.","multidimensional-array#Multidimensional Array":"Multidimensional Array"},"title":"multidimensional-array"},"/gitbook/docs/low-level-architecture/mem-alloc/pointers/":{"data":{"example---integer#Example - Integer":"#include int main(){ int a = 48; int* p = \u0026a; } .text .globl\tmain .type\tmain, @function main: push\trbp mov\trbp, rsp ; int a mov\tDWORD PTR -12[rbp], 48 ; int* p = \u0026a lea\trax, -12[rbp] mov\tQWORD PTR -8[rbp], rax mov\teax, 0 pop\trbp ret There is no stack reservation because main() is not calling any function so it is a leaf function here and by default, it receives a 128-bytes of red zone as per x68 System V ABI.\nThe same result is seen for char, float and double except that you can notice some extra scaffolding in case of float and double because they are pretty complex in themselves. But we need not to worry about that.","general-trivia#General Trivia":"A pointer is a variable that stores a memory address.\nThe size of the pointer is architecture dependent. So, in 64-bit Linux, it is 8-bytes.\nThe pointer’s type defines what it points to, not the storage size. For example:\nint* iptr; char* cptr; Both the integer and character pointers are sized 8-bytes but they point to different values. The integer pointer points to a 4-byte value (general) while the character pointer points to a 1-byte value.\nThe type of pointer decides how the memory would be interpreted.\nAt assembly level, there is no dedicated “pointer type”. Every symbol you create becomes a reference in the memory at runtime.","pointer-dereferencing#Pointer Dereferencing":"","pointers#Pointers":"Pointers"},"title":"pointers"},"/gitbook/docs/low-level-architecture/mem-alloc/primitive-types/":{"data":{"conclusion#Conclusion":"Even a variable declaration is not that straightforward.\nThis completes primitive data allocation. Next is complex data allocation.\nIt is overwhelming and I won’t deny that. Take your time and enjoy.","experiment-1-function-scope-and-default-storage-class#Experiment 1: Function scope and default storage class":"#include int main(void){ int a; } Expectation: An integer is sized 4-bytes but that makes rsp misaligned, so we are expecting the compiler to reserve 16 bytes on stack.\nReality: Function prologue and epilogue. No allocation on stack.\nChange: Maybe the previous program was too short. So, we have added a printf.\n#include int main(void){ int a; printf(\"Hi!\\n\"); } Expectation: Same.\nReality: No allocation on stack.\nChange: Lets use this declaration somewhere. Lets take user input.\n#include int main(void){ int a; printf(\"Enter a: \"); scanf(\"%d\", \u0026a); } Expectation: Same.\nThere you go. We have it.\nmain: push\trbp mov\trbp, rsp sub\trsp, 16\t; \u003c- Here lea\trax, .LC0[rip] mov\trdi, rax mov\teax, 0 call\tprintf@PLT lea\trax, -4[rbp] mov\trsi, rax lea\trax, .LC1[rip] mov\trdi, rax mov\teax, 0 call\t__isoc99_scanf@PLT mov\teax, 0 leave ret Change: What if we “declare + initialize”, instead of user input?\n#include int main(void){ int a = 45; } main: push\trbp mov\trbp, rsp mov\tDWORD PTR -4[rbp], 45 mov\teax, 0 pop\trbp ret Here, we are not subtracting to reserve space. Instead, we are using rbp as a reference point (for the current stack frame) and subtracting 4 bytes from there. Then we are storing 45 at the 4th block (byte).\nThe stack is not misaligned because we are moving rbp relative, not rsp relative. rsp is still 16-bytes aligned.\nCompiler optimization, you know! Doesn’t this behavior makes it hard for accessing the value? Let’s see.\n#include int main(void){ int a = 45; printf(\"%d\\n\", a); } Expectation: Extra work to access a due to this “so called optimization”.\nmain: push\trbp mov\trbp, rsp sub\trsp, 16 mov\tDWORD PTR -4[rbp], 45 mov\teax, DWORD PTR -4[rbp] mov\tesi, eax lea\trax, .LC0[rip] mov\trdi, rax mov\teax, 0 call\tprintf@PLT mov\teax, 0 leave ret Result: Now it is using subtraction.","experiment-2-outside-function-scope-and-default-storage-class#Experiment 2: Outside function scope and default storage class":"#include int BASE; int main(void){} Expectation: Since it is uninitialized, it should be zero-initialized at runtime and declared in .bss.\nReality: Indeed.\n.text .globl\tBASE .bss .align 4 .type\tBASE, @object .size\tBASE, 4 PI: .zero\t4 We can use readelf to check its linkage as we are not using .c multiple files so there is no other way to verify if it is “globally” available or not.\n$ readelf ./out --symbols | grep BASE 31: 0000000000004014 4 OBJECT GLOBAL DEFAULT 25 BASE Verified. Change: Initialize it.\n#include int BASE = 16; int main(void){} Expectation: Now the declaration should be in .data.\nReality: Indeed.\n.text .globl\tBASE .data .align 4 .type\tBASE, @object .size\tBASE, 4 PI: .long\t16","experiment-3-outside-function-scope-and-static-class#Experiment 3: Outside function scope and `static` class":"#include static int BASE; int main(void){} Expectation: Internal linkage and a declaration in .bss.\nReality: Indeed.\n.text .local\tBASE .comm\tBASE,4,4 You might find it different from previous ones. There is no PI: as a label. And, as of now (29 August 2025), I have no answer for that.\n#include static int BASE = 16; int main(void){} Expectation: In .bss.\nReality: Indeed.\n.data .align 4 .type\tBASE, @object .size\tBASE, 4 PI: .long\t16","experiment-4-function-scope--static--the-final-boss#Experiment 4: Function scope + `static` :: The Final Boss":"#include int main(void){ static int num; } Expectation: In .bss.\nReality: Indeed.\n.local num.0 .comm num.0,4,4 #include int main(void){ static int num = 45; } Expectation: In .data.\nReality: Indeed.\n.data .align 4 .type\tnum.0, @object .size\tnum.0, 4 num.0: .long\t45 The question is, how the lifetime is increased but the scope remains block level? This is the only edge case here.\nThe answer is, there is no such “program lifespan but block scope” thing. It’s an abstraction. The only true scopes are program level scope, file level scope and block level scope. Just as “no CGI” is just “very good CGI”, this is also a perfectly crafted illusion. And the best part is, we can break that illusion ourselves. Let’s try to find the answer.\nTo understand this, we have to understand how scopes are enforced.\nThe way .local and .global directives work is that they affect the symbol’s “linker visibility” attribute. .global makes the symbol visible to the linker while .local doesn’t.\nYou don’t want a block scope symbol and a file scope symbol to be available outside of the current translation unit, which is why “no linkage” and “internal linkage” both uses LOCAL as visibility. If you were given an unstripped binary (because stripped ones don’t have .symtab), you can’t tell if a LOCAL symbol is a file scope static or a block scope static. Making a block scope declaration static is a rule enforced at compilation level.\nFirst, the code undergoes lexical analysis. Then abstract syntax tree formation. Next comes semantic analysis, where the magic happens. An internal symbol table is created using the AST, and that table enforces this rule. It sees that this variable requires allocation in static storage but the scope has to be kept block. So, it doesn’t explicitly allows any code that refers to that declaration because it is not available outside. But once you’re done with compilation and reach assembly, there is no such thing. And we are going to prove this point. There are 2 ways in which we can prove out point and we are going to explore both. Although it’s not necessary as the approach remains the same, but there is a very-very small difference which we must be aware of.","inside-function-declarations#Inside Function Declarations":"// main.c #include int main(void){ int BASE = 16; // auto, by default auto int BASE = 16; static int BASE = 16; // Lifetime is changed to \"until the program exist\" // But the scope is still block-level extern int BASE = 16; // Raises error; A block scope declaration can't be made extern // But we can refer to an already existing variable with external linkage/global visibility extern int global_BASE; // refers to the declaration in another.c } // another.c int global_BASE = 16; // extern, by default","method-1#Method 1":"Take this example we have used before:\n#include int sq(int n, int flag){ static int ncalls = 0; if (flag != 1){ ncalls++ ; printf(\"sq(%d) = %d\\n\", n, n*n); return ncalls; } if (flag == 1){ return ncalls; } return -1 } int main(){ sq(2, 0); sq(3, 0); sq(4, 0); sq(5, 0); printf(\"Number of calls made to square function: %d\\n\", sq(1, 1)); return 0; } This is the assembly.\n.text .globl\tsq .type\tsq, @function sq: push\trbp mov\trbp, rsp mov\tDWORD PTR -4[rbp], edi\t; arg1 saved on stack mov\tDWORD PTR -8[rbp], esi\t; arg2 saved on stack ; flag != 1 check cmp\tDWORD PTR -8[rbp], 1 je\t.L2 ; continue downwards if not 1 ; increment ncalls mov\teax, DWORD PTR ncalls.0[rip] add\teax, 1 mov\tDWORD PTR ncalls.0[rip], eax ; setup return value mov\teax, DWORD PTR -4[rbp] imul\teax, eax jmp\t.L3\t; jump to return Label ; (flag == 1) Label .L2: cmp\tDWORD PTR -8[rbp], 1 jne\t.L4 mov\teax, DWORD PTR ncalls.0[rip] jmp\t.L3 ; setup return value == -1 .L4: mov\teax, -1 ; return .L3: pop\trbp ret ; %d string for printfs .section\t.rodata .LC0: .string\t\"%d\\n\" .text .globl\tmain .type\tmain, @function main: push\trbp mov\trbp, rsp mov\tesi, 0 mov\tedi, 2 call\tsq\t; sq(2, 0) mov\tesi, eax\t; arg2 for printf lea\trax, .LC0[rip] mov\trdi, rax\t; arg1 for printf mov\teax, 0 call\tprintf@PLT\t; printf for sq(2, 0) mov\tesi, 0 mov\tedi, 3 call\tsq\t; sq(3, 0) mov\tesi, eax\t; arg2 for printf lea\trax, .LC0[rip] mov\trdi, rax\t; arg1 for printf mov\teax, 0 call\tprintf@PLT\t; printf for sq(3, 0) mov\tesi, 0 mov\tedi, 4 call\tsq\t; sq(4, 0) mov\tesi, eax\t; arg2 for printf lea\trax, .LC0[rip] mov\trdi, rax\t; arg1 for printf mov\teax, 0 call\tprintf@PLT\t; printf for sq(4, 0) mov\tesi, 0 mov\tedi, 5 call\tsq\t; sq(5, 0) mov\tesi, eax\t; arg2 for printf lea\trax, .LC0[rip] mov\trdi, rax\t; arg1 for printf mov\teax, 0 call\tprintf@PLT\t; printf for sq(4, 0) mov\tesi, 1 mov\tedi, 1 call\tsq\t; sq(1, 1) mov\tesi, eax\t; arg2 for printf lea\trax, .LC0[rip] mov\trdi, rax\t; arg1 for printf mov\teax, 0 call\tprintf@PLT\t; printf for sq(1, 1) mov\teax, 0 pop\trbp ret ; ncalls declaration .local\tncalls.0 .comm\tncalls.0,4,4 I have added comments to understand the flow better.\nEvery printf is taking 2 arguments. First one is the %d string and the second one is the actual integer to be printed. The second argument is the return value from the sq function call, via eax. If you are using VS Code, you can select all the mov esi, eax lines and replace them with mov, esi, DWORD PTR ncalls.0[rip] to see a magic. The assembly is perfectly assembled and linked. And the output is completely transformed. But we shouldn’t be “allowed” to access ncalls, right?\ngcc main.s -o out ./out 1 2 3 4 4 Voila. This proves our point that lifetime + block scope is just a rule, not a hardly imposed impossibility, which is why it can be bypassed at assembly level.\nThose who may ask that the compiler is using stack to save the arguments temporarily but never reserving space for them, the answer is that sq is a leaf function for which there exist 128 bytes of red zone just below the rsp, for temporary use without reserving space.\nWe can use -mno-red-zone to remove red zone support and there would be stack allocation again.","method-2#Method 2":"Via hidden symbols\nIDE like VS Code have language server protocol, which is basically a real time parser of the source code and verifies it against the language rules. If there are anomalies, it flags them. There is nothing magical.","outcomes-of-e-1#Outcomes Of E-1":"Any allocation in block scope goes on stack by default. In case of uninitialized declarations, if the declaration is not used later in the program, the compiler has no incentive to reserve space for it. rsp is subtracted 16-bytes aligned to reserve space. rbp is used as a stable pointer to reference allocations inside a stack frame.","outcomes-of-e-2#\u003cstrong\u003eOutcomes Of E-2\u003c/strong\u003e":"A global declaration has external linkage and always exist in memory, unlike block scope declarations which require usage.","outside-function-declarations#Outside Function Declarations":"#include int BASE = 16; // extern, by default auto int BASE = 16; // Not allowed static int BASE = 16; // Makes the variable File Scoped. extern int BASE = 16; // Raises a warning as there is no need to explicitly say 'extern' int main(void);","practical-view#Practical View":"Let’s do some experiments to understand storage classes, the concept of scope and linkage.\nWe will use this command to compile our source to assembly.\ngcc ./main.c -S -O0 -fno-asynchronous-unwind-tables -fno-dwarf2-cfi-asm -masm=intel This ensures that we see intel syntax, no optimization and no cfi* directives, just pure assembly. You can use godbolt.org as well.","primitive-types#Primitive Types":"Primitive Types27, 28, 29, 30 August 2025","theoretical-view#Theoretical View":"We can classify memory allocation in terms of function scope and file scope.\n#include // FILE SCOPE int another_function(){ // FUNCTION SCOPE return something } int main(void){ // FUNCTION SCOPE } In both of these scopes, we can have:\nDeclaration only.\nint num; char gen; Declaration + Initialization.\nint num = 45; char gen = 'M';","what-is-the-use-of-static-in-block-scope#What is the use of `static` in block scope?":"#include int main(void){ static int BASE = 16; } This increases the lifetime of a variable but it remains block scoped. It makes the variable a “private global”.\nTake this:\n#include int sq(int n, int flag){ static int ncalls = 0; if (flag != 1){ ncalls++ ; return n*n; } if (flag == 1){ return ncalls; } return -1; } int main(){ printf(\"%d\\n\", sq(2, 0)); printf(\"%d\\n\", sq(3, 0)); printf(\"%d\\n\", sq(4, 0)); printf(\"%d\\n\", sq(5, 0)); printf(\"%d\\n\", sq(1, 1)); return 0; } Here ncalls is a static variable, so, its state is retained in every function call, instead of being allocated every single time, which is why the last printf prints 4.\nWait, the lifetime is increased but the scope remains the same, how does that work?\nThis is the most complicated case and we are going to talk about it soon."},"title":"primitive-types"},"/gitbook/docs/low-level-architecture/mem-alloc/structures/":{"data":{"auto#Auto":"#include struct Point { int x; int y; }; int main(void){ struct Point p1; // No Initialization p1.x = 6; p1.y = 5; printf(\"Hello.\\n\"); } This is the assembly.\nmain: push\trbp mov\trbp, rsp sub\trsp, 16 mov\tDWORD PTR -8[rbp], 6 mov\tDWORD PTR -4[rbp], 5 lea\trax, .LC0[rip] mov\trdi, rax call\tputs@PLT mov\teax, 0 leave ret","block-static#Block Static":"#include struct Point { int x; int y; }; int main(void){ static struct Point p1; p1.x = 6; p1.y = 5; printf(\"Hello.\\n\"); } main: push\trbp mov\trbp, rsp mov\tDWORD PTR p1.0[rip], 6 mov\tDWORD PTR p1.0[rip+4], 5 lea\trax, .LC0[rip] mov\trdi, rax call\tputs@PLT mov\teax, 0 pop\trbp ret .local\tp1.0 .comm\tp1.0,8,8","conclusion#Conclusion":"At the end of the day, everything boils down to stack discipline. Once you understand stack management, everything else just builds upon that.","extern#Extern":"#include struct Point { int x; int y; }; struct Point p1; int main(void){ p1.x = 6; p1.y = 5; printf(\"Hello.\\n\"); } .text .globl\tp1 .bss .align 8 .type\tp1, @object .size\tp1, 8 p1: .zero\t8 .section\t.rodata .LC0: .string\t\"Hello.\" .text .globl\tmain .type\tmain, @function main: push\trbp mov\trbp, rsp mov\tDWORD PTR p1[rip], 6 mov\tDWORD PTR p1[rip+4], 5 lea\trax, .LC0[rip] mov\trdi, rax call\tputs@PLT mov\teax, 0 pop\trbp ret","file-static#File Static":"#include struct Point { int x; int y; }; static struct Point p1; int main(void){ p1.x = 6; p1.y = 5; printf(\"Hello.\\n\"); } .text .local\tp1 .comm\tp1,8,8 .section\t.rodata .LC0: .string\t\"Hello.\" .text .globl\tmain .type\tmain, @function main: push\trbp mov\trbp, rsp mov\tDWORD PTR p1[rip], 6 mov\tDWORD PTR p1[rip+4], 5 lea\trax, .LC0[rip] mov\trdi, rax call\tputs@PLT mov\teax, 0 pop\trbp ret","internal-padding-in-structs#Internal Padding In Structs":"Padding and alignment is very important to understand in structures.\nSo far, we only had integer members in the structure. When we add more members, of different types, the dynamics change.\nFor example:\nstruct Point { int x; int y; char gen; int* memo; }; int main(){ int num = 88; struct Point P; P.x = 4; P.y = 5; P.gen = 'M'; P.memo = # printf(\"Hello\\n\"); } The assembly is this:\n.LC0: .string\t\"Hello\" main: push\trbp mov\trbp, rsp sub\trsp, 32 mov\tDWORD PTR -4[rbp], 88\t; num mov\tDWORD PTR -32[rbp], 4\t; P.x mov\tDWORD PTR -28[rbp], 5\t; P.y mov\tBYTE PTR -24[rbp], 77\t; P.gen (ASCII value of 'M') lea\trax, -4[rbp]\t; addr of -4[rbp], which is n mov\tQWORD PTR -16[rbp], rax\t; P.memo lea\trax, .LC0[rip] mov\trdi, rax call\tputs@PLT mov\teax, 0 leave ret We need 21 bytes so 32 are reserved on stack.\nLet’s have a look at stack layout.\nrbp *---------* -04, -03, -02, -01 | num 88 | *---------* -08, -07, -06, -05, | Empty | | Padding | *---------* -16, -15, -14, -13, | P.memo | -16[rbp] (8-byte) -12, -11, -10, -09 *---------* *---------* -24, -23, -22, -21, | P.gen | -24[rbp] (1-byte + 7-byte for padding) -20, -19, -18, -17 *---------* *---------* -28, -27, -26, -25 | P.y | -28[rbp] (4-byte) *---------* -32, -31, -30, -29 | P.x | -32[rbp] (4-byte) *---------* If we print the size of struct, it is 24 bytes.\n4 (x), 4(y), 8(memo) and 8(gen). The padding within the struct is measured according to the biggest type member. Here, it is pointer, which requires 8-bytes of space. x and y together keeps it 8-byte aligned but gen breaks that so 7-bytes of padding is given to it.","introduction-to-structures#Introduction To Structures":"A structure is just a contiguous block of memory that groups different variables under one name.\n#include struct Point { int x; int y; }; int main(void){ struct Point p1; // No Initialization p1.x = 6; p1.y = 5; struct Point p2 = {6, 6}; // Complete Initialization struct Point p3 = {}; // Empty Initialization: Zero-initialize struct Point p4 = {7}; // Incomplete Initialization: The rest is automatically zeroed. } The struct declaration alone doesn’t reserve space. It exist compilation level only. When you create a variable out of it, that’s when storage is reserved.\nLet’s setup the old baseline, before we move any further.","pointer-to-struct#Pointer To Struct":"#include struct Point { int x; int y; }; int main() { struct Point p = {10, 20}; struct Point *ptr = \u0026p; ptr-\u003ex = 30; ptr-\u003ey = 40; printf(\"%d %d\\n\", p.x, p.y); } This is the assembly:\n.LC0: .string\t\"%d %d\\n\" main: push\trbp mov\trbp, rsp sub\trsp, 16 mov\tDWORD PTR -16[rbp], 10 mov\tDWORD PTR -12[rbp], 20 lea\trax, -16[rbp] mov\tQWORD PTR -8[rbp], rax mov\trax, QWORD PTR -8[rbp] mov\tDWORD PTR [rax], 30 mov\trax, QWORD PTR -8[rbp] mov\tDWORD PTR 4[rax], 40 mov\tedx, DWORD PTR -12[rbp] mov\teax, DWORD PTR -16[rbp] mov\tesi, eax lea\trax, .LC0[rip] mov\trdi, rax mov\teax, 0 call\tprintf@PLT mov\teax, 0 leave ret Lets draw the initial state of stack.\n3992 \u003c-\u003e rbp \u003c-\u003e 3988 \u003c-\u003e -4[rbp] \u003c-\u003e 3984 \u003c-\u003e -8[rbp] \u003c-\u003e 3980 \u003c-\u003e -12[rbp] \u003c-\u003e 20 p.y 3972 \u003c-\u003e -16[rbp] \u003c-\u003e 10 p.x Let’s go instruction by instruction as too much movement in close proximity is happening.\nThese two instructions setup our struct pointer ptr which goes at -8[rbp] . lea rax, -16[rbp] ; obtain the address of p.x mov QWORD PTR -8[rbp], rax ; save it at -8[rbp] State of stack:\n3992 \u003c-\u003e rbp \u003c-\u003e 3988 \u003c-\u003e -4[rbp] \u003c-\u003e 3984 \u003c-\u003e -8[rbp] \u003c-\u003e 3972 ptr 3980 \u003c-\u003e -12[rbp] \u003c-\u003e 20 p.y 3972 \u003c-\u003e -16[rbp] \u003c-\u003e 10 p.x These two instructions obtain the address of p.x and update the value at it with 30.\nmov rax, QWORD PTR -8[rbp] mov DWORD PTR [rax], 30 State of stack:\n3992 \u003c-\u003e rbp \u003c-\u003e 3988 \u003c-\u003e -4[rbp] \u003c-\u003e 3984 \u003c-\u003e -8[rbp] \u003c-\u003e 3972 ptr 3980 \u003c-\u003e -12[rbp] \u003c-\u003e 20 p.y 3972 \u003c-\u003e -16[rbp] \u003c-\u003e 30 p.x These two instructions obtain the address of p.y and update the value at it with 40.\nmov rax, QWORD PTR -8[rbp] mov DWORD PTR 4[rax], 40 State of stack:\n3992 \u003c-\u003e rbp \u003c-\u003e 3988 \u003c-\u003e -4[rbp] \u003c-\u003e 3984 \u003c-\u003e -8[rbp] \u003c-\u003e 3972 ptr 3980 \u003c-\u003e -12[rbp] \u003c-\u003e 40 p.y 3972 \u003c-\u003e -16[rbp] \u003c-\u003e 30 p.x And we are done.","struct-in-function-arguments#Struct In Function Arguments":"Unlike arrays, a struct doesn’t decay into a pointer when passed to a function. Take this:\n#include struct Point { int x; int y; }; void taketPointPtr(struct Point *p){} void takePoint(struct Point p){} int main() { struct Point p = {10, 20}; printPoint(p); printPointPtr(\u0026p); } This is the assembly, which is same for both the functions.\ntakePointPtr: push\trbp mov\trbp, rsp mov\tQWORD PTR -8[rbp], rdi nop pop\trbp ret takePoint: push\trbp mov\trbp, rsp mov\tQWORD PTR -8[rbp], rdi nop pop\trbp ret main: push\trbp mov\trbp, rsp sub\trsp, 16 mov\tDWORD PTR -8[rbp], 10 mov\tDWORD PTR -4[rbp], 20 mov\trax, QWORD PTR -8[rbp]\t; loading a QWORD value mov\trdi, rax call\ttakePoint lea\trax, -8[rbp]\t; loading an address mov\trdi, rax call\ttakePointPtr mov\teax, 0 leave ret","struct-types-as-functions#Struct Types As Functions":"A struct definition like this:\nstruct Point{ int x; int y; }; can be used to create a function of its type like this:\nstruct Point takePoint(struct Point P){} If you want to avoid writing struct every time, use a type definition instead.\ntypedef struct{ int x; int y; } Point; Point takePoint(Point P); The reason is simple, a type definition helps you create user defined types, on the other hand, normal struct declarations are just variables, so they can’t be used in place of types.","structures#Structures":"Structures2,3, 5, 22 September 2025"},"title":"structures"},"/gitbook/docs/low-level-architecture/mem-alloc/type-definition/":{"data":{"type-definition#Type Definition":"Type DefinitionSeptember 07, 2025\nType definitions are used to create alias for existing data types. That’s it. Nothing fancy.\nFor example:\n#include typedef long long int LLint; int main() { LLint num = 5; long long int num1 = 5; } This is the assembly.\nmain: push\trbp mov\trbp, rsp mov\tQWORD PTR -8[rbp], 5 mov\tQWORD PTR -16[rbp], 5 mov\teax, 0 pop\trbp ret Both takes 8 bytes of space.\nAs you have guessed already, type definitions exist during compilation only."},"title":"type-definition"},"/gitbook/docs/low-level-architecture/mem-alloc/unions/":{"data":{"conclusion#Conclusion":"Only the last written member is valid — reading another member is undefined behavior.","introduction#Introduction":"Unions are like structures syntactically but they differ in terms of memory allocation.\nUnions are based on the concept of unified memory where the size of the union is the size of the biggest member it contains.\nEvery member access the same memory and only one allocation can last at a time.\nUnions are best understood with an example.\n#include union Point { int x; int y; }; int main() { union Point u; u.x = 2; u.y = 3; printf(\"%d, %d\\n\", u.x, u.y); printf(\"sizeof union `u`: %d\\n\", sizeof(u)); } This is the assembly.\nmain: push\trbp mov\trbp, rsp sub\trsp, 16 mov\tDWORD PTR -4[rbp], 2\t; stack usage only 4 bytes mov\tDWORD PTR -4[rbp], 3\t; same memory is updated mov\tedx, DWORD PTR -4[rbp] mov\teax, DWORD PTR -4[rbp] mov\tesi, eax lea\trax, .LC0[rip] mov\trdi, rax mov\teax, 0 call\tprintf@PLT mov\tesi, 4\t; size of union is 4 , not 8 lea\trax, .LC1[rip] mov\trdi, rax mov\teax, 0 call\tprintf@PLT mov\teax, 0 leave ret Here, we updated the same location when we try to populate u.y. The size of the union is also 4 bytes only.\nLet’s take another example.\n#include union Point { int x; int y; int* ptr; }; int main() { int num = 5; union Point u; u.x = 2; u.y = 3; u.ptr = # printf(\"%d, %d, %d\\n\", u.x, u.y, u.ptr); printf(\"sizeof union `u`: %d\\n\", sizeof(u)); } Since the last update to the union’s memory is a pointer, we expect the output to be a random memory address. And the size of the union would be 8 bytes because of pointer variable.\nmain: push\trbp mov\trbp, rsp sub\trsp, 16 mov\tDWORD PTR -4[rbp], 5\t; num mov\tDWORD PTR -16[rbp], 2\t; u.x mov\tDWORD PTR -16[rbp], 3\t; u.y lea\trax, -4[rbp]\t; \u0026num mov\tQWORD PTR -16[rbp], rax\t; u.ptr mov\trcx, QWORD PTR -16[rbp] mov\tedx, DWORD PTR -16[rbp] mov\teax, DWORD PTR -16[rbp] mov\tesi, eax lea\trax, .LC0[rip] mov\trdi, rax mov\teax, 0 call\tprintf@PLT mov\tesi, 8 lea\trax, .LC1[rip] mov\trdi, rax mov\teax, 0 call\tprintf@PLT mov\teax, 0 leave ret Indeed.\nBasically, every member of a union is just an alias to the union’s memory.","unions#Unions":"UnionsSeptember 07, 2025","whats-the-use-of-unions#What\u0026rsquo;s The Use Of Unions?":"There can be multiple use cases but the best one and the most easily comprehensible one is memory critical systems.\nIf you have to use multiple temporary variables, they take a lot of space, which might be bad on a low memory system like embedded systems. If we are sure that we have one time use case of temporary variables, we can create a union instead.\nYou may ask, if the variable has to be used once per context only, why can’t we reuse that variable? Nice question. And the answer is we can. No problem. But when you follow standard coding practices or a team is working on that project, you’d prefer stricter naming rules so that no one accidentally creates a problem. That’s it. You can use temp everywhere but temp.ptr, temp.num etc makes the temp variable more specific without costing any extra space."},"title":"unions"},"/gitbook/docs/low-level-architecture/orientation/":{"data":{"the-rules-the-make-assembly-slightly-easier#The Rules The Make Assembly Slightly Easier":"The Rules The Make Assembly Slightly EasierAssembly is not a high-level language, therefore, the usual mental models of learning a programming language will not help. Below are a few rules that I have learned throughout this journey.\nC is not spared of abstractions. Its abstractions are revealed when you look at assembly. Context and Interpretation are paramount while managing data in memory. Assembly is bare-metal. It knows no bounds. At one moment, your program might run perfectly. And the next moment, it might run into an issue. The most straightforward way to improve your understanding of assembly is by doing. You do, you fail, you understand, you don’t repeat. That’s it. Your code is just one step away from “undefined behavior” territory. Sometimes, swapping two lines and bringing them at their original position changes the result. And I am not joking. Just writing a character and then backspacing it changes the outcome. And I am not joking. Progressive learning is the key. As long as you don’t hurry and move progressively, you will live a happy life. Intent is more important than learning every single way of doing something. Understand what you want to achieve, then map how the generated assembly does that. What seems like an overwhelming incomprehensible black magic is just logic and compiler optimizations, where the logic becomes primitive as you go deep into trenches \u0026 understand what we are trying to do and the compiler optimizations, which are just decades of work done by intelligent beings to make code efficient stops haunting you because you understand that an intent can be fulfilled in n number of ways. Tools like Compiler explorer at godbolt.org are excellent to understand the different manifestations of the same principles."},"title":"Orientation"},"/gitbook/docs/low-level-architecture/orientation/anatomy/":{"data":{"":"","data-addressing-modes#Data Addressing Modes":"Immediate Mode. The simplest method. Here, the data to access is embedded in the instruction itself. Example: mov eax, 5 ; Move the value 5 into EAX register Register Addressing Mode. The instruction contains a register to access, rather than a memory location. Direct Addressing Mode. The instruction contains the reference to the memory address to access. Example: mov eax, [some_address] ; Move data from memory at some_address into EAX Indexed Addressing Mode. The instruction contains a memory address to access, and also specifies an index register to offset that address. Example: mov eax, [ebx + 4] ; Move data from the address in EBX + 4 into EAX Indirect Addressing Mode. The instruction contains a register that contains a pointer to where the data should be accessed. Example: mov eax, [ebx] ; Move data from the address in EBX into EAX Base Pointer Addressing Mode. This is similar to indirect addressing, but you also include a number called the offset to add to the register’s value before using it for lookup. Always leave an empty line at end of the program. This gracefully marks the end of assembly code. Otherwise, you’ll get a warning by the assembler.","registers-as-operands#Registers (As Operands)":"If you’ve attempted assembly before, you might have seen old lectures using ax, bx, cx, dx like registers, or eax, ebx, ecx, edx and the most recent ones might be using rax, rsi, rdi etc….\nWe know that x86 architecture emerged from 8086 processor, which was a 16-bit processor. The two lettered registers we see belongs to the 16-bit architecture.\nThese 16-bit registers also have smaller ones. They are sized 8-bits each. They are called high and low. For example, ax has al and ah. Intel extended them to 32-bit. This increased the register width and all the registers from 16-bit architecture got prefixed by an e. So, ax become eax, bx become ebx and so on. e stands for extended.\nAMD extended it further to 64-bit. The register width increased again and we get new registers prefixed with r, while retaining the existing ones. So, eax become rax, ebx become rbx and so on. Along with this, we have got 8 new general purpose registers from r8-r15.\nThe newer systems are also backward compatible. This means that x86_32 still supports x86 registers, x86_64 still supports x86 and x86_32.\nWhen we use rax, we are using the complete 64-bit register. When we use eax, we are using the lower 32-bits of the rax register. When we use ax, we are using the lower 16-bits of the rax register. When we use ah, we are using the 8-bits after al, 8-15. When we use al, we are using the lowest 8-bits of the rax register, 0-7. a visual diagram A complete list of general purpose registers, link.","section#Section":"Sections define how the memory layout at runtime would be prepared. Common sections include:\n.text, for code or instructions. .data, for initialized data. .bss, for uninitialized data, where bss stands for “Block Started by Symbol”. It refers to a label (symbol) that marks the start of a block of uninitialized data in memory. It helps in reducing the size of the object files by leaving a note for the system to allocate x bytes at runtime for this block and zero-initialize them. As allocating zeros at compile time makes no sense. .rodata, for read-only data.","system-calls#System Calls":"A system call is the controlled gateway between a user-space program and the kernel. It lets your code request services that require higher privileges — like writing to the screen, reading a file, or exiting the program.","user-mode-vs-kernel-mode#User Mode vs Kernel Mode":"The CPU operates in two modes:\nUser Mode: Restricted environment in which our code runs. Kernel Mode: Full-access mode where the operating system runs. Our program cannot perform privileged operations directly. Instead, it uses syscalls to request the kernel to perform them on its behalf.\nLinux supports hundreds of syscalls. Here are a few common ones:\nPurpose Syscall Syscall Number Read from a file read 0 Write to a file write 1 Open a file open 2 Map memory mmap 9 Exit the program exit 60","variables-vs-labels#Variables v/s Labels":"A variable is a container to store a value. A label is a named memory location. Both are different.\nA label can point to a group of instructions, a constant value, a procedure, anything. But a variable only stores some value. It can store the result of a computation, but not the instruction itself.\nIn simple terms, every variable is a label, but every label need not to be a variable."},"title":"Anatomy Of An Assembly Program"},"/gitbook/docs/low-level-architecture/orientation/basic-intro/":{"data":{"":"Learning assembly is not similar to learning yet another programming language. There are no high-level constructs. There is memory and instructions. That’s it.","assembly-time-vs-run-time#Assembly-Time vs Run-Time":"Assembler directives mean nothing to the CPU. They exist to streamline development. Assembler resolves them into machine understandable things while assembling the code. This is called assembly-time management. (Checkout a-high-level-overview-of-build-process-in-c.md)\nThere are things that are consistent across all the assemblers because the CPU directly understand them. These are runtime managed things.\nFor example, OFFSET is an assembler directive, while lea is a CPU understood operation, defined in the ISA itself.","central-processing-unit#Central Processing Unit":"CPU is the physical component that actually executes instructions.\nWhile modern systems are very complex, the role of CPU is quite simple: fetch, decode, and execute instructions stored in memory. And this what a fetch-execute cycle is.\nThe CPU needs to know the instruction to be executed, for this, it has got Program Counter, whose purpose is to hold the memory address of the next instruction to be executed.\nThe CPU has the instruction now and it needs to figure-out what this instruction means. For this, the CPU has instruction decoder.\nIn our everyday life, anything we do involves movement. We are moving things from one place to other. Locations are one the most important things in our life. And so as with CPU.\nEvery instruction uses some memory locations, where some data is stored, which is required in the instruction. Instructions themselves can be stored at a memory location. To fetch that data from that memory location, what comes handy is the data bus. It is the connection between CPU and memory. Suppose you have to go to your best friend’s house. You have picked up the keys for your bike, the cap, the scarf to protect from heat, and the sun glasses. All these things are stored somewhere in your house.\nThe key is probably on the top of the fridge or in the key-holding area. The sun glasses are at the dressing table. The scarf is in the closet. And the cap is on the table. What about the address to your friend’s house? Where is that? In your mind? If you don’t go mad on me, can I ask why don’t you write the address of your friend on a paper and store it your closet? Obviously, you are saying, “I am not mad!” Keeping the address in your mind ensures that it is accessible all the time. The CPU also has some highly-efficient and rapidly-accessible locations for this exact purpose. They are called general purpose registers. These are high-speed memory locations inside the processor itself, that takes part in the actual execution. At last, the Arithmetic and Logic Unit. The data and the decoded instruction is passed here for further processing. Here, the instruction is actually executed. The computed results are placed on the data bus and sent to the appropriate location (a memory, or a register), as specified by the instruction.\nAnd these are the core elements of the CPU.","fetch-execute-cycle#Fetch-Execute Cycle":"Fetch – Read the instruction from memory (address held in instruction pointer). Decode – Understand what the instruction means. Execute – Perform the operation (move data, add, compare, etc). Store – Write the result (often into a register or memory). Repeat – Move to the next instruction. And this happens billions of times.","isa-vs-assemblers#ISA v/s Assemblers":"There are multiple assemblers in the market. GNU assembler (GAS), Netwide assembler (NASM), fish assembler (FASM), Microsoft assembler (MASM) and so on….\nAssembler does one thing — translate human readable assembly instructions into machine opcodes, which the CPU can understand and execute directly.\nA CPU’s instruction set architecture defines its capabilities. It conceptualizes everything that the CPU can do.\nAssemblers are the programs that decides how the programmers will interact with the CPU.\nTake this, there are handful of firms that research on semiconductor chips, but there are relatively many who does the manufacturing. ISA is that research while assemblers are the manufacturers. Each manufacturer (assembler) has the freedom to manufacture its own way.","memory-ram#Memory (RAM)":"Imagine a room of personal-lockers inside of a bank. Every locker is the same.\nSame size and color, Same capacity, Same access mechanism, and An addressing system to uniquely identify them. A locker can contain anything, but what identifies them commonly is valuables. A person can keep gold or silver items while another person can keep the photos of their family. Items are different but both of them identifies as a valuable.\nSimilarly, memory is a huge collection of boxes, which have common properties, like:\nThey are fixed in capacity, 1 byte (or 8-bits). Each box is identified by a unique number, called memory address. Just as a locker itself can’t identify its contents, everything is just a valuable, the same is with memory.\nEverything is raw bytes. What defines a byte as an integer, a decimal, an emoji, an alphabet is the interpretation of that byte (or a group of bytes). Previously, we have read that context and interpretation is what that rules assembly. We can see it in practice here. A byte can be interpreted as digit, as an alphabet. When those same bytes are grouped, and interpreted, their meaning changes. Context decides the kind of interpretation required in order to get the right meaning out of those bytes.","registers#Registers":"In addition to the memory outside of the processor, the processor itself has some special, high-speed memory locations called registers.\nThe primary purpose of a register is to hold the data that the CPU is actively working on. Just like the desk has a pen-holder which holds necessary pens like black and blue ball pens, a correction pen, a ruler, pencil, and a rubber. But it doesn’t contain the whole stationary. We have cupboards for that.\nSince registers are high-speed and are located within the processor itself, they are limited in number, for various valid reasons.\nMost information is stored in the main memory, brought in the registers [,for processing], and then put back into memory when the processing is completed.\nJust like memory, registers also hold bits. It’s up to you to interpret them correctly (as numbers, characters, addresses, etc.)\nMainly, there are two types of registers:\nGeneral Purpose Registers, this is where the main action happens. Addition, subtraction, multiplication, comparisons, and other operations generally use GPRs. They are expansive and are very less in number. There are 16 GPRs in x86_64 (amd64 or 64-bit) architecture. Special Purpose Registers, self-explanatory?","sizewidth-of-a-register#Size/Width Of A Register":"Word size is the size of the data, or the number of bits the CPU can process at once.\nIt is architecture dependent. 32-bit systems has a word size of 32-bits or 4-bytes, while 64-bit systems have 64-bit wide registers.\nWord is the size of the registers in a particular CPU architecture.\n“Word” is the fundamental unit of CPU. Just like m/s is for velocity.","what-is-32-bit--64-bit#What is 32-bit \u0026amp;\u0026amp; 64-bit?":"We download software. If that’s a technical software, it is always mentioned whether you are at 32-bit or 64-bit OS. And there are different software for both.\nMost modern systems are based on 64-bit architecture.\nThis 32-bit and 64-bit tells us how many numbers the CPU is capable of dealing at once.\nA 32-bit CPU can work with numbers that are 32 bits long. A 64-bit CPU can work with numbers that are 64-bits long."},"title":"Basic Computer Theory"},"/gitbook/docs/low-level-architecture/orientation/binary-number-system/":{"data":{"":"Polished on 07 September 2025 (written in May 2025)","1s-complement#1\u0026rsquo;s Complement":"The value you must add to a number so the result is a string of 1s or the result is the maximum value that you can represent with the number of bits.\nTo get the 1’s complement of a binary number: Flip all bits (change 0s to 1s and 1s to 0s).\nExample:\n5 in binary is represented by: 0b00000101 1’s complement of 5 (= -5): 0b11111010 The problem with 1’s complement is that it has two representation of 0.\n+0 = 0b00000000 -0 = 0b11111111 But 0 is one digit. + and - are insignificant for it.","2s-complement#2\u0026rsquo;s Complement":"In 2’s complement, the number of possible combinations are divided into two halves.\nLowe halve: positive integers Upper halve: negative integers To obtain 2’s complement of a number:\nStart with a binary representation. Get 1’s complement. Add 1 (0001) to the result. Example:\n5 in binary is represented by: 0b00000101 1’s complement of 5: 0b11111010 Add 1: 0b11111010 + 0b00000001 = 0b11111011 We get -5 in 2’s complement as 0b11111011 0b00000101 - 0b11111011 = 0b00000000, Hence proved!","addition#Addition":"Carry once the sum exceeds 1. And dispose than 1 only when the result of a sum is 0.\nBasically, if the digits in sum are more than 1, you have to carry.\nExample:\n0011 (3) 1101 (13) = 10000 (16) We have to add a new bit because the result exceeded the bit-limit. 0011 (3) 0110 (6) = 1001 (9)","binary-arithmetic#Binary Arithmetic":"Take out your elementary mathematics notes because we are going to need them","binary-to-decimal-conversion#Binary To Decimal Conversion":"Before we dive any further, it is important to understand how bits relate with decimals.\nTo obtain the decimal equivalent of a group of bits, we just have to multiply each bit with a power of 2. For example: 0101 ; to obtain it decimal equivalent, we have to multiply each digit right to left with a power of 2. The power starts from 0 and goes up to digit - 1.\n0101 = 0* 2^3 + 1* 2^2 + 0* 2^1 + 1* 2^0 = 0 + 4 + 0 + 1 = 5. If you have any problem, it is discussed in detail below.","conclusion#Conclusion":"It is no black magic, first of all.\nSecond, signed numbers and floats is where the problem is.","how-2s-complement-this-solves-the-problem-of-0#How 2\u0026rsquo;s complement this solves the problem of 0?":"Lets take an example using 4-bits, because combinations here are neither too less, nor too more.\n4-bits can represent 16 combinations, or better, 16 unsigned integers from 0 to 15. These combinations are: 0000, 0001, 0010, 0011, 0100, 0101, 0110, 0111, 1000, 1001, 1010, 1011, 1100, 1101, 1110, 1111 2’s complement divides these into two halves. Both of them gets 8 values each.\nLower Half :: 0000, 0001, 0010, 0011, 0100, 0101, 0110, 0111, Upper Half :: 1000, 1001, 1010, 1011, 1100, 1101, 1110, 1111 We can use the range formula mentioned above to verify this:\n=\u003e [(-2)^(4-1), 2^(4-1)-1] =\u003e [(-2)^3, (2^3)-1] =\u003e [-8, +7] Let’s see how the upper halve maps to negative integers.\nFirst of all, If you notice, all the combination in the upper half have the most significant bit set to 1.\nSecond, to obtain integers from 8-15, 4th bit must be set to 1. But it is not required with 0-7. This is the distinction.\nIn 2’s complement, positive integers have their MSB set to 0 and negative integers have this bit set to 1. Now take the upper half and put it in the left of the lower halve. We’ll get something like this:\n1000, 1001, 1010, 1011, 1100, 1101, 1110, 1111, 0000, 0001, 0010, 0011, 0100, 0101, 0110, 0111 -8 -7 -6 -5 -4 -3 -2 -1 0 +1 +2 +3 +4 +5 +6 +7 To obtain the negative representation of 0, we need the MSB as 1 and rest of the bits as 0, which brings us to 1000, which represents -8.\nWe can notice a pattern from the grid above. Every negative integer is of the form -(2^n) + (+ve integer) . For example:\n-8 =\u003e -(2^4) + (+8) = -16 + 8 = -8 (1000) -1 =\u003e -(2^4) + (+1) = -16 + 1 = -15 (1111) If we try the same for 0, we get\n-0 =\u003e -(2^4) + (+0) = -16 + 0 = -16 But 16 as a combination is not possible using 4-bits.\nThis proves that 2’s complement by design has no room for two representations of zero.","number-system-refresher#Number System Refresher":"Primarily we have 4 number systems. They are: binary, octal, decimal and hexadecimal.\nNormally we use the decimal number system.\n1 Byte = 8 Bits\nNow it’s time to dive into binary number system.","representing-negative-integers#Representing Negative Integers":"In binary number system, + and - holds no meaning. Complements are how we represent negative numbers here.\nComplements are mathematical transformations of binary numbers, specifically designed for how binary arithmetic works in computers. The most common ones are: 1’s and 2’s complement.","revision#Revision":"Everyone learns number lines in elementary school.\nA pictorial representation of numbers on a straight line. That number line has a zero which separates negative numbers from positive numbers.\nA whole number (-infinity to +infinity) with no fractional part is called an integer. A whole number with fractional part is a decimal.","signed-arithmetic#Signed Arithmetic":"Computers use 2’s complement so it is pretty straightforward.\nA - B becomes A + (-B)\nWe obtain -B using 2’s complement.\nExample:\nA = 0101 (5) B = 0011 (3) 5 - 3 = 2 A - B =\u003e A + (-B) -B = 1's complement (B) + 0011 = 1100 + 0001 = 1101 A - B = A + (-B) = 0101 + 1101 = 10010 (discard carry) = 0010 = 2 Hence Proved","subtraction#Subtraction":"Take out you subtraction notes now. We are gonna need them. Just joking. If you don’t remember, you’ll.\nWe know that 10 - 8 = 2. And we can do the same for any operands. But this process has become so automatic that we have internalize the theory subconsciously and forget it consciously.\nTo understand binary subtraction, we have to revisit how subtraction works.\nIn decimal number system, every digit in a number has a position attached to it. For example, take 49521 . Moving from right to left,\n1 is the ones digit, 2 is the tens digit, 5 is the hundreds digit, 9 is the thousands digit, and 4 is the ten-thousands digit. These positions aren’t NPCs, they have a purpose. And this purpose is the whole purpose of our revisit.\nWe know that decimal system is a base-10 system. But what does that actually mean?\nIt means that every position has a weight attached to it, where weights are indices raised to the power of base and indices refers to a numerical identity, given to a position. These indices start from 0 and go till (number of digits - 1). Therefore,\nThe ones position carries a weight of 10^0 = 1. The tens position carries a weight of 10^1 = 10 The hundreds position carries a weight of 10^2 = 100 The thousands position carries a weight of 10^3 = 1000 The ten-thousands position carries a weight of 10^4 = 10000 In division, we have dividend (numerator) and divisor (denominator). In subtraction, we have minuend and subtrahend.\nEven I can’t remember if I have heard these words before. To keep things simple, if we have op1 - op2 operation, op1 is the minuend and op2 is the subtrahend.\nThere are multiple techniques to do subtraction.\nIn school, we have learned column-based subtraction, which involves borrowing from the left digits. We can add equal numbers to both minuend and subtrahend and make the subtrahend end with zero. It helps visually, that’s it. Subtraction by complement. A quite easy way to do subtraction, and computers this. Most simplest way, counting, even though I confuse that as well, sometimes. Decomposition. I love this one and use it quite often, when I don’t have calculator. We break numbers in pairs of 10s. Like 4215 can be broken into 4200 + 15. 2307 can be broken into 2300 + 7. 4200 - 2300 is simple, 1900. So as 15 - 7, 8. Add at last, 1900 + 8, result = 1908. I use this in addition as well. And yes, I didn’t forget calculator! We have to study column-based subtraction for binary bits.\nWhen doing column-based subtraction, we subtract digits at corresponding positions in minuend and subtrahend. For example: 4215 and 2307. The digit at ones position (7) will be subtracted from the digit at ones position (5) only.\nSometimes, we get stuck when the corresponding minuend is lesser than the subtrahend. In that case, we borrow from the left side. This is where the problem is.\nLet’s take 10 - 4 as borrowing is inevitable here.\nWe are taught that when we borrow, we reduce the one from the lender and add 10 to the borrower. So, 0 borrowing from 1 becomes 10. Now it can subtract.\nYou may ask, it was 10 before as well. And seriously, I also want to know the “commonsense” behind this. But anyways. Similarly, in 20 - 4, we just need 10, not the entire 20. Upon looking closely, 20 is just 10+10. Problem solved. Take out one 10 and give it to 0.\nBut why 10 only? Why not any arbitrary number? And the answer is weight. Treat positions like containers, where every container has a limit on how much it can contain.\nThe ones position has a weight 10^0 or 1. It can’t contain more than that. And 1 here represents the base value, which is 10, not the literal 1. That is why we have never borrowed more than 10. Because it can’t hold more. And to keep thing consistent, we borrow the max value, not any arbitrary digit. That’s all we need to know. Looks easy huh?\nLets tackle binary subtraction now.\nThere are 4 rules, 3 of them are straightforward. And one is the rebel.\n0 - 0 = 0 1 - 1 = 0 1 - 0 = 1 0 - 1 = 1 # the problem Take this:\n_ 0110 (6) 0100 (4) = 0010 (2) Simple. And these?\n_ 0100 (4) 0001 (1) = 0011 _ 1010 (10) 0011 (3) = 0111 If you are questioning what and how, then we are on the same page. I have also wasted hours figuring out the same.\nAnyone who has spend time with binary knows about “8 4 2 1”. Some might know it by name, others use it as is. I fall in the others category.\nMy search brought me to this YouTube video: Binary Addition and Subtraction Explained (with Examples). And the first time, I got introduced to the term weights.\nThese 8 4 2 1 are the weights in binary number system.\nTo obtain weight for any given position, we use this formula: Weight For Position i = (base)^i Lets look at the minuend, 1010. The weights attached to each digit are: [2^3:1, 2^2:0, 2^1:1, 2^0:0]\nTo convert binary representation into decimal, we have to multiply the digit with its weight and sum-up the result. For example, 1010. The weights are 8421. We get 8 + 2 = 10.\nFrom this, can we say that,\n1 at 0th bit position represents 2^0? 1 at 3rd bit represents 2^3? And a 1 at position i represents 2^i? A bit at index 3 is basically inside a container which can hold a maximum value of 2^3 (size in decimals). But in binary, it is still about 0 and 1.\nI think 10 - 3 is a really complex binary subtraction primarily because of how borrowing works.\nTo visualize borrowing, lets take a simple example.\n8 - 3 = 5 _ 1000 (Minuend, 8) 0011 (Subtrahend, 3) = ____ Subtraction process starts the same, from right towards left.\nHere is a simple table to condense this information.\nAttribute Value At Bit Value At Bit Value At Bit Value At Bit Index 3 2 1 0 Weight 2^3 = 8 2^2 = 4 2^1 = 2 2^0 = 1 Minuend 1 0 0 0 Subtrahend 0 0 1 1 Now lets understand borrowing.\nbit-0 subtraction needs borrowing. It goes to bit-1. bit-1 also needs borrowing. It goes to bit-2. bit-2 also needs borrowing. It goes to bit-3. bit-3 is 1, so it can lend. The weight attached to bit 3 is 8. bit-2 has come to ask for lending from bit-3. But the maximum that bit-2 can contain is 2^2. But bit-3 can lend 2^3 only. Because in binary, either you have 0 or you have 1. So bit-3 breaks itself as 4+4, which is same as 2 units of 2^2. And notice, 2^2 is exactly what bit-2 can hold at max. But there are two units. Lets not go any further and assume it can hold it. Lending successful. Status: bit-3 = 0 bit-2 = something that 2*(2^2) might refer to. Now bit-2 lends to bit-1. It has got two units of 2^2. But again, bit-1 can hold up to 2^1 only. So bit-2 breaks one of its units 2^2 as 2 * (2^1). And lends it to bit-1. Lending successful. Status: bit-3: 0 bit-2: something that 2^2 might refer to. One unit is now given to bit-1. bit-1: something that 2*(2^1) might refer to. Now bit-1 lends to bit-0. It has two units of 2^1. But once again, bit-0 can only hold up to 2^0. So bit-1 breaks one of its units 2^1 as 2 * (2^0), And lends it to bit-0. Lending successful. Status: bit-3: 0 bit-2: something that 2^2 might refer to. bit-1: something that 2^1 might refer to. One unit is now given to bit-0. bit-0: something that 2*(2^0) might refer to. Now, no more lending or borrowing.\nWhat is this “something that __ might refer to”? What do they actually refer to?\nJust look at them, you’ll find your answer. bit-3 is already zeroed, so no confusion. bit-2 is 2^2, from the table above, it is exactly the weight it can contain, that means, a 1. bit-1 is 2^1, from the table above, it is exactly the weight it can contain, that means, a 1. bit-0 is 2 units of 2^0 or better, 2 units of 1. Now, lets do the subtraction.\n_ 1 0 0 0 0 0 1 1 can be written as _ 0 1 1 (1+1) 0 0 1 1 I don’t think its tough anymore. The answer is 0101, precisely what we needed.\nLets tackle the final boss now, which spiraled me to understand subtraction from its roots.\n_ 1 0 1 0 (10) 0 0 1 1 (3) can be written as _ 0 1 (1+1) (1+1) 0 0 1 1 = 0 1 1 1 (7) And, we are done!\nIn the end, binary subtraction is mysterious because of the fundamentals not being right.\nMost of the people advising “Your fundamentals have to be really strong” might not even realize the depth of their statement. It can easily turn into a rabbit hole because of our incomplete understanding and things becoming automatic (subconscious).\nBut don’t assume this was the final boss.","types-of-bits#Types Of Bits":"Primarily there are two types of bits.\nLeast Significant Bit (LSB) : It is the rightmost bit in the binary representation of an integer. It is called least because setting this ON/OFF has the least impact on the magnitude of the value. Most Significant Bit (MSB) : It is the leftmost bit in the binary representation of an integer. It is called most because setting this ON/OFF has the largest possible impact on the magnitude of the value. 15 is represented by 1111 in 4-bits.\nIf you set LSB to 0, we get 1110 which is 14. The magnitude is lowered by 1 only. If you set MSB to 0, we get 0111, which is 7. The magnitude is lowered by 8.","types-of-numbers#Types Of Numbers":"In computer science, there are 3 types of numbers, or, better is, 3 levels of difficulty with numbers.\nUnsigned integers are positive integers. Signed integers are “positive and negative Integers” together. Floating point values. There is no name for “negative integers only” in computer science.\nFractions are popularly known as floating point integers.\nIn unsigned integers, it is simple.\nWhen all the bits are 0, 0b00000000, it is 0. When all the bits are 1, 0b11111111, it is 255. With signed integers comes problems.\nSigned integers are implemented using two’s complement.\nWhat is 2’s complement? Discussed below. To represent signed integers, we use the most significant bit as the sign bit. It is how we keep tracks of positivity and negativity.\n0 = positive 1 = negative What is this most significant bit? Discussed below.\nThus, a group of 8-bits can represent -128 to +127. Their representation is as follows:\n0 =\u003e 0b00000000 +127 =\u003e 0b01111111 -128 =\u003e 0b10000000 -1 =\u003e 0b11111111","unsigned-arithmetic#Unsigned Arithmetic":"Everything is same, only the borrowing value is different. But the borrow value is calculated the same way."},"title":"Numbers In Computer Science"},"/gitbook/docs/low-level-architecture/orientation/bitwise-operations/":{"data":{"":"08 September 2025","conclusion#Conclusion":"Bitwise Operation Symbol in C Instruction In x64 Assembly Description Bitwise AND \u0026 and reg, reg/mem/imm Keeps only common 1 bits Bitwise OR | or reg, reg/mem/imm Bitwise XOR ^ xor reg, reg/mem/imm Sets bit if different Bitwise NOT ~ not reg/mem Flips all bits Logical AND (boolean) \u0026\u0026 test reg, reg/mem + jcc In C it’s short-circuit; in asm use test/cmp + jump Logical OR (boolean) || Logical NOT (boolean) ! test/cmp + setz Inverts truth value Left Shift \u003c\u003c shl reg, imm/cl or sal reg, imm/cl Fill right with 0 Right Shift (logical) \u003e\u003e (unsigned) shr reg, imm/cl Fill left with 0 Right Shift (arith.) \u003e\u003e (signed) sar reg, imm/cl Fill left with sign bit Rotate Left – rol reg, imm/cl Bits wrap left-to-right Rotate Right – ror reg, imm/cl Bits wrap right-to-left Rotate Left w/ Carry – rcl reg, imm/cl Includes CF as extra bit Rotate Right w/ Carry – rcr reg, imm/cl Includes CF as extra bit","data-compression#Data Compression":"We can use one a single int to compress multiple information and use bit shifts to obtain the correct values.","examples#Examples":"In bitwise AND, compare the bits in both the operands and put 1 in the output for that bit if both the operands have 1, else 0.\n5 \u0026 5 = 5 0101 \u0026 0101 ------ = 0101 ------ 5 \u0026 6 = 4 0101 \u0026 0110 ------ = 0100 ------ In bitwise OR, compare the bits in both operands and put 1 in the output bit if at least one operand bit is 1, else 0.\n5 | 5 = 5 0101 | 0101 ------ = 0101 ------ 5 | 6 = 7 0101 \u0026 0110 ------ = 0111 ------ In bitwise NOT, flip all the bits in the output.\n6 = 0110 ~6 = 1001 (-7) Be cautious and remember the rule of interpretation as the output might not be what you expect if you don’t factor it. Take this: int a = ~6; If you use %d format specifier, you -7 as ints are signed by default. If you use %u format specifier, you get 4294967289 as it interprets the value as unsigned int. And it is applicable to any pattern that can produce both signed and unsigned interpretations. The bitwise XOR operation is made up of all the three fundamental operations.\nXOR(A, B) = (A AND NOT B) OR (B AND NOT A) . In simple words, set the bit to 1 where the bits are different.\n5 ^ 6 = 3 0101 ^ 0110 ------ = 0011 ------ Let’s talk about shifts now.\nIn left shift, we move from right to left and shift all the bits by n positions and the new bits in right will be 0. For example,\n5 \u003c\u003c 2 =\u003e 00000101 \u003c\u003c 2 =\u003e 00010100 So 5 \u003c\u003c 2 = 20. In right shift, we move all bits to the right by n positions. We have two options here.\nArithmetic right shift is where the sign is preserved. The new bits on the left are copies of the sign bit. For example:\n11010000 \u003e\u003e 2 =\u003e 11110100 00001100 \u003e\u003e 2 =\u003e 00000011 Logical right shift is normal shifting. For example:\n11010000 \u003e\u003e 2 =\u003e 00110100 If the value get passed the limit, it is discarded. For example:\n11001100 \u003c\u003c 2 =\u003e 00110000 00001100 \u003e\u003e 4 =\u003e 00000000","introduction#Introduction":"As named, bitwise operations are logical operations performed on the individual bits of a binary representation.\nWe have logic gates like AND, OR, NOT and others which are made up of these fundamental logic gates, like NOR and XOR. When we perform these logical operations on bits, it becomes a bitwise operation.\nFrom boolean algebra, we know this.\n1 and 1 = 1 1 or 1 = 1 1 and 0 = 0 1 or 0 = 1 0 and 0 = 0 0 or 0 = 0 The same idea is applied either it is normal logical operations or bitwise operations.\nIn C, we use:\nOperation Symbol AND \u0026\u0026 OR || NOT ! Bitwise AND \u0026 Bitwise OR | Bitwise NOT ~ Left shift « Right shift » Let’s take some examples to understand how bitwise operations work","performance#Performance":"Bitwise operation are single instructions and multiplication/division is more costlier. So, if the system can achieve the same thing with bitwise ops, it optimizes for that.\nFor example:\n(x \u003c\u003c n``) ~= (x* 2^n) . 5 \u003c\u003c 2 = 20 5*(2^2) = 5*4 = 20 (x \u003e\u003e n``) ~= (x/ 2^n).\n5 \u003e\u003e 2 = 1 5/(2^2) = 1","rotation#Rotation":"Rotation is shifting with wrapping. The values which get past the container limit are wrapped on the other side.\n10110001 rol 2 =\u003e 11000110 10110001 ror 2 =\u003e 01101100","what-use#What Use?":""},"title":"Bitwise Operations"},"/gitbook/docs/low-level-architecture/orientation/calling-conventions/":{"data":{"":"Registers are fast, low-level storage locations inside the CPU. While x86_64 offers 16 general-purpose registers, their usage is often guided by conventions rather than absolute freedom.\nThe use of these registers usually depends on the context. Primarily there exist two contexts:\nFunction Call Context System Call Context How the registers would be used in the above two contexts is defined in a system-level agreement, called ABI, which stands for Application Binary Interface.\nAs long as we are out of these two contexts, we can use a register as we want. And we will demonstrate this later.","but-why-do-these-conventions-even-exist#But Why Do These Conventions Even Exist?":"System V ABI (where V is 5) is not the only ABI that exist. It is the one that Linux uses. Microsoft Windows 64-bit uses x64 ABI.\nIf every organization used their own calling convention, one thing is sure to suffer — cross-compatibility. These ABIs are contracts that everyone agrees upon. When every system uses the same convention, cross-compatibility improved. Software support improved.","function-call-convention#Function Call Convention":"Argument Register Description Syscall # rax Identifier Argument 1 rdi First parameter Argument 2 rsi Second parameter Argument 3 rdx Third parameter Argument 4 r10 Fourth parameter Argument 5 r8 Fifth parameter Argument 6 r9 Sixth parameter","syscall-convention#Syscall Convention":"Note: Caller is the function that makes a call to another function. The function that is being called is termed as callee.\nWhen a register is callee-saved, the callee function must preserve the state of that register if it wants to use it and restore its state before returning. When a register is caller-saved, the caller function must preserve its state if it wants to use it later because a function call in-between might use it and it has no reason to preserve its state. In Linux (x86_64), the most common calling convention is System V AMD64 ABI. It defines how functions and system calls exchange data by assigning specific roles to specific registers.\nTo successfully invoke a system call, our data must be placed in these registers accordingly. Otherwise, the kernel will not interpret our request correctly."},"title":"Calling Conventions"},"/gitbook/docs/low-level-architecture/orientation/common-terms/":{"data":{"":"Mnemonic: The actual CPU operation. Operand: Arguments passed to the mnemonic, which could be a register, intermediate or a memory location. Instruction: Something the CPU can execute. It includes both the mnemonic and the operands. Immediate: An immediate is a constant value, like 4. Label: A name given to a particular memory address in the code. It is made up of letters, digits and underscore. A label must start with a letter or underscore. A label must end with a colon. There are . prepended labels which are used to make a label available to its parent and hides from others. Just don’t think about it for now, it’s a complicated thing. It holds meaning for the assembler (GAS, in our case), not the CPU. The assembler replaces the labels with virtual addresses or offsets. Directive (or Pseudo-Instruction): Instructions defined for the assembler program, not the CPU. They begin with a period (.). Ex: .section creates a section within the program. Section: The code is divided into multiple sections to organize the memory layout. Comment: Anything after a semi-colon (;) or hash (#) is ignored by the assembler and is a note for the programmer itself. Keyword: In high-level languages, keywords are reserved words (like if, for, while). In assembly, the idea of keywords basically overlaps with mnemonics and directives. Symbol: Everything is a symbol. Ever label is a symbol, but every symbol need not to be a label."},"title":"Common Terminologies"},"/gitbook/docs/low-level-architecture/orientation/hello-world/":{"data":{"":"How To Print “Hello, World!” ?\nStep 1: Store the hello world string somewhere in the memory. Step 2: Prepare a write syscall. Step 3: Invoke the syscall. Step 4: Prepare an exit syscall. Step 5: Invoke the syscall. The assembly code for hello world looks like:\n.intel_syntax noprefix .section .data msg: .ascii \"Hello, world!\\n\" len = . - msg .section .text .global _start _start: mov rax, 1 # syscall number for write (1) mov rdi, 1 # file descriptor (stdout) mov rsi, offset msg # pointer to the buffer to print mov rdx, len # buffer length syscall # invoke kernel # Exit syscall mov rax, 60 # syscall number for exit (60) xor rdi, rdi # exit code 0 syscall Note: Indentation makes no sense in assembly, but I have write it for visual clarity.\nLets explore this line by line.","exit-syscall#Exit Syscall":"mov rax, 60 sets the register for the next syscall, which is exit.\nxor rdi, rdi sets rdi to 0 using a bitwise XOR (which is faster than mov rdi, 0). This sets the exit code to zero.\nsyscall, invoke the syscall.\nDone.","fun-thing#Fun Thing":"A hello world program written in C won’t translate to such an assembly. That’s a complete rabbit hole but the reason is that it uses C runtime for setup and cleanup, which makes the assembly slightly different.\nI have explored it in depth here. But it is lengthy and takes time, so I would suggest not to explore that now.","line-1-assembler-directive-for-syntax-clarity#Line 1: Assembler Directive For Syntax Clarity":"Since we are writing in intel syntax but assembling it through GAS, we have to inform GAS about it.\n.intel_syntax tells GAS we are writing in intel syntax. .no_prefix tells GAS to strictly avoid % and $ in instructions.","line-2-define-data#Line 2: Define Data":"Remember, sections are assembler directives, not something that the CPU understands. This is for the assembler to maintain clarity in asm code.\nThe data section keeps all the static and global initialized data variables.\n.section marks the start of a section and .data tells that it is data section.\nA section continues until a new section is defined. Therefore, both msg and len are part of the data section, where msg and len are constant labels.\nNote: Sections can be user-defined as well. But they are for advance use-cases.","line-3-and-4-data-global-and-static-variables#Line 3 And 4: DATA (Global And Static Variables)":"msg: .ascii \"Hello, world!\\n\" len = . - msg msg: is a label. It can contain anything “but we are storing a string buffer in it”.\nWhat else can it contain? A future topic. len is a assembly-time constant. It calculates and contains the length of the buffer. It does this using the . directive in GAS.\nThe . directive represents the current address in memory, where current address is defined as how much the assembler has moved in the memory. When we created a label msg, memory started to get occupied. Once that is done, we came on len. Now the pointer is just after where the buffer finished. This is what the . directive “in GAS” represents. And the msg label is a pointer to the start of the “hello world” buffer. Subtracting the two gives us the effective length of the “hello world!” buffer. Data section completed.","syscall-setup#Syscall Setup":"mov rax, 1\nBy convention, rax is used to specify the system call number. 1 identifies the “write” system call. move rdi, 1 sets the file descriptor to 1, which is used for standard output. This ensures that we see “hello world!\\n” in the terminal.\nmov rsi, offset msg sets rsi to hold the pointer to the message buffer.\noffset is a GAS directive. mov rdx, len sets the length of the buffer in rdx.\nsyscall, invoke the syscall.","the-instructions-section#The Instructions Section":".section .text marks the start of text section. This section contains the instructions to the CPU. Basically, here goes the actual code.\n.global is used to declare a symbol as globally accessible. Here, globally accessible means accessibility outside of the current file.\nAgain, an assembler directive, allowing a symbol to be accessed in other files or modules during the linking process. If you remove it, the code will still work. But it is important when there are multiple files.\nExample, a C project might have multiple source files combined to generate one binary. There it becomes important. _start is the memory location (a label) where the execution starts from. Same as main() in C.\nIt is called by the OS. It is the entry-point label. .global _start makes _start globally accessible.\nNote: _start is just another label. But this is what everyone has mutually agreed upon. Thus, assemblers look for it. We can define our own label and tell the linker to use it as well.","the-write-syscall#The \u003ccode\u003ewrite\u003c/code\u003e Syscall":"write(fd=rdi, buffer=rsi, buffer_len=rdx)` The write syscall is used to output data to a file descriptor.\nIt takes 3 arguments which are passed through registers.\nArgument 1: File descriptor.\nThis defines where to write or where the output would go. It goes in the rdi register. 0 for stdin. 1 for stdout. 2 for stderr. Argument 2: Pointer to the buffer that contains the data to be written.\nThis defines where is the item that has to be written. This goes in rsi register. Argument 3: Length of the buffer.\nThis defines the number of bytes to write. This goes in the rdx register."},"title":"Hello, World!"},"/gitbook/docs/low-level-architecture/orientation/history/":{"data":{"":"Assembly is a low-level programming language, which provides direct-access over computer hardware. It almost maps one-to-one with machine instructions.\nAssembly is the closest we can get to the CPU while still understanding what’s going on.\nWhat distinguishes assembly from other programming languages is that every CPU architecture (like x86, ARM, MIPS, RISC-V) has its own assembly language.\nWhy different flavors of assembly exist?\nBecause of the Instruction Set Architecture (ISA). Each CPU has its own set of instructions, which creates demand for specific-assembly.","1-core-purpose#1. Core Purpose":"High level languages abstract the core functionality. Their core purpose is to provide the programmer with ease of usability and reduce development time.\nAssembly, on the other hand, is completely raw. There’s no abstraction. Everything is open and the programmer has to write every single instruction themselves.\nThis exposes the reality that even a simple “Hello, World!” program requires multiple low-level steps before it can actually run.","2-platform-dependency#2. Platform Dependency":"We are used to download python for our windows, Linux or mac machine and start coding. We don’t have to worry about how CPU will understand our hand writing.\nBut assembly doesn’t work like that. It is architecture-dependent. Assembly written for x86 will not work on ARM.","3-control--convenience#3. Control \u0026raquo; Convenience":"Assembly gives you maximum control, at the cost of convenience.\nWhat is convenience?\nPretty and verbose named variables? Not in assembly. You work directly with memory and registers. Data types? Everything is just bytes. It’s up to you to interpret the bytes as intended. Control flow? Nothing is built in. You implement it using jump instructions. Loops? You use jump statements to create them as well. I/O? Use direct syscalls. No dedicated printf and scanf Data Structures? Functions? DO. IT. YOURSELF. You have got every raw material, and building anything is your responsibility.\nHow these things exist then? We will explore each of them soon.","architectural-history#Architectural History":"The x86 CPU architecture, which refers to a family of processors — 8086, 80186, 80286, 80386, 80486 — was originally developed by Intel Corporation, starting in 1978.\nThe x86 ISA is a Intel’s proprietary, but other companies (notably AMD) were licensed to create compatible CPUs.\nAll these processors share the same Instruction Set Architecture (ISA) and are collectively known as x86.\nThe original 8086 was a 16-bit processor. Intel extended it to 32-bit, known as IA-32 (or x86_32). Then in 2003, AMD extended the architecture to 64-bit, releasing AMD64. Intel later adopted the same ISA, calling their version Intel 64 — both are functionally identical.\nSince Intel designed the architecture, they also defined the original syntax for its assembly — known as, Intel syntax. This style is widely used in Microsoft development tools and the Windows ecosystem.\nBut Intel is not the only syntax for x86 processors.","difference-between-assembly-and-other-languages-c-python#Difference Between Assembly And Other Languages (C, Python\u0026hellip;..)":"","fun-fact#Fun Fact":"C is called portable assembly.","the-origins-of-att-syntax#The Origins of AT\u0026amp;T Syntax":"Along the same time, AT\u0026T, through Bell Labs, created Unix in 1969, originally for PDP-7 machines, which was initially written in assembly. By 1973, Unix was rewritten in C, making it portable.\nIn 1978, Intel introduced the x86 architecture, which eventually became popular on personal computers.\nIn 1983, GNU Project was launched to create a free, Unix-like operating system. It needed a new assembler and adopted a new assembly syntax for x86 that was more consistent and easier to parse than Intel’s. Also, they wanted to avoid the proprietary issues, and this created the need for a new syntax.\nThis new syntax, although created by GNU, was based on conventions from Unix systems — and since Unix was synonymous with AT\u0026T, this style became popular as AT\u0026T syntax.\nWhen Linux was created in 1991, it relied heavily on GNU tools like GCC and GAS, which used AT\u0026T syntax. As a result, AT\u0026T syntax became the de facto in the Linux ecosystem.","which-syntax-we-are-going-to-use#Which Syntax We Are Going To Use?":"We are learning assembly, not syntaxes. The only thing that matters here is an assembler. Because an assembly code heavily uses assembler directives. That’s it.\nWe will write intel syntax and use GAS. Because, the operand style matches the mathematical style, LHS = RHS.\n// Intel mnemonic destination, source // AT\u0026T mnemonic source, destination Learn Assembly, Not Syntaxes."},"title":"Historical Introduction"},"/gitbook/docs/low-level-architecture/orientation/instructions/":{"data":{"":"An instruction is an atomic operation that tells the CPU what to do.\nmnemonic destination, source Mnemonic is the actual CPU operation. Destination and source are the operands it is generally performed on.\nSome mnemonics take one operand only. So, this is not strict.\nExample:\nmov rax, 1 move 1 into rax register. It also aligns with the mathematical assignment of values, a = 4, assign 4 to a.","cmp#\u003ccode\u003ecmp\u003c/code\u003e":"It compares two values by subtracting them, later deciding what might be the case.\nIn C, we can do something like this: a = (4 \u003e 2) and a will contain the result. However, that’s not the case here.\nSyntax:\ncmp a, b which is evaluated as a - b.\nWhen we do cmp 4, 2, cmp does 4-2, and the result is 2. This result is not stored. Instead, certain CPU flags are changed based on the result. Jump statements use these flags to decide what to do next.","common-instructions#Common Instructions":"","common-operations#Common Operations":"There are hundreds of instructions in assembly. But the core ones are as follows.","cpu-flags#CPU Flags":"CPU flags are binary indicators (either 0 or 1) that reflect the outcome of certain operations or hold special status information. They’re part of the processor’s status register, which is used by instructions like cmp (compare) and test.\nWhen an instruction modifies the flags, other instructions can check the state of these flags to make decisions, like jumping to different parts of code based on conditions.\nExample:\nFlag Description ZF (Zero Flag) Set to 1 if the result of an operation is zero; otherwise, it’s 0. SF (Sign Flag) Set to 1 if the result of an operation is negative (the most significant bit of the result is 1). There are other flags as well.","jump-statements#Jump Statements":"They change the flow of execution. Instead of executing the next line, they send the CPU to another part of the code based on some condition. This is what if-else stands on.\nThere are two types of jumps, conditional and unconditional.\nAn unconditional jump always goes to some label, no matter what. jmp some_label is an unconditional jump.\nA conditional jump is based on the flags set by cmp.","memorypointer-dereferencing#Memory/Pointer Dereferencing":"It refers to obtaining the actual value stored at a memory location.\nIt is done by [].\nFor example, if a memory location like 100000 stores a number, such as 45, dereferencing the memory location would give 45, like this, [100000] = 45","mov#\u003ccode\u003emov\u003c/code\u003e":"In simple words, it is assignment operator (=) in assembly.\nSyntax:\nmov destination, source Mathematically, it is destination = source.\nMost commonly, these operands are registers like rax, rsi etc…. But there are other options as well.\nmov rax, rsi means rsi = rax. mov rax, [rsi]: dereference the value in rsi and put it into rax. mov [rsi], rax: dereference the value in rsi and store what’s inside rax in there. Note: mov copy data from one place to other. Its not ‘move’ in literal sense.","type-specifier#Type Specifier":"Type specifiers are used to explicitly tell the assembler what size of data we’re working with while accessing memory.\nThey ensure that the assembler knows how much data to read or write.\nCommon type specifiers include:\nbyte ptr: load only 1-byte from the memory address. word ptr: load a word or 2-bytes (in x86_64) from the memory address. dword ptr: load a double word or 4-bytes from the memory address. qword ptr: load a quad word or 8-bytes from the memory address. They are particularly important (actually necessary) when working with memory operands and dereferencing pointers because x86_64 architecture can handle different size of data (like bytes, words, double words, etc).\nMany assemblers offer separate mnemonics for special data movements, like GAS."},"title":"Instructions"},"/gitbook/docs/low-level-architecture/orientation/invisible-symbols/":{"data":{"":"What are symbols? why everything is a symbol?\n. prefixed symbols"},"title":"The Idea Behind \"Invisible Symbols\""},"/gitbook/docs/low-level-architecture/simd/":{"data":{"":"Why rsp is 16 bytes aligned when we are on 8-byte arch?"},"title":"SIMD Instructions"},"/gitbook/docs/low-level-architecture/type-casting/":{"data":{"":"Implicit, explicit and pointer type casting"},"title":"Type Casting"},"/gitbook/docs/understanding-hello-world/":{"data":{"":"Everyone starts their coding journey with Hello, World!. It takes only 4 lines to do this in C:\n{% code title=“hello.c” %}\n#include\u003cstdio.h\u003e int main(void){ printf(\"Hello, World!\\n\"); } {% endcode %}\nBut how does it really work? Let’s find out.\nNote: It’s a complete rabbit hole. And this exploration will prove that hello-ing the world is not that easy.\nIt’s going to be a long journey, so sit tight and stay hopeful."},"title":"Understanding Hello World"},"/gitbook/docs/understanding-hello-world/a-brief-introduction-to-elf/":{"data":{"":"A Brief Introduction To ELFAn executable file is a type of computer file containing instructions that the computer’s processor can directly execute.\nExecutable files differ greatly on Windows and Linux. The reason is that both the environments are designed differently.\nProperty Linux Windows Binary File Format Executable and Linkable File Format (or ELF) Portable Executable File Format (or PE) File Extension No extension required .exe, .dll System Call Interface Linux System Calls (syscall) Windows NT syscall layer or WinAPI Dynamic Linker/Loader ld-linux.so Windows Loader C Runtime Library GNU C Library (or glibc) Microsoft C Runtime (MSVCRT) Linker ld link.exe Calling Convention System V AMD64 ABI Microsoft x64 ABI Format For Dynamic Linking Shared objects (.so) Dynamic Link Library (.dll)","a-brief-introduction-to-elf#A Brief Introduction To ELF":"","brief-history#Brief History":"It was first released in “System V ABI, Release 4”.\nIn 1999, it was chosen as the standard binary file format for Unix and Unix-like systems on x86 processors by the 86open project.","modern-elf#Modern ELF":"ELF is a general-purpose file format in Linux ecosystem, which defines the structure for binaries, libraries, and core files.\nIt is used throughout the build and execution pipeline.\nDifferent “types” of ELF files (relocatable, executable, shared object) exist to serve different roles in this pipeline. These are typically created during specific phases.","structure-of-an-elf-file#Structure Of An ELF File":"Regardless of the type of ELF, the structure of an ELF file remains mostly the same.\nAn ELF file can be divided into 4 parts.\nIf you assembly this code and stop before linking, you’ll get an object file.\n{% code title=“hello.c” %}\n#include ​ int main(void){ printf(\"Hello, World!\\n\"); } {% endcode %}\ngcc -c hello.c hello_object.o Object files are binary representations of a program’s source code.\nELF is not reserved for executable files only. It is used by a variety of other files of the same genre, and object code (.o) \u0026 shared object (.so) libraries are two of them.\nELF files aren’t readable by a text editor. Therefore, we use some command line utilities. The two most versatile utilities are objdump and readelf.","understanding-the-result-of-file#Understanding The Result Of `file`":"We can check the type of this object file using the file utility. It is designed to read certain bytes in a file to find the type of it.\n$ file hello_object.o hello_object.o: ELF 64-bit LSB relocatable, x86-64, version 1 (SYSV), not stripped LSB tells that the binary is structured in little-endian format, which means that the the least significant bit (or LSB) comes first. It is different from big-endian where the MSB comes first. In simple words, normal arithmetic is big-endian based and CPU arithmetic is little-endian based. relocatable means the file is ready to be linked with shared libraries but not for execution. A relocatable ELF is one which has unresolved references to symbols whose definition lies in shared libraries. For example, printf comes from glibc. not stripped means that the file still contains items which are not necessary and the code will still function the same if they are removed. version 1 (SYSV) means it uses System V AMD64 ABI (for conventions). x86-64 tells we are on 64-bit Linux. A linked binary file also follows the same structure. The above object file can be linked like this:\ngcc hello_object.o -o hello_elf Lets check this one.\n$ file hello_elf hello_elf: ELF 64-bit LSB pie executable, x86-64, version 1 (SYSV), dynamically linked, interpreter /lib64/ld-linux-x86-64.so.2, for GNU/Linux 3.2.0, not stripped pie executable means position independent executable. No static locations are required. dynamically linked means the binary resolves references during runtime via an interpreter called ld-linux.so (can be different though). Here comes the end of a brief introduction to ELF. Very soon we will explore it in depth."},"title":"a-brief-introduction-to-elf"},"/gitbook/docs/understanding-hello-world/a-brief-introduction-to-processes-in-linux/":{"data":{"a-brief-introduction-to-processes-in-linux#A Brief Introduction To Processes In Linux":"A Brief Introduction To Processes In Linux","a-wedding-ceremony#A Wedding Ceremony":"Everyone is familiar with wedding ceremonies. It is a high chance that you have witnessed a marriage in your own family.\nAlthough marriages vary a lot based on culture and traditions, but a few things are consistent across everything.\nYou have to decide the venue where everyone in your family would come together and the act of marriage would happen. It might be an open garden, a banquet hall or something else.\nNext you have to decide what kind of decoration you want to have at the venue. Flower decoration, color choices, ceiling decoration, stage background, lights etc….\nA meal is also offered to everyone who comes to bless the bride and the groom. So you decide the menu.\nA return gift is also offered to the closed ones so you have to decide on that.\nDepending on culture and traditions, this thing can go long or short. But every wedding does at least this as a bare minimum.\nA marriage is a complete process, isn’t it? And the act of marriage (the ceremonial part) is only a small part when compared to everything that goes behind. Only that is shown to everyone but when you count what enables it, you know that the act of marriage is just a tiny part of the whole process.\nIf that makes sense, you have already understood processes at a higher level. You only lack terminologies and the nuances, which we are going to be exploring very soon.","introducing-processes-in-linux#Introducing Processes In Linux":"A process is a container created by the operating system that holds all the necessary resources to execute a program.\nWhat are these necessities?\nVirtual address space, or the marriage venue. Segments, a.k.a the preparations. Program headers, or the schedule, the invitation card. A dynamic interpreter, a.k.a the not necessary, but very common person these days, a wedding anchor. Relocations, or the event-day preparations. Execution, a.k.a the day of marriage. At last, cleanup.","premise#Premise":"When we execute a program, what really happens?\n$ whoami my_username_is_amazing That’s what we are going to find now.","program-headers#Program Headers":"Program headers are a set of structures in an ELF file that describe how to create a process image in the memory.\nIt describes how the operating system should load the ELF binary into the memory. It maps parts of the binary into memory regions with specific permissions and purposes.\nThat’s all we need to know about processes for now.\nNote: This is just the start. We will explore processes in real depth when the time comes. Until then, lets move on to the next thing.","segments#Segments":"Segments are logical grouping of multiple sections.","virtual-address-space#Virtual Address Space":"There are thousands of processes running normally. The number rises when we multitask.\nJust imagine how crazy and chaotic it would get to manage millions of process inside the RAM, with all demanding various services like stack and heap.\nThis is why an abstraction known as virtual address space (or VAS) exist. Every process is executed in an isolated environment called virtual address space.\nA process image is the complete in-memory layout of a program after it has been loaded into memory by the OS.\nIt is the answer to the question, “What the process looks like in the RAM?” A process image is the memory representation of a program at runtime.\nIt includes code, data, stack, heap, environment, memory-mapped regions, loaded libraries etc….\nIt is created by the kernel based on the ELF layout."},"title":"a-brief-introduction-to-processes-in-linux"},"/gitbook/docs/understanding-hello-world/a-high-level-overview-of-build-process-in-c/":{"data":{"a-misconception-about-gcc#A Misconception About GCC":"GCC isn’t just a compiler—it’s actually a toolchain. If it were only a compiler, how could it assemble and link code?","assembling#Assembling":"The assembly code undergoes a transformation process that lays the foundation for linking. This involves several steps, including:\nLexing and parsing the assembly source Encoding instructions into machine code Creating sections Resolving labels within the file Generating the symbol table Creating relocation entries for unresolved references Constructing ELF headers The object code can be generated as:\ngcc -c hello.s -o hello.o The file produced in this step is an object file with a .o extension.\nObject files are strict in structure and follow a format called the Executable and Linkable Format (ELF).\nThis object file isn’t an executable yet. It needs to be linked.","build-process-in-c#Build Process In C":"Build Process In CTo compile this source, we can use a variety of compilers.\n{% code title=“hello.c” %}\n#include ​ int main(){ printf(\"Hello, World!\\n\"); return 0; } {% endcode %}\nHere we are using gcc.\n$ gcc hello.c -o hello_executable $ ./hello_executable ​ Hello, World! Although it looks simple, this process has four layers of hidden complexity. Let’s peel them back to understand what happens under the hood.\nA source code turns into an executable file through these four steps:\nPreprocessing Compilation Assembling Linking Let’s dive into each.","compilation#Compilation":"The intermediate C code is compiled into assembly instructions—the closest we get to the CPU while still keeping it somewhat readable.\nThe assembly flavor (Intel or AT\u0026T) depends on the assembler used to compile the source code.\nIf GNU Assembler is used, it generates AT\u0026T assembly by default. Although it can be configured to generate Intel assembly as well. Same is followed by gcc. If netwide assembler is used, it generates Intel assembly. Architecture-specific details (x86 and x86_64) are handled by the assembler.\nTo compile the intermediate C code into assembly code, we do:\ngcc -S -masm=intel hello.i -o hello.s","linking#Linking":"To make the object code executable, we link it with the necessary libraries.\ngcc hello.o -o hello_elf In the above program, we are using a function called printf for printing Hello, World! to the output.\nWhere is that function coming from? The header file! Where is the header file coming from? glibc! Where is glibc? Somewhere on the OS! Object code contains unresolved references to various library functions. Until these are resolved, the file cannot be executed.\nLinking can be static or dynamic, and both have their use cases.\nDynamic linking is commonly used, but we can also instruct the compiler to link statically.\nNow the binary is ready to be executed.\n$ ./hello_elf Hello, World!","preprocessing#Preprocessing":"Every C program includes at least this line: #include , where #include is a preprocessing directive.\nThese directives must be handled before we move further.\nThis preprocessing is carried out using:\ngcc -E hello.c -o hello.i This step produces an intermediate .i file, a raw C file where all preprocessing directives are resolved.\nNote: If you look at hello.i and stdio.h side by side, you’ll see it isn’t a direct copy. That’s because the header file contains various macros, and preprocessing continues until all directives are resolved.\nFor more information on preprocessing, check out preprocessing-directives.md."},"title":"a-high-level-overview-of-build-process-in-c"},"/gitbook/docs/understanding-hello-world/c-greater-than-assembly/":{"data":{"c---assembly#C -\u0026gt; Assembly":"C -\u003e AssemblyLet’s start our journey with the output of compilation.\nThe source can be compiled into assembly using gcc -S -masm=intel hello.c -o hello_asm.s Previously, we’ve learned that void main(); is a wrong signature for main function. But if we compile the code with void main(); signature, we get an almost similar assembly.\n{% code title=“C_source_to_assembly.asm” %}\n.file \"hello.c\" .intel_syntax noprefix .text .section .rodata .LC0: .string \"Hello, World!\" .text .globl main .type main, @function main: .LFB0: .cfi_startproc push rbp .cfi_def_cfa_offset 16 .cfi_offset 6, -16 mov\trbp, rsp .cfi_def_cfa_register 6 lea\trax, .LC0[rip] mov\trdi, rax call puts@PLT nop pop\trbp .cfi_def_cfa 7, 8 ret .cfi_endproc .LFE0: .size\tmain, .-main .ident\t\"GCC: (Debian 14.2.0-19) 14.2.0\" .section\t.note.GNU-stack,\"\",@progbits {% endcode %}\nAnd this is the assembly generated for int main(void); signature.\n{% code title=“C_source_to_assembly.asm” %}\n.file \"hello.c\" .intel_syntax noprefix .text .section .rodata .LC0: .string \"Hello, World!\" .text .globl main .type main, @function main: .LFB0: .cfi_startproc push rbp .cfi_def_cfa_offset 16 .cfi_offset 6, -16 mov\trbp, rsp .cfi_def_cfa_register 6 lea\trax, .LC0[rip] mov\trdi, rax call puts@PLT mov eax, 0 pop\trbp .cfi_def_cfa 7, 8 ret .cfi_endproc .LFE0: .size\tmain, .-main .ident\t\"GCC: (Debian 14.2.0-19) 14.2.0\" .section\t.note.GNU-stack,\"\",@progbits {% endcode %}\nIt’s clear that only line 21 is different, from nop to mov eax, 0.\nnop translates to no operation. We need not to think about it right now. In the other one, we are zeroing the accumulator to pass the exit code to the next sequence in the pipeline. Lets understand this assembly now.\n.file is an assembler (GAS) directive to make the file name available to the binary. .intel_syntax noprefix is a GAS specific directive, which is specified to use intel style assembly. .text marks the start of code section. .rodata marks the start of a read-only section. .LC0 is a label for a literal constant. .string is used to define null-terminated, C-style string literals. This is where our \"Hello, World\\n\" goes. .globl makes a symbol visible to the linker. .type main, @function specifies that main is a function symbol. .LFB0 stands for local function begin label, used internally by GCC for debugging info. push rbp pushes the base pointer of current frame on stack. mov rbp, rsp sets up a new stack frame. lea rax, .LC0[rip] uses RIP-relative addressing to load the address of “Hello, World!\\0” string. mov rdi, rax moves the address of the string into destination index register. call puts@plt calls the puts function in glibc via procedure linkage table (plt). We’ll expand on it further. mov eax, 0 zeroes the accumulator to send as exit code. pop rbp pops the current base address. ret return. If you are not learning passively, you are definitely wondering what about .cfi_* directives?\nIt stands for call frame information directives. And we don’t have to worry about them. There is something that is missing here. Can you spot it? There is no exit syscall. There is nothing like\nmov rax, 60 xor rdi, rdi syscall Exit is never controlled by our source code. When we write raw assembly, we manage exit ourselves. When we use shared libraries, we are using a complete infrastructure to run something. Now it is the duty of the infrastructure to take care of this. In the upcoming articles, we will find that there is so much that goes before our source code gets executed and there is so much that comes after it is executed, we’ll understand how tiny our source code really is. If we visit https://godbolt.org/ and paste our source code there, we can find that the assembly generated there is very different. Something like this:\n{% code title=“asm_from_godbolt_org.asm” %}\n.LC0: .string \"Hello, World!\" main: push rbp mov rbp, rsp mov edi, OFFSET FLAT:.LC0 call puts mov eax, 0 pop rbp ret {% endcode %}\nIn the right section, where the assembly part is displayed, you can find a clickable link to Libraries. Above that is green tick. Click on that and you will find that different options are passed to the compiler to optimize the command.\nIt is the result of those compiler options that we see such a simple and stripped away assembly. By default, there is no optimization done on the code, which is why it is lengthy and readable. Shortening (optimizing) the code would result in less readability, which is not good for us as are trying to understand things.\nAnd we will stick to no optimization. And we have walked the first step. This marks the end of understanding assembly.\nNow we will move to object code."},"title":"c-greater-than-assembly"},"/gitbook/docs/understanding-hello-world/linked-elf-analysis/":{"data":{"linked-elf-analysis#Linked ELF Analysis":"Linked ELF Analysis","macro-level-roadmap#Macro Level Roadmap":"Complete output of readelf can be found at GitHub.\nstep-1-analyzing-elf-headers.md Before moving to the next step, it would be nice to clear a doubt, if it exists. full-disassembly.md The next step is analyzing program headers table but it has got some prerequisites. part-ii-analyzing-section-headers.md a-brief-introduction-to-processes-in-linux.md step-2-analyzing-program-headers-table.md","setup#Setup":"We had the object code from previous step, which we can link in order to make it an executable.\ngcc object_code.o -o linked_elf This one is going to be quite lengthy. So, be prepared.","type-inspection#Type Inspection":"Lets start by inspecting the type of this file.\n$ file linked_elf linked_elf: ELF 64-bit LSB pie executable, x86-64, version 1 (SYSV), dynamically linked, interpreter /lib64/ld-linux-x86-64.so.2, for GNU/Linux 3.2.0, not stripped For comparison, this was the output for object code.\n$ file object_code.o hello.o: ELF 64-bit LSB relocatable, x86-64, version 1 (SYSV), not stripped The first thing that is changed is relocatable is transformed into pie executable. This tells that the binary has got execution powers.\nThe word pie matters here. It stands for position independent executable. A pie executable is loaded randomly in the memory. It is not fixed, unlike non-pie executable binaries. It is dynamically linked . It means the ELF binary depends on external shared libraries that are loaded at runtime by the dynamic linker.\ninterpreter /lib64/ld-linux-x86-64.so.2 is the program that helps the binary achieve its dynamic nature. It loads the shared libraries, resolves the cross-references and do so extra stuff we’ll see very soon, which is very very interesting.\nI have omitted the BuildId thing. It has no use.\nfor GNU/Linux 3.2.0 means the binary was built targeting Linux kernel version 3.2.0 as the minimum ABI (Application Binary Interface) compatibility level.\nIt can run on Linux 3.2.0 or any newer kernel. It may fail on older kernels due to missing syscalls or ABI changes.","what-really-happens-during-linking#What really happens during linking?":"This question is really hard to answer for me at this point. Why not to see it directly?"},"title":"_index"},"/gitbook/docs/understanding-hello-world/linked-elf-analysis/after-relocation/":{"data":{"after-relocation#After Relocation":"After RelocationAfter .rela.dyn entries are processed, the interpreter looks at the PLT entries. Since lazy binding is enabled by default, relocations for .rela.plt are deferred.\nThe interpreter now sets up the PLT stubs to point to the dynamic resolver (_dl_runtime_resolve) via the global offset table, skips JMPREL and leaves the .got.plt entries unresolved.\n.init_array is a section used to store pointers to initialization functions (constructors) that run before main(). It’s mostly relevant in C++ or programs with global constructors. For simple C programs like hello world, it’s typically unused or trivial, and doesn’t affect the understanding of relocations, GOT/PLT, or dynamic linking — so it’s safe to skip for now.\nNow the interpreter program calls the INIT function (from INIT dynamic tag).\nAfter it is done with INIT, it transfers the control to the program’s entry point which _start .\n_start calls `__libc_start_main()` , which calls main . main does its work and return the exit code in the accumulator register. Remember this instruction? 114c:\tb8 00 00 00 00 mov eax,0x0 The control is given back to the `__libc_start_main()` function, which receives the exit code and calls the exit(return_value) function from libc.\nexit() runs, calls destructors from .fini-array and finally, the exit syscall comes in effect to terminate the program.\nThe kernel receives the control back, does some cleanup and sends SIGCHLD to the parent process.","conclusion#Conclusion":"Ladies and gentlemen, this marks the end of statically analyzing the hello world binary. ~35 days of chaos, but worth it."},"title":"after-relocation"},"/gitbook/docs/understanding-hello-world/linked-elf-analysis/full-disassembly/":{"data":{"full-disassembly#Full Disassembly":"Full Disassembly","premise#Premise":"While analyzing the object code, we have checked out full disassembly of the compiled file. We can do that here as well.\nobjdump linked_elf -D -M intel Shocked, right! The disassembly is 800 lines long. The first and the immediate question is WHERE ALL THIS ASSEMBLY IS COMING FROM?","the-big-picture#The Big Picture":"Our source code is a tiny part of the bigger picture.\nThere exist a complete infrastructure that runs this hello world thing. And the build process is what that constructs this infrastructure, step-by-step.\nIn short, it is all compiler (gcc) generated.","what-is-this-infrastructure-really-about-what-does-it-look-like#What is this infrastructure really about? What does it look like?":"That’s a very important question to ask, not a idea to gloss over just because it is complex to answer.\nAt the end of the day, everything is just code. There are bunch of object files that makes up the invisible infrastructure.\nThe next question would be, how does all of this code become one single entity?\nThat’s the job of linking. It’s not the only thing that the linker do, but one of the things. Each object code has its own ELF structure. When the linker combines them, a unified structure comes out, which is our final binary.","what-to-do-with-this-disassembly#What to do with this disassembly?":"This disassembly exposes almost everything, therefore, it is a roadmap for us. But to read that roadmap, we have to understand the things it points to.\nThere is no meaning in reading it line by line. We will use it in the process to make sense of what we are doing and why we are doing it.\nAlthough I could have avoid it now because we are going to visit it anyway, but I don’t want to make those visits look strange. It is possible that someone had visited it earlier, it would mess with their mind. This is the reason why I decided to left an explanation before it consumes that learner. Throughout the process, we are going to use the full disassembly. It’s an incredible tool at our disposal. With that in mind, we can move now."},"title":"full-disassembly"},"/gitbook/docs/understanding-hello-world/linked-elf-analysis/global-offset-table/":{"data":{"a-generalized-structure#A Generalized Structure":"---------------------------------------------------------------------------------------- | // Eager Binding Division -\u003e .got at offset 0x0000 (RELA) (.rela.dyn) | | ---------- -------------------------- ---------- -------------------------- | | | GOT[0] | -\u003e | *(func/symb 1) | -\u003e | 0x0000 | -\u003e | Actual Runtime Address | | | ---------- -------------------------- ---------- -------------------------- | | | GOT[1] | -\u003e | *(func/symb 2) | -\u003e | 0x0008 | -\u003e | Actual Runtime Address | | | ---------- -------------------------- ---------- -------------------------- | | | GOT[2] | -\u003e | *(func/symb 3) | -\u003e | 0x0010 | -\u003e | Actual Runtime Address | | | ---------- -------------------------- ---------- -------------------------- | | | GOT[3] | -\u003e | *(func/symb 4) | -\u003e | 0x0018 | -\u003e | Actual Runtime Address | | | ---------- -------------------------- ---------- -------------------------- | | | GOT[4] | -\u003e | *(func/symb 5) | -\u003e | 0x0020 | -\u003e | Actual Runtime Address | | | ---------- -------------------------- ---------- -------------------------- | | .... | | .... | | ---------- -------------------------- ---------- -------------------------- | | | GOT[N] | -\u003e | *(func/symb N) | -\u003e | 0x.... | -\u003e | Actual Runtime Address | | | ---------- -------------------------- ---------- -------------------------- | |--------------------------------------------------------------------------------------| | // Lazy Binding Division -\u003e .got.plt at offset 0x0028 (JMPREL) (.rela.plt) | | ---------- -------------------------- ---------- -------------------------- | | | GOT[0] | -\u003e | *(.dynamic) | -\u003e | 0x0028 | -\u003e | Actual Runtime Address | | \u003c- Reserved for enabling lazy binding | ---------- -------------------------- ---------- -------------------------- | | | GOT[1] | -\u003e | *(link_map) | -\u003e | 0x0030 | -\u003e | Actual Runtime Address | | \u003c- Reserved for enabling lazy binding | ---------- -------------------------- ---------- -------------------------- | | | GOT[2] | -\u003e | *(_dl_runtime_resolve) | -\u003e | 0x0038 | -\u003e | Actual Runtime Address | | \u003c- Reserved for enabling lazy binding | ---------- -------------------------- ---------- -------------------------- | | | GOT[3] | -\u003e | *(func 1) | -\u003e | 0x0040 | -\u003e | Actual Runtime Address | | | ---------- -------------------------- ---------- -------------------------- | | | GOT[4] | -\u003e | *(func 2) | -\u003e | 0x0048 | -\u003e | Actual Runtime Address | | | ---------- -------------------------- ---------- -------------------------- | | .... | | .... | | ---------- -------------------------- ---------- -------------------------- | | | GOT[M] | -\u003e | *(func M) | -\u003e | 0x.... | -\u003e | Actual Runtime Address | | | ---------- -------------------------- ---------- -------------------------- | ---------------------------------------------------------------------------------------- But, this table is still incomplete. And we will complete it in the next section, which is procedure linkage table.","conclusion#Conclusion":"I took ~5 days to write understand global offset table and write this article. Its painful, chaotic, confusing, agitating, frustrating and what not. But it is worth it.\nThank you. Next we would go through PLT as it is necessary to understand .rela.plt based relocations.\nUntil then, take rest.","finding-the-structure-of-global-offset-table#Finding the structure of global offset table":"","global-offset-table#Global Offset Table":"Global Offset TableSo far, we know that when we are talking about relocation, we are updating a placeholder value at an offset in the loaded segments by the actual runtime address of the symbol.\nBut, there is a problem. To update means to write. To write something, you need write permission, correct? Is the .text section writable? Or, should it be writable?\nSee, the symbols we have resolved so far, like the __libc_start_main function symbol, these are called by the _start symbol. And they are a part of the .text section. We know that .text is not limited to our source code only. But, if the .text section is writable, doesn’t that pose a security risk?\nFirst of all, is the .text section writable? No.\nHow to find whether a section is writable or not? Check the `Flags` attribute in the section headers table.\n[14] .text PROGBITS 0000000000001050 00001050 0000000000000103 0000000000000000 AX 0 0 16 The flags are AX here, which means allocate and execute. This section is clearly not writable. Also, the segment it belongs to, which is the 2nd LOAD segment in the program headers table (checkout section to segment mapping just below the program headers table), that is also not writable. LOAD 0x0000000000001000 0x0000000000001000 0x0000000000001000 0x000000000000015d 0x000000000000015d R E 0x1000 It says RE, which means, read-only and executable. Second, should a section like .text be writable?\nThe obvious answer would be NO. That poses a security threat. It shouldn’t be writable at runtime. Then where is relocation happening? Where we are patching the actual runtime address?\nTake this entry 000000003fc0 000100000006 R_X86_64_GLOB_DAT 0000000000000000 __libc_start_main@GLIBC_2.34 + 0 The offset is 0x3fc0. The section it belongs to is .got . The first shock.\nThe segment .got section belongs to is the 4th LOAD segment.\nCheckout the properties for this segment. LOAD 0x0000000000002dd0 0x0000000000003dd0 0x0000000000003dd0 0x0000000000000248 0x0000000000000250 RW 0x1000 R W? It is readable and writable. The second shock.\nNow we have to take help from the full disassembly.\nHave a look at the .text section, line 341 on wards.\n106b:\tff 15 4f 2f 00 00 call QWORD PTR [rip+0x2f4f] # 3fc0 \u003c__libc_start_main@GLIBC_2.34\u003e The above instruction is calling that libc function. In the comment, we can see that it resolves to a location 0x3fc0. Lets visit that.\nI suggest you to use VS Code’s Search functionality for this. Otherwise, I am already putting the lines numbers here.\nOffset 0x3fc0 is found at line 764. And what it points to? .got. And guess what, this offset matches the one in the relocation table. Disassembly of section .got: 0000000000003fc0 \u003c.got\u003e: ... The rest of the functions in the .rela.dyn table have the same story. But, here is the twist and it can only be found by people who have not lost their sanity so far. Offsets 0x3fc8 0x3fd0 3fd8 3fe0 are nowhere to be found in the .got section. Just a call in their respective sections but no entry in .got.\nIf you have found that, here is the answer. Global offset table is a runtime thing. It is meant to be filled dynamically, which is why no placeholder offsets are present in the .got disassembly as it is meaningless. We can also notice that the only sections which have got placeholder addresses are the ones which are read-only, and .got is writable.\nWe had a look at the disassembly. Can you infer anything from it? Why is it like this?\nThe .text section is not writable. So, it points to entries in a section which is writable. The section which is writable is responsible for providing the real runtime address of the symbol. The .text section calls the symbol indirectly via this entry. That’s how relocations are carried out.\nEnough building hype. Lets introduce global offset table now.","introduction-to-global-offset-table#Introduction to global offset table":"The Global Offset Table (GOT) is a table in an ELF binary used at runtime to hold the absolute addresses of global variables and functions, allowing for position-independent code (PIC) by deferring the resolution of symbol addresses until execution.\nEach entry in this table is an address. When it is created, it is a placeholder address, later, when it is resolved, it becomes the actual runtime address of that symbol.\nThe one thing that makes global offset table and procedure linkage table intimidating is the absence of the ability to visualize it. It is simple to say that it is just a pointer table but how does it really look like? That changes the game.","lets-dive-in#Lets dive in!":"We are going to start with this relocation entry.\n000000003fc0 000100000006 R_X86_64_GLOB_DAT 0000000000000000 __libc_start_main@GLIBC_2.34 + 0 The offset is 0x3fc0. If we locate it in the full disassembly, we can find that it points to the global offset table itself.\nIf we go to the disassembly of the _start symbol, we can find this instruction telling that the call to this function symbol points to this offset in the disassembly. And the offset is again 0x3fc0.\n106b:\tff 15 4f 2f 00 00 call QWORD PTR [rip+0x2f4f] # 3fc0 \u003c__libc_start_main@GLIBC_2.34\u003e This means that the first entry in the global offset table is allocated to __libc_start_main symbol.\nAnd it should not be hard to think that the rest of the entries in the .rela.dyn table follows the same trend. Just because it is a runtime table, those entries don’t exist.\nWe know that, each address in 64-bit architecture is 8-byte long. That means, addresses should be separated by 8 units. With that in mind, the eager binding part of the global offset table should look like this:\nGOT[0] -\u003e *(__libc_start_main) -\u003e 0x3fc0 (placeholder) -\u003e [Actual Runtime Address] GOT[1] -\u003e *(_ITM_deregisterTM) -\u003e 0x3fc8 (placeholder) -\u003e [Actual Runtime Address] GOT[2] -\u003e *(__gmon_start__) -\u003e 0x3fd0 (placeholder) -\u003e [Actual Runtime Address] GOT[3] -\u003e *(_ITM_registerTMCl) -\u003e 0x3fd8 (placeholder) -\u003e [Actual Runtime Address] GOT[4] -\u003e *(__cxa_finalize) -\u003e 0x3fe0 (placeholder) -\u003e [Actual Runtime Address] Now comes the lazy binding part.\nTo do lazy binding, you need to know certain things. Since we have not touched on lazy binding yet, we will keep it simple. There are 3 entries required to be reserved for lazy binding in the global offset table. These are offsets to .dynamic segment, link_map, and the runtime resolver function. Link map is a data structure which tracks all the loaded objects and runtime resolver function is the function that find those symbols in the loaded shared objects and resolve their runtime address. Since the last entry was at 0x3fe0 offset, the next entry should start at 0x3fe8, right? Have a look at the disassembly for .got.plt section. Just look at the offset, the disassembly is garbage.\nThat means, the lazy binding section in the global offset table should look like this:\nGOT[5] -\u003e *(.dynamic) -\u003e 0x3fe8 -\u003e [Actual Runtime Address] GOT[6] -\u003e *(link_map) -\u003e 0x3ff0 -\u003e [Actual Runtime Address] GOT[7] -\u003e *(runtime resolver) -\u003e 0x3ff8 -\u003e [Actual Runtime Address] GOT[8] -\u003e *(puts) -\u003e 0x4000 -\u003e [Actual Runtime Address] Combining both of them, the final structure should emerge something like this:\n---------- ------------------------ ---------- -------------------------- | GOT[0] | -\u003e | *(__libc_start_main) | -\u003e | 0x3fc0 | -\u003e | Actual Runtime Address | ---------- ------------------------ ---------- -------------------------- | GOT[1] | -\u003e | *(_ITM_deregisterTM) | -\u003e | 0x3fc8 | -\u003e | Actual Runtime Address | ---------- ------------------------ ---------- -------------------------- | GOT[2] | -\u003e | *(__gmon_start__) | -\u003e | 0x3fd0 | -\u003e | Actual Runtime Address | ---------- ------------------------ ---------- -------------------------- | GOT[3] | -\u003e | *(_ITM_registerTMCl) | -\u003e | 0x3fd8 | -\u003e | Actual Runtime Address | ---------- ------------------------ ---------- -------------------------- | GOT[4] | -\u003e | *(__cxa_finalize) | -\u003e | 0x3fe0 | -\u003e | Actual Runtime Address | ---------- ------------------------ ---------- -------------------------- | GOT[5] | -\u003e | *(.dynamic) | -\u003e | 0x3fe8 | -\u003e | Actual Runtime Address | ---------- ------------------------ ---------- -------------------------- | GOT[6] | -\u003e | *(link_map) | -\u003e | 0x3ff0 | -\u003e | Actual Runtime Address | ---------- ------------------------ ---------- -------------------------- | GOT[7] | -\u003e | *(runtime) | -\u003e | 0x3ff8 | -\u003e | Actual Runtime Address | ---------- ------------------------ ---------- -------------------------- | GOT[8] | -\u003e | *(puts) | -\u003e | 0x4000 | -\u003e | Actual Runtime Address | ---------- ------------------------ ---------- -------------------------- That’s it.","structure-of-got#Structure of GOT":"Global offset table is logically divided into two parts. But it is one entity at the lowest level.\nWe are building the binary with default options, i.e gcc hello.c -o hello_elf Default options use eager binding for startup functions and lazy binding for source code functions. And the distinction in global offset table is based on this binding principle only.\nThe global offset table starts from the eager binding section. This section is very simple as there is no requirement for anything extra. So, it is all offset entries.\nWhen the eager binding section ends, lazy binding section starts. And here we need certain entries before the actual relocation entries using PLT can come. After those entries, regular relocation entries start.\nThe best we can do to actually visualize how the global offset table looks like is to see the one for our binary. Since it is a runtime thing, we can’t actually see it right now, as we are doing static analysis. But that should not hinder our understanding, right?\nWhat we are going to do is, we are going to utilize the full disassembly of the .got section, relocation tables along with the theory to form a structure. By the way, we will verify it later when we do the dynamic analysis.","supplies#Supplies":"Relocation entries\nRelocation section '.rela.dyn' at offset 0x550 contains 8 entries: Offset Info Type Sym. Value Sym. Name + Addend 000000003dd0 000000000008 R_X86_64_RELATIVE 1130 000000003dd8 000000000008 R_X86_64_RELATIVE 10f0 000000004010 000000000008 R_X86_64_RELATIVE 4010 000000003fc0 000100000006 R_X86_64_GLOB_DAT 0000000000000000 __libc_start_main@GLIBC_2.34 + 0 000000003fc8 000200000006 R_X86_64_GLOB_DAT 0000000000000000 _ITM_deregisterTM[...] + 0 000000003fd0 000400000006 R_X86_64_GLOB_DAT 0000000000000000 __gmon_start__ + 0 000000003fd8 000500000006 R_X86_64_GLOB_DAT 0000000000000000 _ITM_registerTMCl[...] + 0 000000003fe0 000600000006 R_X86_64_GLOB_DAT 0000000000000000 __cxa_finalize@GLIBC_2.2.5 + 0 Relocation section '.rela.plt' at offset 0x610 contains 1 entry: Offset Info Type Sym. Value Sym. Name + Addend 000000004000 000300000007 R_X86_64_JUMP_SLO 0000000000000000 puts@GLIBC_2.2.5 + 0 The disassembly of the _start symbol to find where is __libc_start_main coming from.\nDisassembly of section .text: 0000000000001050 \u003c_start\u003e: 1050:\t31 ed xor ebp,ebp 1052:\t49 89 d1 mov r9,rdx 1055:\t5e pop rsi 1056:\t48 89 e2 mov rdx,rsp 1059:\t48 83 e4 f0 and rsp,0xfffffffffffffff0 105d:\t50 push rax 105e:\t54 push rsp 105f:\t45 31 c0 xor r8d,r8d 1062:\t31 c9 xor ecx,ecx 1064:\t48 8d 3d ce 00 00 00 lea rdi,[rip+0xce] # 1139 106b:\tff 15 4f 2f 00 00 call QWORD PTR [rip+0x2f4f] # 3fc0 \u003c__libc_start_main@GLIBC_2.34\u003e 1071:\tf4 hlt 1072:\t66 2e 0f 1f 84 00 00 cs nop WORD PTR [rax+rax*1+0x0] 1079:\t00 00 00 107c:\t0f 1f 40 00 nop DWORD PTR [rax+0x0] Global offset table section\nDisassembly of section .got: 0000000000003fc0 \u003c.got\u003e: ... Disassembly of section .got.plt: 0000000000003fe8 \u003c_GLOBAL_OFFSET_TABLE_\u003e: 3fe8:\te0 3d loopne 4027 \u003c_end+0x7\u003e ... 3ffe:\t00 00 add BYTE PTR [rax],al 4000:\t36 10 00 ss adc BYTE PTR [rax],al 4003:\t00 00 add BYTE PTR [rax],al 4005:\t00 00 add BYTE PTR [rax],al ..."},"title":"global-offset-table"},"/gitbook/docs/understanding-hello-world/linked-elf-analysis/part-ii-analyzing-section-headers/":{"data":{"analyzing-section-headers#Analyzing Section Headers":"Analyzing Section Headers","baseline-understanding-of-sections#Baseline Understanding Of Sections":"","conclusion#Conclusion":"Sections are used at build time. Segments are used at runtime. The final binary is a mixture of our source code and shared object files.","how-does-it-fit-in-the-bigger-picture#How does it fit in the bigger picture?":"As we have read recently that the infrastructure we are understanding is basically made up of various object files, which get combined with our source code and the final binary becomes an executable. Those object files are also ELF at the end of the day. They will also have sections like .text and .got . To form a combined binary, these sections are combined in a thoughtful way and a final version of that section emerges.\nTherefore, the .text section we see in the final elf binary doesn’t only contain our source code, but the source code from various shared object files.\nJust for knowledge, some of these files include crt1.o, crtn.o etc.\nThis can be verified by looking at the full disassembly.\nLine 341 on wards starts the disassembly of .text section. First, we have _start. Then deregister_tm_clones, register_tm_clones, __do_global_dtors_aux, frame_dummy and, at last, main. All these are coming from the shared object files.","what-are-sections#What are sections?":"Sections are formally defined logical divisions inside an ELF file in the ELF specification. They describe how an ELF file organizes its contents for linking, loading, and debugging.\nThese are the section headers in our binary.\nSection Headers: [Nr] Name Type Address Offset Size EntSize Flags Link Info Align [ 0] NULL 0000000000000000 00000000 0000000000000000 0000000000000000 0 0 0 [ 1] .note.gnu.pr[...] NOTE 0000000000000350 00000350 0000000000000020 0000000000000000 A 0 0 8 [ 2] .note.gnu.bu[...] NOTE 0000000000000370 00000370 0000000000000024 0000000000000000 A 0 0 4 [ 3] .interp PROGBITS 0000000000000394 00000394 000000000000001c 0000000000000000 A 0 0 1 [ 4] .gnu.hash GNU_HASH 00000000000003b0 000003b0 0000000000000024 0000000000000000 A 5 0 8 [ 5] .dynsym DYNSYM 00000000000003d8 000003d8 00000000000000a8 0000000000000018 A 6 1 8 [ 6] .dynstr STRTAB 0000000000000480 00000480 000000000000008d 0000000000000000 A 0 0 1 [ 7] .gnu.version VERSYM 000000000000050e 0000050e 000000000000000e 0000000000000002 A 5 0 2 [ 8] .gnu.version_r VERNEED 0000000000000520 00000520 0000000000000030 0000000000000000 A 6 1 8 [ 9] .rela.dyn RELA 0000000000000550 00000550 00000000000000c0 0000000000000018 A 5 0 8 [10] .rela.plt RELA 0000000000000610 00000610 0000000000000018 0000000000000018 AI 5 24 8 [11] .init PROGBITS 0000000000001000 00001000 0000000000000017 0000000000000000 AX 0 0 4 [12] .plt PROGBITS 0000000000001020 00001020 0000000000000020 0000000000000010 AX 0 0 16 [13] .plt.got PROGBITS 0000000000001040 00001040 0000000000000008 0000000000000008 AX 0 0 8 [14] .text PROGBITS 0000000000001050 00001050 0000000000000103 0000000000000000 AX 0 0 16 [15] .fini PROGBITS 0000000000001154 00001154 0000000000000009 0000000000000000 AX 0 0 4 [16] .rodata PROGBITS 0000000000002000 00002000 0000000000000012 0000000000000000 A 0 0 4 [17] .eh_frame_hdr PROGBITS 0000000000002014 00002014 000000000000002c 0000000000000000 A 0 0 4 [18] .eh_frame PROGBITS 0000000000002040 00002040 00000000000000ac 0000000000000000 A 0 0 8 [19] .note.ABI-tag NOTE 00000000000020ec 000020ec 0000000000000020 0000000000000000 A 0 0 4 [20] .init_array INIT_ARRAY 0000000000003dd0 00002dd0 0000000000000008 0000000000000008 WA 0 0 8 [21] .fini_array FINI_ARRAY 0000000000003dd8 00002dd8 0000000000000008 0000000000000008 WA 0 0 8 [22] .dynamic DYNAMIC 0000000000003de0 00002de0 00000000000001e0 0000000000000010 WA 6 0 8 [23] .got PROGBITS 0000000000003fc0 00002fc0 0000000000000028 0000000000000008 WA 0 0 8 [24] .got.plt PROGBITS 0000000000003fe8 00002fe8 0000000000000020 0000000000000008 WA 0 0 8 [25] .data PROGBITS 0000000000004008 00003008 0000000000000010 0000000000000000 WA 0 0 8 [26] .bss NOBITS 0000000000004018 00003018 0000000000000008 0000000000000000 WA 0 0 1 [27] .comment PROGBITS 0000000000000000 00003018 000000000000001f 0000000000000001 MS 0 0 1 [28] .symtab SYMTAB 0000000000000000 00003038 0000000000000360 0000000000000018 29 18 8 [29] .strtab STRTAB 0000000000000000 00003398 00000000000001db 0000000000000000 0 0 1 [30] .shstrtab STRTAB 0000000000000000 00003573 000000000000011a 0000000000000000 0 0 1 Key to Flags: W (write), A (alloc), X (execute), M (merge), S (strings), I (info), L (link order), O (extra OS processing required), G (group), T (TLS), C (compressed), x (unknown), o (OS specific), E (exclude), D (mbind), l (large), p (processor specific) Note: You may not find your output like this because I have formatted it manually so that it is clear and makes sense.\nWe are already familiar with the attributes of this table, so we’ll skip that. Lets have a look at these sections.","what-is-their-significance#What is their significance?":"Sections are used by the linker to organize our code/data/symbols etc. Lets usage an analogy to understand the importance of their existence.\nConsider an ELF as a room, containing so many different types of things. There are clothes, pens and paper, bottle, bag, notebooks etc. And everything is scattered.\nTo make sense of them, you decided to clear the floor and put everything categorized on the floor.\nYou put papers together. You put notebooks together. You put pens together. You put jeans together. You put shirts together. You put books together. This resembles sections.\nNow we have so many things, a little bit organized. But, further organization can be made.\nBooks, papers and pens are related things. They are all stationary. So, they can be put together in the bookshelf. Jeans, shirts, t-shirts, lower, jacket etc…. all of them are clothes. So, they can be put together in the wardrobe. And so on…. This further categorization is what segments are.\nWhen you have to look for fiction books, or course books, you don’t go to different place. They are within the same bookshelf. When you want a lower or a jacket, you don’t go to separate places again. They are within the same wardrobe. You want party clothes or comfy clothes, all of them are in the same wardrobe. The idea behind segments is that you group sections logically to the point that no further categorization can be made and all the related things can be accessed at one place in a logical manner. Ultimately, do you end up accessing the clothes individually or through the wardrobe? The answer is through the wardrobe.\nSections are just kind of intermediaries. They are used initially to do things but they are not the ones that are ultimately used in the end. Segments, on the other hand, evolve from sections and these are what that get used at runtime. Once these segments are created, program headers come into existence."},"title":"part-ii-analyzing-section-headers"},"/gitbook/docs/understanding-hello-world/linked-elf-analysis/preparation-for-symbol-resolution/":{"data":{"premise#Premise":"We are using printf() to print Hello, World!\\n in the output screen. But where is printf()?\nI have not written it. So where is it coming from? glibc? Yes.\nWhat is glibc? A shared library? Yes.\nGreat. printf() is coming from glibc. But our source code and the glibc are two distinct things. How will my source code know where is glibc and where is printf() in it?\nWe also know that our source code is just a tiny part of the infrastructure that runs it. I didn’t write that infrastructure. That infra would also require various functions and other things. Where are those things coming from?\nThe answer is symbol resolution. And we are going to discuss the same in this article.","preparation-for-symbol-resolution#Preparation For Symbol Resolution":"Preparation For Symbol Resolution","relocation-entries#Relocation Entries":"Relocations are instructions for the linker/loader program (ld-linux.so).\nIn simple words, a relocation entry asks to replace the mentioned placeholder offset with the real address or offset for this symbol. Primarily, there are two kinds of relocation entries.\nRelocation with addend, RELA. Relocation without addend, REL. An addend is a constant value added to the symbol’s address during relocation.\nWhen this constant is stored in the relocation entry itself, we call it RELA, which means, “Relocation with Addend”. When this constant is embedded in the section being relocated, we call it REL, which means, “Relocation without Addend”. There are two relocation tables in our binary, .rela.dyn and .rela.plt.\n.rela.dyn is for general data/function pointer relocations. .rela.plt is for function calls through the PLT, typically used for lazy binding. These are the relocation entries in our binary.\nRelocation section '.rela.dyn' at offset 0x550 contains 8 entries: Offset Info Type Sym. Value Sym. Name + Addend 000000003dd0 000000000008 R_X86_64_RELATIVE 1130 000000003dd8 000000000008 R_X86_64_RELATIVE 10f0 000000004010 000000000008 R_X86_64_RELATIVE 4010 000000003fc0 000100000006 R_X86_64_GLOB_DAT 0000000000000000 __libc_start_main@GLIBC_2.34 + 0 000000003fc8 000200000006 R_X86_64_GLOB_DAT 0000000000000000 _ITM_deregisterTM[...] + 0 000000003fd0 000400000006 R_X86_64_GLOB_DAT 0000000000000000 __gmon_start__ + 0 000000003fd8 000500000006 R_X86_64_GLOB_DAT 0000000000000000 _ITM_registerTMCl[...] + 0 000000003fe0 000600000006 R_X86_64_GLOB_DAT 0000000000000000 __cxa_finalize@GLIBC_2.2.5 + 0 Relocation section '.rela.plt' at offset 0x610 contains 1 entry: Offset Info Type Sym. Value Sym. Name + Addend 000000004000 000300000007 R_X86_64_JUMP_SLO 0000000000000000 puts@GLIBC_2.2.5 + 0","setting-up-the-grounds#Setting Up The Grounds":"From assembly, we know that a lot of things are just symbols of different kinds.\nThe instruction call puts@PLT is a call to a function symbol puts via the procedure linkage table or PLT.\nOften, these symbols are located in code written beyond the current file or something entirely written by a different person. To use these symbols, there has to be a way through which these are made available to our binary and our binary knows where they are.\nSymbols are the entities which are required to be resolved. Relocation is the process that resolves the final runtime address of these symbols.","string-table#String Table":"String table is the general table which serves as the central table for symbol names, just like we talked about the section header string table.\nTo access it, run\n$ readelf ./linked_elf -p .strtab String dump of section '.strtab': [ 1] Scrt1.o [ 9] __abi_tag [ 13] crtstuff.c [ 1e] deregister_tm_clones [ 33] __do_global_dtors_aux [ 49] completed.0 [ 55] __do_global_dtors_aux_fini_array_entry [ 7c] frame_dummy [ 88] __frame_dummy_init_array_entry [ a7] hello.c [ af] __FRAME_END__ [ bd] _DYNAMIC [ c6] __GNU_EH_FRAME_HDR [ d9] _GLOBAL_OFFSET_TABLE_ [ ef] __libc_start_main@GLIBC_2.34 [ 10c] _ITM_deregisterTMCloneTable [ 128] puts@GLIBC_2.2.5 [ 139] _edata [ 140] _fini [ 146] __data_start [ 153] __gmon_start__ [ 162] __dso_handle [ 16f] _IO_stdin_used [ 17e] _end [ 183] __bss_start [ 18f] main [ 194] __TMC_END__ [ 1a0] _ITM_registerTMCloneTable [ 1ba] __cxa_finalize@GLIBC_2.2.5 [ 1d5] _init","symbol-tables#Symbol Tables":"A symbol table is a metadata table about symbols. That’s it.\nThere are two symbol tables in our binary. These are .symtab and .dynsym.\nSymbol table '.dynsym' contains 7 entries: Num: Value Size Type Bind Visibility Ndx Name 0: 0000000000000000 0 NOTYPE LOCAL DEFAULT UND 1: 0000000000000000 0 FUNC GLOBAL DEFAULT UND _[...]@GLIBC_2.34 (2) 2: 0000000000000000 0 NOTYPE WEAK DEFAULT UND _ITM_deregisterT[...] 3: 0000000000000000 0 FUNC GLOBAL DEFAULT UND puts@GLIBC_2.2.5 (3) 4: 0000000000000000 0 NOTYPE WEAK DEFAULT UND __gmon_start__ 5: 0000000000000000 0 NOTYPE WEAK DEFAULT UND _ITM_registerTMC[...] 6: 0000000000000000 0 FUNC WEAK DEFAULT UND [...]@GLIBC_2.2.5 (3) Symbol table '.symtab' contains 36 entries: Num: Value Size Type Bind Vis Ndx Name 0: 0000000000000000 0 NOTYPE LOCAL DEFAULT UND 1: 0000000000000000 0 FILE LOCAL DEFAULT ABS Scrt1.o 2: 00000000000020ec 32 OBJECT LOCAL DEFAULT 19 __abi_tag 3: 0000000000000000 0 FILE LOCAL DEFAULT ABS crtstuff.c 4: 0000000000001080 0 FUNC LOCAL DEFAULT 14 deregister_tm_clones 5: 00000000000010b0 0 FUNC LOCAL DEFAULT 14 register_tm_clones 6: 00000000000010f0 0 FUNC LOCAL DEFAULT 14 __do_global_dtors_aux 7: 0000000000004018 1 OBJECT LOCAL DEFAULT 26 completed.0 8: 0000000000003dd8 0 OBJECT LOCAL DEFAULT 21 __do_global_dtor[...] 9: 0000000000001130 0 FUNC LOCAL DEFAULT 14 frame_dummy 10: 0000000000003dd0 0 OBJECT LOCAL DEFAULT 20 __frame_dummy_in[...] 11: 0000000000000000 0 FILE LOCAL DEFAULT ABS hello.c 12: 0000000000000000 0 FILE LOCAL DEFAULT ABS crtstuff.c 13: 00000000000020e8 0 OBJECT LOCAL DEFAULT 18 __FRAME_END__ 14: 0000000000000000 0 FILE LOCAL DEFAULT ABS 15: 0000000000003de0 0 OBJECT LOCAL DEFAULT 22 _DYNAMIC 16: 0000000000002014 0 NOTYPE LOCAL DEFAULT 17 __GNU_EH_FRAME_HDR 17: 0000000000003fe8 0 OBJECT LOCAL DEFAULT 24 _GLOBAL_OFFSET_TABLE_ 18: 0000000000000000 0 FUNC GLOBAL DEFAULT UND __libc_start_mai[...] 19: 0000000000000000 0 NOTYPE WEAK DEFAULT UND _ITM_deregisterT[...] 20: 0000000000004008 0 NOTYPE WEAK DEFAULT 25 data_start 21: 0000000000000000 0 FUNC GLOBAL DEFAULT UND puts@GLIBC_2.2.5 22: 0000000000004018 0 NOTYPE GLOBAL DEFAULT 25 _edata 23: 0000000000001154 0 FUNC GLOBAL HIDDEN 15 _fini 24: 0000000000004008 0 NOTYPE GLOBAL DEFAULT 25 __data_start 25: 0000000000000000 0 NOTYPE WEAK DEFAULT UND __gmon_start__ 26: 0000000000004010 0 OBJECT GLOBAL HIDDEN 25 __dso_handle 27: 0000000000002000 4 OBJECT GLOBAL DEFAULT 16 _IO_stdin_used 28: 0000000000004020 0 NOTYPE GLOBAL DEFAULT 26 _end 29: 0000000000001050 34 FUNC GLOBAL DEFAULT 14 _start 30: 0000000000004018 0 NOTYPE GLOBAL DEFAULT 26 __bss_start 31: 0000000000001139 26 FUNC GLOBAL DEFAULT 14 main 32: 0000000000004018 0 OBJECT GLOBAL HIDDEN 25 __TMC_END__ 33: 0000000000000000 0 NOTYPE WEAK DEFAULT UND _ITM_registerTMC[...] 34: 0000000000000000 0 FUNC WEAK DEFAULT UND __cxa_finalize@G[...] 35: 0000000000001000 0 FUNC GLOBAL HIDDEN 11 _init","understanding-the-attributes#Understanding The Attributes":"","understanding-the-attributes-1#Understanding The Attributes":"Addend is probably the only foreign term here. Next we are going to understand that. But before that we need to clear a small concept.","what-are-these-two-tables-used-for#What are these two tables used for?":"","what-is-required-for-relocation#What is required for relocation?":"What are all the symbols that require relocation? These are those symbols whose runtime address is not known. Relocation entries which define the metadata about these symbols. A process to manage symbol resolution."},"title":"preparation-for-symbol-resolution"},"/gitbook/docs/understanding-hello-world/linked-elf-analysis/procedure-linkage-table/":{"data":{"final-structure-of-the-global-offset-table#Final Structure Of The Global Offset Table":"+--------------------------------------------------------------------------------------+ | // Eager Binding Division -\u003e .got at offset 0x0000 (RELA) (.rela.dyn) | | +--------+ +------------------------+ +--------+ +------------------------+ | | | GOT[0] | -\u003e | *(func/symb 1) | -\u003e | 0x0000 | -\u003e | Actual Runtime Address | | | +--------+ +------------------------+ +--------+ +------------------------+ | | | GOT[1] | -\u003e | *(func/symb 2) | -\u003e | 0x0008 | -\u003e | Actual Runtime Address | | | +--------+ +------------------------+ +--------+ +------------------------+ | | | GOT[2] | -\u003e | *(func/symb 3) | -\u003e | 0x0010 | -\u003e | Actual Runtime Address | | | +--------+ +------------------------+ +--------+ +------------------------+ | | | GOT[3] | -\u003e | *(func/symb 4) | -\u003e | 0x0018 | -\u003e | Actual Runtime Address | | | +--------+ +------------------------+ +--------+ +------------------------+ | | | GOT[4] | -\u003e | *(func/symb 5) | -\u003e | 0x0020 | -\u003e | Actual Runtime Address | | | +--------+ +------------------------+ +--------+ +------------------------+ | | .... | | .... | | +--------+ +------------------------+ +--------+ +------------------------+ | | | GOT[N] | -\u003e | *(func/symb N) | -\u003e | 0x.... | -\u003e | Actual Runtime Address | | | +--------+ +------------------------+ +--------+ +------------------------+ | |--------------------------------------------------------------------------------------| | // Lazy Binding Division -\u003e .got.plt at offset 0x0028 (JMPREL) (.rela.plt) | | +--------+ +------------------------+ +--------+ +------------------------+ | | | GOT[0] | -\u003e | *(.dynamic) | -\u003e | 0x0028 | -\u003e | Actual Runtime Address | | \u003c- Reserved for enabling lazy binding | +--------+ +------------------------+ +--------+ +------------------------+ | | | GOT[1] | -\u003e | *(link_map) | -\u003e | 0x0030 | -\u003e | Actual Runtime Address | | \u003c- Reserved for enabling lazy binding | +--------+ +------------------------+ +--------+ +------------------------+ | | | GOT[2] | -\u003e | *(_dl_runtime_resolve) | -\u003e | 0x0038 | -\u003e | Actual Runtime Address | | \u003c- Reserved for enabling lazy binding | +--------+ +------------------------+ +--------+ +------------------------+ | | | GOT[3] | -\u003e | *(PLT stub for func 1) | -\u003e | 0x0040 | -\u003e | Actual Runtime Address | | | +--------+ +------------------------+ +--------+ +------------------------+ | | | GOT[4] | -\u003e | *(PLT stub for func 2) | -\u003e | 0x0048 | -\u003e | Actual Runtime Address | | | +--------+ +------------------------+ +--------+ +------------------------+ | | .... | | .... | | +--------+ +------------------------+ +--------+ +------------------------+ | | | GOT[M] | -\u003e | *(PLT stub for func M) | -\u003e | 0x.... | -\u003e | Actual Runtime Address | | | +--------+ +------------------------+ +--------+ +------------------------+ | +--------------------------------------------------------------------------------------+","first-of-all-what-is-stub#First of all, what is stub?":"I have understood stub as a piece of instructions, consistent across all entries.\n+------------------------------+ PLT[0] -\u003e | push *GOT[link_map] | ; jump to the entry in GOT which points to thedynamic linker | jmp RESOLVER_FUNC | ; usually _dl_runtime_resolve() +------------------------------+ +------------------------------+ PLT[1] -\u003e | jmp *GOT[FUNC1] | ; jump to resolved address of FUNC(1) | push FUNC1_INDEX | ; index for FUNC(1) in .rela.plt | jmp plt[0] | ; fallback to resolver +------------------------------+ +------------------------------+ PLT[2] -\u003e | jmp *GOT[FUNC2] | ; jump to resolved address of FUNC(2) | push FUNC2_INDEX | ; index for FUNC(2) in .rela.plt | jmp plt[0] | ; fallback to resolver +------------------------------+ ... +------------------------------+ PLT[N] -\u003e | jmp *GOT[FUNCN] | ; jump to resolved address of FUNC(N) | push FUNCN_INDEX | ; index for FUNC(N) in .rela.plt | jmp plt[0] | ; fallback to resolver +------------------------------+ PLT[0] is reserved for providing the base for relocation. PLT[1 to n] onward are the actual relocation entries requiring lazy binding.","introduction-to-procedure-linkage-table#Introduction To Procedure Linkage Table":"The Procedure Linkage Table is a section in an ELF binary used to support dynamic function calls to external (shared library) functions. It acts as an indirect jump table, allowing a program to call shared library functions whose actual memory addresses are not known until runtime, typically resolved through lazy binding.\nThe global offset table stores pointers to both function symbols and global variables, but procedure linkage table is used for external functions only.\nProcedure linkage table is tightly coupled with the global offset table.","possible-look-of-our-gotplt-and-plt-section#Possible Look Of Our `.got.plt` And `.plt` Section":"[ERASER.IO DRAWING]","problem-statement#Problem Statement":"Now we are inside the main symbol and our code i executing. The program reached the instruction where a call to puts is made. Just think about how it would get resolved?\nRight now, the control is in the hands of main. And puts is hiding somewhere in the shared libraries. It is essential that the interpreter program has the control, so that we can use the runtime resolver function to find puts. Right? Where we would get it? The global offset table is designed to contain offsets only. How will you encapsulate the logic for finding puts or any function that needs lazy binding?\nYou may say that the GOT entry can point to the interpreter program. In that case, how will you jump to the runtime resolver function? How will you tell the runtime resolver function which symbol is needed to be found? A single entry in the global offset table can’t afford to do all of this. So, whats the solution, then?","procedure-linkage-table#Procedure Linkage Table":"Procedure Linkage Table","structure-of-procedure-linkage-table#Structure of Procedure Linkage Table":"","understanding-puts-relocation#Understanding `puts` Relocation":"0000000000001139 : 1139:\t55 push rbp 113a:\t48 89 e5 mov rbp,rsp 113d:\t48 8d 05 c0 0e 00 00 lea rax,[rip+0xec0] # 2004 \u003c_IO_stdin_used+0x4\u003e 1144:\t48 89 c7 mov rdi,rax 1147:\te8 e4 fe ff ff call 1030 114c:\tb8 00 00 00 00 mov eax,0x0 1151:\t5d pop rbp 1152:\tc3 ret Disassembly of section .plt: 0000000000001020 : 1020:\tff 35 ca 2f 00 00 push QWORD PTR [rip+0x2fca] # 3ff0 \u003c_GLOBAL_OFFSET_TABLE_+0x8\u003e 1026:\tff 25 cc 2f 00 00 jmp QWORD PTR [rip+0x2fcc] # 3ff8 \u003c_GLOBAL_OFFSET_TABLE_+0x10\u003e 102c:\t0f 1f 40 00 nop DWORD PTR [rax+0x0] 0000000000001030 : 1030:\tff 25 ca 2f 00 00 jmp QWORD PTR [rip+0x2fca] # 4000 1036:\t68 00 00 00 00 push 0x0 103b:\te9 e0 ff ff ff jmp 1020 \u003c_init+0x20\u003e Disassembly of section .got: 0000000000003fc0 \u003c.got\u003e: ... Disassembly of section .got.plt: 0000000000003fe8 \u003c_GLOBAL_OFFSET_TABLE_\u003e: 3fe8:\te0 3d loopne 4027 \u003c_end+0x7\u003e ... 3ffe:\t00 00 add BYTE PTR [rax],al 4000:\t36 10 00 ss adc BYTE PTR [rax],al 4003:\t00 00 add BYTE PTR [rax],al 4005:\t00 00 add BYTE PTR [rax],al ... Relocation section '.rela.plt' at offset 0x610 contains 1 entry: Offset Info Type Sym. Value Sym. Name + Addend 000000004000 000300000007 R_X86_64_JUMP_SLO 0000000000000000 puts@GLIBC_2.2.5 + 0 PLT stub for puts() jmp *GOT[puts] push puts_idx jmp plt[0] When the main has control, and it is executing instructions one-by-one, it eventually encounters the call to puts via plt. This can be verified from the disassembly of main .\nSince main has the control, the control has to be passed to dynamic linker (interpreter) program so that the runtime resolver function (_dl_runtime_resolve()) can be called which would eventually find the address of puts.\nOffset 0x1030 resolves to the plt stub for puts function. The instruction at this offset jumps to an entry in the plt section of the global offset table, which is at the offset 0x4000. Although the disassembly at 0x4000 is garbage because it is filled at runtime, but we know that offset 0x4000 is where the relocation for puts has to be done. This can be confirmed from the relocation entry in the .rela.plt table. When the interpreter program was creating PLT stubs and corresponding entries in the global offset table, each .got.plt entry points to the PLT stub it corresponds to. This might be confusing so lets understand the flow here.\nWhen a call to puts is made via plt, it goes through the the PLT stub. The first instruction in the plt stub points to the global offset table, which is logical as if the address is resolved, no relocation should be repeated. It is done in accordance to calls after lazy binding is done. Its global offset table entry points to the next instruction in its PLT stub. This instruction pushes the symbol index for puts from the .rela.plt relocation table. We can notice that it is push 0x0, which is a placeholder value, which gets resolved at runtime. Now that the symbol index is pushed on the stack, the third and the final instruction in the plt stub executes, offset 0x103b, which jumps to the PLT[0] stub at offset 0x1020.\nThe instruction at offset 0x1020 is pushing something on the stack. It is an entry in the global offset table, which corresponds to the link_map . link_map is the resolver data, required by the runtime resolver function to resolve the symbol. The instruction at 0x1026 points to the runtime resolver function entry in the global offset table. The interpreter jumps on this and marks the start of actual find and patch process for puts.\nAfter this, the control is given to main again and a call to puts is successfully made.","why-it-is-called-procedure-linkage-table-is-there-any-meaning#Why it is called procedure linkage table? Is there any meaning?":"Just like the global offset table is a table where each entry is designed to be an offset to the actual runtime address of a symbol, the procedure linkage table is also meaningful in its name.\nIt is a table of stubs that handle runtime linkage of procedures (functions) in a dynamically linked environment."},"title":"procedure-linkage-table"},"/gitbook/docs/understanding-hello-world/linked-elf-analysis/relocation-part-1/":{"data":{"binding#Binding":"Binding refers to process of resolving the actual address of a symbol. Although binding can be of various types, we are concerned about two types for now.\nEager Binding: Here, the symbol is resolved immediately. Lazy Binding: Here, the symbol is resolved only when it is required for the first time. Every entry in the .rela.dyn table undergoes eager binding because it includes symbols which are primarily used in the startup code, the code which gets run before the main() of our source code.\nEvery entry in the .rela.plt table undergoes lazy binding because it includes symbols which are primarily used in the actual source code of the binary.\nBut why does lazy binding exist?\nLets have a look at the following code.\n#include #include int main(void){ int input; printf(\"Enter 0 to exit OR 1 to sleep: \"); scanf(\"%d\", \u0026input); if (input==0){ printf(\"Exiting.....\\n\"); } else{ sleep(10); printf(\"Sleeping for 10 seconds.....\\n\"); printf(\"Exiting.....\\n\"); } } Now, it entirely depends upon the user input whether sleep function has to be execute or not.\nIf you resolve it before it is used, it will create a very minute delay as the startup code will execute late. But it is unnoticeable. Big code bases have thousands of such conditions. I hope the delay would be significant there? Lets have a look at this another piece of code.\n#include #include #include int main(int argc, char *argv[]) { int input = atoi(argv[1]); if (input == 0) { return 0; } else { sleep(10); printf(\"Sleeping for 10 seconds.....\\n\"); printf(\"Exiting.....\\n\"); } return 0; } This one is slightly modified. It takes input as an argument.\nHere, even printf()’s usage is dependent on the argument completely. What if the user input 0? printf() would never run, right? Then what is the point of resolving its address? Lazy binding is a solution to this problem.\nNote: The address is resolved when the symbol is referenced for the first time. After that, no more resolution is required.\nThere also exist a very genuine question that big code bases have too many functions, and if the address for every symbol in the source code is resolved like this, isn’t this going to create a runtime overhead?\nAbsolutely right. I mentioned earlier that there exist multiple kinds of binding, but we are concerned about two for now. Lazy binding is a solution, not the only solution. And as we will advance we will learn about them later as well. But keep this in mind that this binary only requires the knowledge of these two. So, we are not going to touch them soon. I guess we have find the answer to our question. The interpreter would go to RELA entries first because they require eager binding. So, what we are waiting for?","introducing-relocations#Introducing Relocations":"The interpreter uses the RELA entry to jump to the .rela.dyn relocation table. RELA entry in the dynamic section has a value of 0x550 and we can verify that the .rela.dyn table is also located at the same offset by this line Relocation section '.rela.dyn' at offset 0x550 contains 8 entries: .\nTo revise, a relocation entry can be read as: at offset in the section, replace the placeholder address of the symbol with its actual address.\nThese are the relocation entries in the .rela.dyn table.\nRelocation section '.rela.dyn' at offset 0x550 contains 8 entries: Offset Info Type Sym. Value Sym. Name + Addend 000000003dd0 000000000008 R_X86_64_RELATIVE 1130 000000003dd8 000000000008 R_X86_64_RELATIVE 10f0 000000004010 000000000008 R_X86_64_RELATIVE 4010 000000003fc0 000100000006 R_X86_64_GLOB_DAT 0000000000000000 __libc_start_main@GLIBC_2.34 + 0 000000003fc8 000200000006 R_X86_64_GLOB_DAT 0000000000000000 _ITM_deregisterTM[...] + 0 000000003fd0 000400000006 R_X86_64_GLOB_DAT 0000000000000000 __gmon_start__ + 0 000000003fd8 000500000006 R_X86_64_GLOB_DAT 0000000000000000 _ITM_registerTMCl[...] + 0 000000003fe0 000600000006 R_X86_64_GLOB_DAT 0000000000000000 __cxa_finalize@GLIBC_2.2.5 + 0 The Info field is 8 bytes long, although here it is 6 bytes, which is I am also wondering why readelf is not showing the remaining 2 bytes, but leave it.\nThe upper 8-bytes refers to the symbol index value and the lower 8-bytes refers to the relocation type.","r_x86_64_glob_dat-relocation#`R_X86_64_GLOB_DAT` Relocation":"Offset Info Type Sym. Value Sym. Name + Addend 000000003fc0 000100000006 R_X86_64_GLOB_DAT 0000000000000000 __libc_start_main@GLIBC_2.34 + 0 000000003fc8 000200000006 R_X86_64_GLOB_DAT 0000000000000000 _ITM_deregisterTM[...] + 0 000000003fd0 000400000006 R_X86_64_GLOB_DAT 0000000000000000 __gmon_start__ + 0 000000003fd8 000500000006 R_X86_64_GLOB_DAT 0000000000000000 _ITM_registerTMCl[...] + 0 000000003fe0 000600000006 R_X86_64_GLOB_DAT 0000000000000000 __cxa_finalize@GLIBC_2.2.5 + 0 Lets take the first entry here.\nThe symbol index is 1 and relocation type is 6.\nR_X86_64_GLOB_DAT does require symbol lookup. This is a global data relocation, commonly used for symbol pointers in the GOT (Global Offset Table).\nIts purpose is to fill a pointer with the runtime address of a symbol — typically function pointers or global variables imported from shared libraries. The symbol is __libc_start_main. The relocation logic is *(base_addr + 0x3fc0) = address_of(__libc_start_main) The symbol is looked up in all the loaded shared libraries using the dynamic symbol table and the symbol hash tables. Once the runtime address is found in memory, the address is written in place of 0x3fc0 in the global offset table. And the relocation is done.\nThe term global offset table is new here.\nWe are done with .rela.dyn relocations.\nNow the interpreter jumps to the JUMPREL entry in the dynamic section and finds .rela.plt. The real chaos starts here.\nPLT entries are about lazy binding, by default. For lazy binding, we need to understand global offset table (GOT) and procedure linkage table (PLT). Both of which are really complex and confusing. Since it is fairly long, it deserves its own separate place. Therefore, we are dividing this article into two parts. Here ends the first part.","r_x86_64_relative-relocation#`R_X86_64_RELATIVE` Relocation":"Offset Info Type Sym. Value Sym. Name + Addend 000000003dd0 000000000008 R_X86_64_RELATIVE 1130 Its symbol index is 0 and the relocation type is 8, which resolves to R_X86_64_RELATIVE.\nEntries of type 8 doesn’t require any symbol lookup.\nOffset is where we have to write the result. And the result is calculated as follows: *(Offset) = Base Address of the binary + Addend In simple words, take the base address of the binary and add the value in the Sym. Name + Addend field to it. Now write the obtained value at the mentioned offset. Relocation is done.\nrelocation_address = base_addr + 0x3dd0; value_to_write = base_addr + 1130; *(relocation_address) = value_to_write The remaining two entries are relocated in the same manner.\n000000003dd8 000000000008 R_X86_64_RELATIVE 10f0 000000004010 000000000008 R_X86_64_RELATIVE 4010","relocations---part-1#Relocations - Part 1":"Relocations - Part 1","setup#Setup":"You remember the dynamic section?\nDynamic section at offset 0x2de0 contains 26 entries: Tag Type Name/Value 0x0000000000000001 (NEEDED) Shared library: [libc.so.6] 0x000000000000000c (INIT) 0x1000 0x000000000000000d (FINI) 0x1154 0x0000000000000019 (INIT_ARRAY) 0x3dd0 0x000000000000001b (INIT_ARRAYSZ) 8 (bytes) 0x000000000000001a (FINI_ARRAY) 0x3dd8 0x000000000000001c (FINI_ARRAYSZ) 8 (bytes) 0x000000006ffffef5 (GNU_HASH) 0x3b0 0x0000000000000005 (STRTAB) 0x480 0x0000000000000006 (SYMTAB) 0x3d8 0x000000000000000a (STRSZ) 141 (bytes) 0x000000000000000b (SYMENT) 24 (bytes) 0x0000000000000015 (DEBUG) 0x0 0x0000000000000003 (PLTGOT) 0x3fe8 0x0000000000000002 (PLTRELSZ) 24 (bytes) 0x0000000000000014 (PLTREL) RELA 0x0000000000000017 (JMPREL) 0x610 0x0000000000000007 (RELA) 0x550 0x0000000000000008 (RELASZ) 192 (bytes) 0x0000000000000009 (RELAENT) 24 (bytes) 0x000000006ffffffb (FLAGS_1) Flags: PIE 0x000000006ffffffe (VERNEED) 0x520 0x000000006fffffff (VERNEEDNUM) 1 0x000000006ffffff0 (VERSYM) 0x50e 0x000000006ffffff9 (RELACOUNT) 3 0x0000000000000000 (NULL) 0x0 We can classify the entries in the Type field based on when the interpreter goes to them.\nSo far, we know that the first thing that the interpreter does is to load the shared libraries.\nIn our case, it is libc.so.6. After all the shared libraries are loaded, the interpreter goes about relocation.\nWe know that we have two relocation tables in our binary. Also, there are two entries in the dynamic section regarding relocation, these include JMPREL and RELA. The question is, which one the interpreter is going to go at first?\nTo find the answer to this question, we have to understand a simple concept, called binding."},"title":"relocation-part-1"},"/gitbook/docs/understanding-hello-world/linked-elf-analysis/step-1-analyzing-elf-headers/":{"data":{"step-1-analyzing-elf-headers#Step 1: Analyzing ELF Headers":"Step 1: Analyzing ELF Headers","understanding-elf-headers#Understanding ELF Headers":"These are the ELF headers from the object code.\n$ readelf ./object_code.o -h ELF Header: Magic: 7f 45 4c 46 02 01 01 00 00 00 00 00 00 00 00 00 Class: ELF64 Data: 2's complement, little endian Version: 1 (current) OS/ABI: UNIX - System V ABI Version: 0 Type: REL (Relocatable file) Machine: Advanced Micro Devices X86-64 Version: 0x1 Entry point address: 0x0 Start of program headers: 0 (bytes into file) Start of section headers: 536 (bytes into file) Flags: 0x0 Size of this header: 64 (bytes) Size of program headers: 0 (bytes) Number of program headers: 0 Size of section headers: 64 (bytes) Number of section headers: 13 Section header string table index: 12 These are the ELF headers from the linked elf.\n$ readelf ./linked_elf -h ELF Header: Magic: 7f 45 4c 46 02 01 01 00 00 00 00 00 00 00 00 00 Class: ELF64 Data: 2's complement, little endian Version: 1 (current) OS/ABI: UNIX - System V ABI Version: 0 Type: DYN (Position-Independent Executable file) Machine: Advanced Micro Devices X86-64 Version: 0x1 Entry point address: 0x1050 Start of program headers: 64 (bytes into file) Start of section headers: 13968 (bytes into file) Flags: 0x0 Size of this header: 64 (bytes) Size of program headers: 56 (bytes) Number of program headers: 14 Size of section headers: 64 (bytes) Number of section headers: 31 Section header string table index: 30 Lets examine the differences.\nThe first difference is obviously in Type. And we have already spotted this difference in the output of file just before.\nIt has changed from REL to DYN. This means that this ELF is dynamically linked and is ready to be executed. We know that object code can’t be loaded in memory, which is why it is not an executable.\nHere, the value in the Entry point address field has changed from 0x0 to 0x1050. It is because an object code which has undergo linking has everything necessary to be loaded into memory, which we will talk about very soon. 0x1050 is the starting virtual address at which execution begins when the OS transfers the control to the ELF. As we have discussed earlier, program headers are used by the dynamic linker/loader program at runtime to resolve cross-references. And it is build time linker program that actually creates those headers in the final executable ELF.\nWe can notice that Start of program headers entry has a value of 64 (bytes into file). We know that the ELF header is of the size 64-bytes on 64-bit architecture. So, 0-63 bytes in the ELF are occupied by ELF headers. At the 64th byte, program headers start. Number of program headers entry is also populated. There are 14 program headers. They are found together in the program headers table, which we are going to refer to as PHT. Size of program headers entry is also populated. It is 56 bytes. What’s interesting to see here is section headers.\nEarlier section headers started from 536 bytes into the ELF. This time, it is 13968 bytes into the file. Maybe it is a part of restructuring as a lot of things have been added. Wait, the number of section headers has moved from 13 to 31. And the size is still the same, 64 bytes. How’s that possible? The answer is, compilation (.asm to .o) lay downs only a partial structure. Linking completes the object code. These extra section headers are about the things which were absent earlier. If you notice, .note.GNU-stack is not present in the linked ELF. This is because it is empty in size and that means “non-executable stack”. It is not needed in the final binary so it got stripped. Section header string table index: 30 we need not to talk about. We have invested great amount of time understanding this.\nThe entry at index 30 in the section headers table is where the string table for section headers is located.","what-is-the-next-step#What is the next step?":"Now the kernel goes to the program headers, which are present just after the ELF headers at offset 0x64 in the binary.\nWhat are we waiting for? Lets go there. But before that, we need to clear our mind about something.","what-to-do-about-this#What to do about this?":"When this linked elf (or binary) is executed in the terminal, the first thing that happens is the kernel checks if the binary can be loaded into the memory.\nIf the binary can’t be loaded into memory, it doesn’t poses relevant information required to setup the environment in which it can be executed.\nTo find this, the kernel checks up the ELF headers. The Type header we have talked about recently is the one that is checked. If the value is REL, it can’t be loaded.\nSince our elf is linked now, it has the value of DYN in the Type field, which means that it has necessary information required to setup the environment for execution.\nTherefore, the first task is done."},"title":"step-1-analyzing-elf-headers"},"/gitbook/docs/understanding-hello-world/linked-elf-analysis/step-2-analyzing-program-headers-table/":{"data":{"analyzing-program-headers-table#Analyzing Program Headers Table":"Analyzing Program Headers TableProgram headers are a set of structures in an ELF file that describe how to create a process image in memory. These are used by the runtime dynamic linker/loader program (or, the interpreter program).","how-program-headers-are-read#How Program Headers Are Read?":"So far, the kernel has verified that the ELF has necessary information required to be executed.\nNow the kernel is at the program headers table.","introducing-the-interpreter#Introducing The Interpreter":"First of all, the interpreter program and the dynamic linker/loader are the same thing. And we use it interchangeably. So, don’t get confused.\nThe name of this program is ld-linux.\nAnyways, the interpreter looks for the dynamic section within the program headers table of our binary. Since the binary is already mapped into the memory, all the access is via the virtual address space. So, the interpreter goes to 0x3de0 address in the virtual address space of our binary to find the dynamic section.\nHere comes the crazy part. Now sit tight and read carefully, otherwise you might end up glossing over something and it will not settle properly.\nOur binary and the interpreter program, both share the same virtual address space. Although the interpreter program is loaded as a separate ELF and it undergoes everything separately, it is not a separate process. Although the execve syscall starts by loading our binary, but when the kernel finds out that it requires an interpreter, it shifts to loading the interpreter instead. And by that time, the loadable segments are already mapped out in the virtual address space. So, everything that is required is practically loaded already. And the interpreter just have to act on it.","is-any-dynamic-interpreter-required#\u0026ldquo;Is any dynamic interpreter required?\u0026rdquo;":"After loading all the LOAD segments, the kernel looks if there is a requirement for a dynamic linker program. Since our binary is linked dynamically, it contains an INTERP entry.\nThis INTERP segment points to the path of the interpreter, which is stored as a string in the .interp section of the ELF file. The kernel reads the interpreter path from there, locates the dynamic linker program on the disk and loads it as a separate ELF, repeating the entire ELF loading process for it.\nAfter the interpreter (dynamic linker) is loaded, the kernel sets up the process stack and transfers control to the dynamic linker’s entry point, not the main program.\nThe process stack setup includes argv, envp, and auxv (auxiliary vectors with ELF info). We need not to worry about it. Now the kernel gives control to the entry point of the dynamic linker (or the interpreter) program.","setup#Setup":"Program Headers: Type Offset VirtAddr PhysAddr FileSiz MemSiz Flags Align PHDR 0x0000000000000040 0x0000000000000040 0x0000000000000040 0x0000000000000310 0x0000000000000310 R 0x8 INTERP 0x0000000000000394 0x0000000000000394 0x0000000000000394 0x000000000000001c 0x000000000000001c R 0x1 └─ [Requesting program interpreter: /lib64/ld-linux-x86-64.so.2] LOAD 0x0000000000000000 0x0000000000000000 0x0000000000000000 0x0000000000000628 0x0000000000000628 R 0x1000 LOAD 0x0000000000001000 0x0000000000001000 0x0000000000001000 0x000000000000015d 0x000000000000015d R E 0x1000 LOAD 0x0000000000002000 0x0000000000002000 0x0000000000002000 0x000000000000010c 0x000000000000010c R 0x1000 LOAD 0x0000000000002dd0 0x0000000000003dd0 0x0000000000003dd0 0x0000000000000248 0x0000000000000250 RW 0x1000 DYNAMIC 0x0000000000002de0 0x0000000000003de0 0x0000000000003de0 0x00000000000001e0 0x00000000000001e0 RW 0x8 NOTE 0x0000000000000350 0x0000000000000350 0x0000000000000350 0x0000000000000020 0x0000000000000020 R 0x8 NOTE 0x0000000000000370 0x0000000000000370 0x0000000000000370 0x0000000000000024 0x0000000000000024 R 0x4 NOTE 0x00000000000020ec 0x00000000000020ec 0x00000000000020ec 0x0000000000000020 0x0000000000000020 R 0x4 GNU_PROPERTY 0x0000000000000350 0x0000000000000350 0x0000000000000350 0x0000000000000020 0x0000000000000020 R 0x8 GNU_EH_FRAME 0x0000000000002014 0x0000000000002014 0x0000000000002014 0x000000000000002c 0x000000000000002c R 0x4 GNU_STACK 0x0000000000000000 0x0000000000000000 0x0000000000000000 0x0000000000000000 0x0000000000000000 RW 0x10 GNU_RELRO 0x0000000000002dd0 0x0000000000003dd0 0x0000000000003dd0 0x0000000000000230 0x0000000000000230 R 0x1 Section to Segment mapping: Segment Sections... 00 01 .interp 02 .note.gnu.property .note.gnu.build-id .interp .gnu.hash .dynsym .dynstr .gnu.version .gnu.version_r .rela.dyn .rela.plt 03 .init .plt .plt.got .text .fini 04 .rodata .eh_frame_hdr .eh_frame .note.ABI-tag 05 .init_array .fini_array .dynamic .got .got.plt .data .bss 06 .dynamic 07 .note.gnu.property 08 .note.gnu.build-id 09 .note.ABI-tag 10 .note.gnu.property 11 .eh_frame_hdr 12 13 .init_array .fini_array .dynamic .got","setup-virtual-address-space-process-image#Setup Virtual Address Space (Process Image)":"It is looking for all the entries of type LOAD, so that it can map those segments into the virtual address space.\nEach entry in the program headers table map exactly in the same order with the entries inside the section to segment mapping.\nThe LOAD segments are at indices 2, 3, 4 and 5. The segments corresponding to them are these:\n02 .note.gnu.property .note.gnu.build-id .interp .gnu.hash .dynsym .dynstr .gnu.version .gnu.version_r .rela.dyn .rela.plt 03 .init .plt .plt.got .text .fini 04 .rodata .eh_frame_hdr .eh_frame .note.ABI-tag 05 .init_array .fini_array .dynamic .got .got.plt .data .bss Is there any logic behind such segmentation of sections? Yes. If we have a look at the section headers table, we can find that:\nsection 1-10 have the same flags, A except the 10th one, which had I as well. And they are made in one segment. These include read-only metadata. section 11-15 are all AX, which are allocated and executable, section 16-19 are all with A flag. These include read-only data, and section 20-26 are with WX flags. These include writable and allocated data. Every segment is carefully containing sections with same permissions. Also, each segment is page-aligned (typically 4 KB) to simplify mapping and protect boundaries.","understanding-the-attributes#Understanding The Attributes":"Each program header maps to a segment. Type refers to the type of that segment.\nPHDR: Program header table LOAD: Loadable segment DYNAMIC: Dynamic linking info, where is the dynamic section in the binary INTERP: Path to dynamic linker NOTE: Auxiliary information (e.g., build ID, security notes) GNU_STACK: Stack permissions GNU_RELRO: Read-only after relocation GNU_EH_FRAME: Exception handling info Offset is the location of this segment within the ELF binary.\nVirtualAddr is the virtual memory address where the segment should be mapped in memory during execution.\nPhysAddr is usually ignored by modern OS. Same as VirtAddr or meaningless.\nFileSize is the size of the segment in the binary.\nMemSize is the size of the segment in memory after loading.\nFlags refer to memory access permissions.\nR: Readable W: Writable E: Executable Align refers to the required alignment for the segment in memory and in the file.","what-does-the-interpreter-program-do#What does the interpreter program do?":"Now the interpreter went on the 0x3de0 location within the virtual address space and found the dynamic section there."},"title":"step-2-analyzing-program-headers-table"},"/gitbook/docs/understanding-hello-world/linked-elf-analysis/step-3-interpreter-in-action/":{"data":{"how-does-the-interpreter-reads-them#How does the interpreter reads them?":"First, it walks through each entry until it hits the NULL termination entry. And stores pointers to key entries for later use.\nSecond, all the entries of type NEEDED refers to the shared libraries, so it reads the name of the library from .dynstr table and loads the shared library. And it handles transitive dependencies recursively..\nThird, it maps relocation-related sections into memory for later processing during symbol resolution and relocation.\nFourth, cross-references are relocated.\nFifth, it sets up the procedure linkage table for lazy binding.\nSixth, it uses DT_VERNEED, DT_VERNEEDNUM, DT_VERSYM to check symbol versions match between binary and shared libraries.\nSeventh, constructors are called, INIT and INIT_ARRAY .\nFinally, our source gets the control.\nThis was an high level overview of how the interpreter works. Now, its time to learn relocations. And don’t worry. All the things that were left unresolved here will start to get resolved from relocations on wards.\nTake rest.","setup#Setup":"This is the .dynamic section.\nDynamic section at offset 0x2de0 contains 26 entries: Tag Type Name/Value 0x0000000000000001 (NEEDED) Shared library: [libc.so.6] 0x000000000000000c (INIT) 0x1000 0x000000000000000d (FINI) 0x1154 0x0000000000000019 (INIT_ARRAY) 0x3dd0 0x000000000000001b (INIT_ARRAYSZ) 8 (bytes) 0x000000000000001a (FINI_ARRAY) 0x3dd8 0x000000000000001c (FINI_ARRAYSZ) 8 (bytes) 0x000000006ffffef5 (GNU_HASH) 0x3b0 0x0000000000000005 (STRTAB) 0x480 0x0000000000000006 (SYMTAB) 0x3d8 0x000000000000000a (STRSZ) 141 (bytes) 0x000000000000000b (SYMENT) 24 (bytes) 0x0000000000000015 (DEBUG) 0x0 0x0000000000000003 (PLTGOT) 0x3fe8 0x0000000000000002 (PLTRELSZ) 24 (bytes) 0x0000000000000014 (PLTREL) RELA 0x0000000000000017 (JMPREL) 0x610 0x0000000000000007 (RELA) 0x550 0x0000000000000008 (RELASZ) 192 (bytes) 0x0000000000000009 (RELAENT) 24 (bytes) 0x000000006ffffffb (FLAGS_1) Flags: PIE 0x000000006ffffffe (VERNEED) 0x520 0x000000006fffffff (VERNEEDNUM) 1 0x000000006ffffff0 (VERSYM) 0x50e 0x000000006ffffff9 (RELACOUNT) 3 0x0000000000000000 (NULL) 0x0","step-3-interpreter-in-action#Step 3: Interpreter In Action":"Step 3: Interpreter In Action","understanding-each-entry-type#Understanding Each Entry Type":"Lots of information we have no idea about. Don’t stress about it. We will go through everything. Nothing would be left.","understanding-the-attributes#Understanding the attributes":"The Tag field identifies the purpose of each entry. It’s an enum-like value defined in the ELF specification, used by the dynamic linker to understand what kind of information the entry holds.\nThe Type field again is an interpretation of the underlying unique numeric identifier which has a special meaning defined in the ELF specification. All in all, it identifies the type of entry.\nThe Name/Value field is the most complicated one here.\nWhen it comes to a shared library, this attribute holds a string name. When it comes to anything related to size, it stores a size. When it comes to offset, it stores 0x prefixed addresses. It only stores offset or size. It is the elf-parser that resolves it to string names, if possible."},"title":"step-3-interpreter-in-action"},"/gitbook/docs/understanding-hello-world/linked-elf-analysis/what-are-addends/":{"data":{"example-c-program#Example C Program":"This program creates an integer array of size 5. Then it uses a for-loop to iterate over it. While iterating over each element, it uses pointer dereferencing directly rather than subscripting the array variable using square brackets, like this array_ptr[i] .\n#include int main(void){ int array_ptr[] = {0, 4, 7, 6, -2}; for (int i = 0; i \u003c 5; i++) { printf(\"%d, \", *(array_ptr + i)); } }","how-does-that-relate-with-symbol-resolution#How does that relate with symbol resolution?":"We are going to find this in the next article when we explore how relocations actually happen.\nTake rest.","problem-statement#Problem Statement":"We need to focus on the *(array_ptr + i) part.\nTo access the 4th element (index = 3), either we do array_ptr[3] or *(array_ptr + 3). In both the cases, it resolves to the latter one, we know that.\nThe thing is, this is not a direct access to the 4th element? Think about it.\nWe are using the base address at which the array is loaded in the memory and from there we are counting 4 boxes. Right? On 64-bit systems, an integer is made up of 4-bytes.\nBut memory doesn’t exist in chunks of 4. It is 1-1-1-1. But we’re accessing it in the form of 4. The 4 here is a beautiful abstraction. And beneath that lies the mess. We are adding [0, 1, 2, 3, 4] to the base pointer to obtain the address of the next consecutive elements in the array, but effectively we need [0, 4, 8, 12, 16] to reach those positions, right? How is that managed?","the-solution-introduce-addends#The Solution? Introduce Addends!":"Effectively, we are doing *(array_ptr + 0/4/8/12/16), and this 0, 4, 8, 12, 16 is what the addend conceptually is. It is what we are adding to obtain the desired element from the base offset.\nThe base address of the symbol is resolved by the array_ptr part and the addend is calculated via the formula i * sizeof(int)\nTherefore, an addend is a constant value that is required to obtain the final runtime address of a symbol correctly.","what-are-addends#What Are Addends?":"What Are Addends?An addend is a constant value added to the symbol’s address during relocation.\nWhen this constant is stored in the relocation entry itself, we call it RELA, which means, “Relocation with Addend”. When this constant is embedded in the section being relocated, we call it REL, which means, “Relocation without Addend”. To understand the logic behind addend, lets have a look at this example."},"title":"what-are-addends"},"/gitbook/docs/understanding-hello-world/linked-elf-analysis/why-arrays-in-c-follow-0-based-indexing/":{"data":{"premise#Premise":"When we do counting, we start from 1, not 0. So why do computers count from 0? Or, why does C count from 0, instead 1?\nThere exist various programming languages which use 1-based indexing as well.\nThis question always arises when someone studies arrays, but rarely get answered properly. At least I wasn’t answered when I was learning arrays. Let’s try to understand it properly this time.","why#Why?":"Lets have a look at this code.\n#include int main(void){ int array_ptr[] = {0, 4, 7, 6, -2}; for (int i = 0; i \u003c 5; i++) { printf(\"%d, \", *(array_ptr + i)); } } This program creates an integer array of size 5. Then it uses a for-loop to iterate over it.\nWhile iterating over each element, it uses pointer dereferencing directly rather than subscripting the array variable using square brackets, like this array_ptr[i] . But it doesn’t matter. We are iterating from 0 to 4, not 1 to 5, why?\nWhen we are counting on fingers, we can simply point at {0, 4, 7, 6, -2} by 1, 2, 3, 4 and 5. But how a computer is supposed to do this? That’s where the problem lies.\nSuppose the array is loaded at address 1000 (in decimals). That means,\n1000 stores 0, 1001 stores 4, 1002 stores 7, 1003 stores 6, and 1004 stores -2 Assume memory addresses increase by 1 unit just for simplicity. In practice, an int usually occupies 4 bytes on 64-bit systems, so addresses jump accordingly.\nNow, array[1] would ideally refer to the first element in terms of normal mathematics. And array[5] would ideally refer to the fifth element.\nBut, we know that arrays are just pointers in C.\nThe variable name itself refers to the starting point of the array, which is the first element in the array in terms of normal mathematics. And internally the system uses pointer dereferencing to obtain the value stored at a memory address. So, array[i] gets translated to *(array + i). Now if we want to access the first element, in terms of normal mathematics, the value of i is going to be 1 , isn’t it?\nBut the variable array itself points to the first element in the array! That’s the issue we are dealing with. The computer requires a consistent way to obtain consecutive addresses. And normal mathematics collide with how memory works.\nIf you were to use the normal way, you can’t generalize a formula to obtain the consecutive address because of how we treat the first element and how the computer sees it.\nBut, it we point the first element by 0, the problem is solved. Because, we are not adding anything to base address, yet it works. It helps in generalizing a formula which we know as this:\naddress of element ith = base_address_of_array + (i-1)*sizeof(data_type) base address = 1000, i = 4, data type is INT, sizeof(INT) is 4 =\u003e address of 4th element = 1000 + (4-1)*4 = 1000 + 12 = 1012 where 4th is in terms of normal mathematics That is why 0-based indexing is used by computers.","why-arrays-in-c-follow-0-based-indexing#Why arrays in C follow 0-based indexing?":"Why arrays in C follow 0-based indexing?"},"title":"why-arrays-in-c-follow-0-based-indexing"},"/gitbook/docs/understanding-hello-world/object-code-analysis/":{"data":{"object-code-analysis#Object Code Analysis":"Object Code Analysis","what-is-object-code#What is object code?":"Object code is a machine-readable representation of source code, typically the output of a compiler or assembler.\nIn Linux, it follows a specific file format, called Executable and Linkable File format.\nTo obtain object code for our source code, run\ngcc -c hello.c -o object_code.o We can start our analysis by inspecting the file type, using file.\n$ file object_code.o hello.o: ELF 64-bit LSB relocatable, x86-64, version 1 (SYSV), not stripped “ELF” means the file follows Executable \u0026\u0026 Linkable Format. “LSB” means it is in little-endian format. “relocatable” means that the file is ready to be linked and is not an executable yet. “x86-64” means it is compiled for 64-bit architecture. “SYSV” means it follows System V (five, not literal-v) convention. “not stripped” means that file still contains items which are not necessary and the code will function the same if they are removed. Any file of type ELF can’t be opened with standard text editors as they are not designed for that purpose. To open them, we need specialized editors and parsers, which can read these files. These include:\nHex editors like xxd and hexdump. Disassemblers, which convert machine code into readable assembly, like objdump, ndisasm and ghidra. ELF parsers like readelf and objdump. We will inspect our file from the perspective of objdump and readelf. These are enough.\nThis section is long enough which is why it is divided into two separate articles, one is for objdump and the other one is for readelf.\nobjdump-perspective.md readelf-perspective.md Note: The output of certain commands is slightly modified. Otherwise, it would be confusing to understand what it actually means."},"title":"_index"},"/gitbook/docs/understanding-hello-world/object-code-analysis/objdump-perspective/":{"data":{"full-disassembly--d#Full Disassembly (-D)":"It can be found here at GitHub.\nThe full disassembly is 69 lines long. But wait, the assembly generated from source was only 29 lines long!\nAs we have read before, assembling lays down the base at which linking can be performed. Refer to a-high-level-overview-of-build-process-in-c.md Our source is a tiny part of the picture. The instructions for printing the string are in the .text section, while the string itself is a read-only data and thus it is stored in the .rodata section. .comment and .eh_frame are compiler sections. If you notice, there is no sign of “Hello, World!\\n” in the disassembly.\nBut, it is there in encoded form. And we can verify that.\nStrings are immutable, therefore, they must be in the .rodata section.\nThis is the .rodata section.\nDisassembly of section .rodata: 0000000000000000 \u003c.rodata\u003e: 0:\t48 rex.W 1:\t65 6c gs ins BYTE PTR es:[rdi],dx 3:\t6c ins BYTE PTR es:[rdi],dx 4:\t6f outs dx,DWORD PTR ds:[rsi] 5:\t2c 20 sub al,0x20 7:\t57 push rdi 8:\t6f outs dx,DWORD PTR ds:[rsi] 9:\t72 6c jb 77 b:\t64 21 00 and DWORD PTR fs:[rax],eax # Offset Machine Code Disassembly Now visit this website https://www.rapidtables.com/convert/number/ascii-to-hex.html and paste \"Hello, World!\\n\" there.\nIn the bottom box, you can find a stream of characters as 48 65 6C 6C 6F 2C 20 57 6F 72 6C 64.\nVisit an ASCII to Hex reference table. And match the characters above in the HEX column with the Symbol column.\n48(H) 65(e) 6C(l) 6C(l) 6F(o) 2C(,) 20(SP) 57(W) 6F(o) 72(r) 6C(l) 64(d)","introduction-to-objdump#Introduction To `objdump`":"objdump or object dump, is a GNU development tool, which specializes in displaying information from object files.\nSyntax of usage looks like: objdump It is a feature-rich tool. The ones that concern us include these:\nobjdump object_code.o -D -M intel # Complete disassembly using Intel syntax objdump object_code.o -t # Symbol table objdump object_code.o -r # Relocation entries objdump object_code.o -h # Section headers","objdump-perspective#`objdump` Perspective":"`objdump` Perspective","relocation-entries--r#Relocation Entries (-r)":"RELOCATION RECORDS FOR [.text]: OFFSET TYPE VALUE 0000000000000007 R_X86_64_PC32 .rodata-0x0000000000000004 000000000000000f R_X86_64_PLT32 puts-0x0000000000000004 RELOCATION RECORDS FOR [.eh_frame]: OFFSET TYPE VALUE 0000000000000020 R_X86_64_PC32 .text Relocations are instructions for the linker/loader program (ld-linux.so).\nIn simple words, a relocation entry asks to replace the mentioned placeholder offset with the real address or offset for this symbol. The offset value is the position relative from the binary where the relocation is required.","section-headers--h#Section Headers (-h)":"{% code fullWidth=“false” %}\nSections: Idx Name Size VMA LMA File off Algn 0 .text 0000001a 0000000000000000 0000000000000000 00000040 2**0 └─ CONTENTS, ALLOC, LOAD, RELOC, READONLY, CODE 1 .data 00000000 0000000000000000 0000000000000000 0000005a 2**0 └─ CONTENTS, ALLOC, LOAD, DATA 2 .bss 00000000 0000000000000000 0000000000000000 0000005a 2**0 └─ ALLOC 3 .rodata 0000000e 0000000000000000 0000000000000000 0000005a 2**0 └─ CONTENTS, ALLOC, LOAD, READONLY, DATA 4 .comment 00000020 0000000000000000 0000000000000000 00000068 2**0 └─ CONTENTS, READONLY 5 .note.GNU-stack 00000000 0000000000000000 0000000000000000 00000088 2**0 └─ CONTENTS, READONLY 6 .eh_frame 00000038 0000000000000000 0000000000000000 00000088 2**3 └─ CONTENTS, ALLOC, LOAD, RELOC, READONLY, DATA # Section Section Size in Virtual Memory Load Memory Offset In Alignment # Index Name Bytes Addrress Address File Where Requirement # It Begins {% endcode %}\nCONTENTS, ALLOC, LOAD, DATA, RELOC, READONLY, CODE are flags.\nCONTENTS: has data in the file. ALLOC: should exist in memory at runtime. LOAD: should be loaded by the linker/loader program. RELOC: has relocation entries. READONLY: not writable. CODE: contains executable instructions. DATA: contains data. The code section (.text) must be available at runtime, has dynamic entries which are required to be loaded by ld-linux.so and it obviously has data in it. Therefore, it has CONTENTS, ALLOC, LOAD, DATA flags.\nWhat is VMA and LMA ?\nWe are going to talk about this very soon. It deserves its own space. Here comes the end of inspection through objdump. Next we are going to be using readelf.","symbol-table--t#Symbol Table (-t)":"SYMBOL TABLE: 0000000000000000 l df *ABS* 0000000000000000 hello.c 0000000000000000 l d .text 0000000000000000 .text 0000000000000000 l d .rodata 0000000000000000 .rodata 0000000000000000 g F .text 000000000000001a hello 0000000000000000 *UND* 0000000000000000 puts # Value (Offset Linker Symbol Section it Size of symbol Symbol name # relative to Visibility Type belongs to # section) Since it is unlinked, the 00... part in the first column is all about placeholders, which would be replaced at runtime. l: local; g: global. Only main has global visibility, because we made it so. df: file definition (name). Remember the .file directive? *ABS*: absolute section, not relocatable. d: section definition, marks the beginning of a section. F: a function. It is located within the .text section. The size is 1a bytes or 0001 1010, which is 26 bytes. *UND* refers to an undefined symbol which would be resolved at link time. This is for the puts function which comes from glibc."},"title":"objdump-perspective"},"/gitbook/docs/understanding-hello-world/object-code-analysis/readelf-perspective/":{"data":{"elf-headers#ELF Headers":"These headers include information (metadata) that identifies a file as an ELF.\nMagic (magic number) is a stream of characters which are used to identify a file format or protocol.\n(Source Wikipedia)\nThey are often placed at the beginning of a data stream and serve as a unique signature to indicate the type or origin of a data. For example, a PNG file typically starts with 89 50 4E 47 0D 0A 1A 0A Here it is 7f 45 4c 46 02 01 01 00 00 00 00 00 00 00 00 00 Here we have 16 pairs of hexadecimal digits, each representing 1-byte. Therefore, it is 16-bytes long. Magic Number Part Explanation 7f 45 4c 46 This is an ELF file. 02 Architecture is 64-bit (01 for 32-bit). 01 Bits are stored in little-endian notation (02 for big-endian). 01 Version, nothing interesting about it. 00 OS/ABI is System V 7f is 127 in decimals, which is for DEL , which is a non-printable ASCII character. Each binary file format (like PNG, JPEG etc) uses a non-printable ASCII character so that the system can distinguish the file from a random text file. This non-printable character indicates that this is a structured file and the next 3 bytes will tell which structure it really uses. 45 4c 46 translates to E L F . Type field explains the purpose of the ELF file.\nCORE (value 4). DYN (Shared object file), used by libraries (value 3). EXEC (Executable file), used by binaries (value 2). REL (Relocatable file), object code ready to be linked (value 1). ELF headers is the first thing in an ELF file, which is why the entry point address is 0x0 .\nPrograms headers are created during linking, that’s why there is 0 for the start, number and size of program headers.\nNo flags were provided so 0x0 in that.\nSection headers are used by the linker during build time. Also, the assembling process lays down a basic section layout. That’s why we are seeing values related to section headers.\nIt starts from 536 bytes in the binary. Total 13 section headers. And their size is 64-bytes. The size of this ELF header is 64 bytes.\nOn 64-bit architecture, it is 64-bytes. On 32-bit architecture, it is 32-bytes. At last, we have this weird entry in the ELF headers, Section header string table index: 12 . Lets not gloss over it as it contains an important design concept.\nJust for the time being, run readelf hello.o -S . You can see an output like Section Headers: [Nr] Name Type Address Offset Size EntSize Flags Link Info Align [ 0] NULL 0000000000000000 00000000 0000000000000000 0000000000000000 0 0 0 [ 1] .text PROGBITS 0000000000000000 00000040 000000000000001a 0000000000000000 AX 0 0 1 It is less formatted. After formatting, we will get something like this Section Headers: [Nr] Name Type Address Offset Size EntSize Flags Link Info Align [ 0] NULL 0000000000000000 00000000 0000000000000000 0000000000000000 0 0 0 [ 1] .text PROGBITS 0000000000000000 00000040 000000000000001a 0000000000000000 AX 0 0 1 Have a look at all of the attributes here. You will find that most of these attributes are having a fixed size. For example, the Address field is always made up of 16 hexadecimal digits, so as the Size field.\nThe only field we are concerned about is the Name field. This field practically has no bounds. Name of a section has to be verbose for practical reasons. Verbosity often comes at the price of length. But, not all the fields need that much verbosity. For example, there is .text and .rela.eh_frame . This creates a tension.\nThe Name field has to be long enough to contain the longest possible values. But the longest possible range is never really utilized fully. This leads to increase in size, which leads to wastage.\nFor example, the longest entry in the “section headers” section in the output above (readelf -a) is 15 characters long. We have 12 sections in total. 12*15 = 180 bytes . Name has to be 15 characters long for each entry. On the contrary, how many bytes are actually used? 87 bytes ONLY. 93 bytes are wasted space.\nThis was just one table. There are other tables as well which uses the same Name field and similar values. If separate tables were created, that would lead to an awful lot of wastage of space. If we have to change these name entries, we have to change it everywhere they are used. Imagine how much compute would get wasted just to manage these names. What is the solution? Minimize the maximum wastage.\nWhat if we create a central name registry and ask everyone to refer to it, instead of storing it individually?\nOnly the central registry would have to face the space problem. This would drastically reduce the overall wastage of space.\nIf any name needs a change, you have to change it once and it would be reflected everywhere else.\nPlus, since these names are independent now, we can altogether remove these central tables when not required. You remember the output of file? $ file object_code.o hello.o: ELF 64-bit LSB relocatable, x86-64, version 1 (SYSV), not stripped This “not stripped” part is actually about removing stuff which is not really required. Removing these string tables is one of the things that happen when turning an elf from “not stripped” to “stripped”.\nWhy remove? Because these are required only by toolchains and during debugging. A binary ready for production doesn’t really need it, which is why it is stripped.\nSo, Section header string table index: 12 means the section header string table, which contains string names to all of these sections in the section header, is at the 12th entry inside section headers table. It was huge! Anyways, you can take some rest before continuing further.","introduction-to-readelf#Introduction To `readelf`":"readelf is a part GNU Binutils Project.\nIt’s a versatile program that excels at parsing files with ELF structure.\nSyntax of usage looks like: readelf [flag(s)]\nAnd yes, it is feature-rich just like objdump . The ones that concern us now include these:\nreadelf object_code.o -a # Everything under one roof readelf object_code.o -h # ELF headers readelf object_code.o -l # Program headers readelf object_code.o -S # Section headers readelf object_code.o -s # Symbol table readelf object_code.o -r # Relocation entries $ readelf hello.o -a ELF Header: Magic: 7f 45 4c 46 02 01 01 00 00 00 00 00 00 00 00 00 Class: ELF64 Data: 2's complement, little endian Version: 1 (current) OS/ABI: UNIX - System V ABI Version: 0 Type: REL (Relocatable file) Machine: Advanced Micro Devices X86-64 Version: 0x1 Entry point address: 0x0 Start of program headers: 0 (bytes into file) Start of section headers: 536 (bytes into file) Flags: 0x0 Size of this header: 64 (bytes) Size of program headers: 0 (bytes) Number of program headers: 0 Size of section headers: 64 (bytes) Number of section headers: 13 Section header string table index: 12 Section Headers: [Nr] Name Type Address Offset Size EntSize Flags Link Info Align [ 0] NULL 0000000000000000 00000000 0000000000000000 0000000000000000 0 0 0 [ 1] .text PROGBITS 0000000000000000 00000040 000000000000001a 0000000000000000 AX 0 0 1 [ 2] .rela.text RELA 0000000000000000 00000168 0000000000000030 0000000000000018 I 10 1 8 [ 3] .data PROGBITS 0000000000000000 0000005a 0000000000000000 0000000000000000 WA 0 0 1 [ 4] .bss NOBITS 0000000000000000 0000005a 0000000000000000 0000000000000000 WA 0 0 1 [ 5] .rodata PROGBITS 0000000000000000 0000005a 000000000000000e 0000000000000000 A 0 0 1 [ 6] .comment PROGBITS 0000000000000000 00000068 0000000000000020 0000000000000001 MS 0 0 1 [ 7] .note.GNU-stack PROGBITS 0000000000000000 00000088 0000000000000000 0000000000000000 0 0 1 [ 8] .eh_frame PROGBITS 0000000000000000 00000088 0000000000000038 0000000000000000 A 0 0 8 [ 9] .rela.eh_frame RELA 0000000000000000 00000198 0000000000000018 0000000000000018 I 10 8 8 [10] .symtab SYMTAB 0000000000000000 000000c0 0000000000000090 0000000000000018 11 4 8 [11] .strtab STRTAB 0000000000000000 00000150 0000000000000013 0000000000000000 0 0 1 [12] .shstrtab STRTAB 0000000000000000 000001b0 0000000000000061 0000000000000000 0 0 1 Key to Flags: W (write), A (alloc), X (execute), M (merge), S (strings), I (info), L (link order), O (extra OS processing required), G (group), T (TLS), C (compressed), x (unknown), o (OS specific), E (exclude), D (mbind), l (large), p (processor specific) There are no section groups in this file. There are no program headers in this file. There is no dynamic section in this file. Relocation section '.rela.text' at offset 0x168 contains 2 entries: Offset Info Type Sym. Value Sym. Name + Addend 000000000007 000300000002 R_X86_64_PC32 0000000000000000 .rodata - 4 00000000000f 000500000004 R_X86_64_PLT32 0000000000000000 puts - 4 Relocation section '.rela.eh_frame' at offset 0x198 contains 1 entry: Offset Info Type Sym. Value Sym. Name + Addend 000000000020 000200000002 R_X86_64_PC32 0000000000000000 .text + 0 No processor specific unwind information to decode Symbol table '.symtab' contains 6 entries: Num: Value Size Type Bind Vis Ndx Name 0: 0000000000000000 0 NOTYPE LOCAL DEFAULT UND 1: 0000000000000000 0 FILE LOCAL DEFAULT ABS hello.c 2: 0000000000000000 0 SECTION LOCAL DEFAULT 1 .text 3: 0000000000000000 0 SECTION LOCAL DEFAULT 5 .rodata 4: 0000000000000000 26 FUNC GLOBAL DEFAULT 1 main 5: 0000000000000000 0 NOTYPE GLOBAL DEFAULT UND puts No version information found in this file. The output is long but when we’ll inspect a linked binary, the output would be huge. So, in comparison to that, the output of inspecting an object file through and elf parser like readelf is quite short.\nLets start understanding this output.","readelf-perspective#`readelf` Perspective":"`readelf` Perspective","relocation-entries#Relocation Entries":"Relocations are instructions for the linker/loader program (ld-linux.so).\nIn simple words, a relocation entry asks to replace the mentioned placeholder offset with the real address or offset for this symbol. What gets relocated is the symbol. There are cross-references to symbols in shared libraries which are required to be resolved. The process that resolves them is called relocation. Relocation entries are of two types.\nRelocation with addend (RELA) Relocation without addend (REL) .rela.text is read as relocation (with addend) for text (code) section.\n.rela.eh_frame is read as relocation (with addend) for exception handling frame section.\nLets start with attributes.\nat offset 0x168/0x198 means these relocation entries start at 168/198 hexadecimal bytes from the position the binary is located at. Offset is the address in a section where the relocation has to be applied. Offset 0x7 Info encodes the symbol index and relocation type. Type refers to the kind of relocation to apply. Sym. Value is the value of the symbol before relocation. Sym. Name + Addend is the symbol name with addend. This marks the end of inspecting into object code. I know we have not figured out the relocation part and the .symtab part yet. It is because it is impossible to be done right now. Relocation is not a single concept. It’s a group of various small concepts combined before you reach the boss. Plus, relocation can’t be understood solely at object level. You need to be at link level to fully comprehend it.\nFor these reasons, it is in the best of our interest to avoid it for the time being. We are avoiding it right now so that we don’t end up glossing over it. But very soon we will get to it.\nTake rest and we’ll continue again.","section-headers#Section Headers":"An ELF is structured into various sections, and “section headers” form a table that contains metadata about each of these sections, including its type, size, and where in the ELF file the section is located.\n[Nr] is the index field.\nName field in a section header is an offset into the .shstrtab (section header string table), which stores actual section names as strings. A parser program, like readelf in our case, uses this offset to locate and display the human-readable name of the section.\nType defines the type of the section.\nPROGBITS contains actual code/data from the source. RELA holds relocation entries with addends. BSS is used for uninitialized data, which occupies no space in file. SYMTAB stands for symbol table. More on this later. STRTAB stands for string table. More on this later. NULL type exists for alignment purposes. Computers index from 0 (like index in arrays) and ELF has preserved that. But to avoid confusion, it has kept the 0th index to NULL and starts everything from 1. Offset is the position of a section inside the ELF.\nAddress is the virtual address at which the section is loaded inside the virtual address space. This address can be absolute or relative.\nIt is absolute when the elf is linked as a non-PIE executable. It is relative [to the address where the binary is loaded] when the elf is linked as a position-independent executable. The 00…00 is a placeholder value as it is resolved at runtime. Size of the section in hexadecimal bytes.\nEntSize is the size of each individual entry within the section, if the section stores a table of uniform entries. If the section just holds raw data, it is 0.\nFlags in a section header specify how the section should be treated in the memory.\nLink refers to the index of a section related to this section.\nInfo is for extra information.\nAlign refers to the required alignment of the section in memory and/or file. We can gloss over it, for now.\nThere are no section groups, program headers and the dynamic section. Lets examine why.\nSection group is a mechanism to group related sections so that linker program can treat them as a single unit.\nWe need not to think about these. Program headers (which is a topic for later discussion) define how the binary is going to be mapped in the memory.\nIf you remember the ELF header section, the object code is of type REL, which stands for relocatable. Program headers define loadable segments for the OS. When a file can’t be loaded, why program headers would be there? The dynamic section tells the dynamic linker (interpreter program) what it needs to know to link the binary at runtime.\nAn object code is not dynamically linked. There is no reason for the dynamic section to be there. All of these are inserted by the dynamic linker program, which is ld in our case. That is why they are absent in the object code."},"title":"readelf-perspective"},"/gitbook/docs/understanding-hello-world/static-linking-v-s-dynamic-linking/":{"data":{"static-linking-vs-dynamic-linking#Static Linking v/s Dynamic Linking":"Static Linking v/s Dynamic Linking"},"title":"_index"},"/gitbook/docs/understanding-hello-world/static-linking-v-s-dynamic-linking/a-surface-level-overview-of-the-differences/":{"data":{"a-surface-level-overview-of-the-differences#A Surface Level Overview Of The Differences":"A Surface Level Overview Of The Differences","conclusion#Conclusion":"A statically linked ELF is a self-contained ELF, and the high size of specific sections shows why it is called a self-contained elf.\nThanks. Bye.","content-difference#Content Difference":"The dynamically linked elf generates a disassembly of 801 lines, whereas the statically linked elf generated a disassembly of 187385 lines.","disassembly-difference#Disassembly Difference":"objdump -D dynamically_linked \u003e dyn_obj -M intel objdump -D statically_linked \u003e stat_obj -M intel","dynamic-section#Dynamic Section":"There is reason for it to be present in a statically linked ELF.","elf-size-difference#ELF Size Difference":"$ ls -l ./*_linked -rwxrwxr-x 1 kali kali 15952 Aug 4 10:27 ./dynamically_linked -rwxrwxr-x 1 kali kali 758424 Aug 4 10:27 ./statically_linked $ ls -l ./*_linked -h -rwxrwxr-x 1 kali kali 16K Aug 4 10:27 ./dynamically_linked -rwxrwxr-x 1 kali kali 741K Aug 4 10:27 ./statically_linked (15952/758424)*100 = 2.1%\nThe dynamically linked elf is just 2% of the size of the statically linked one.","file-header#File Header":"The most interesting part here is the difference in ABI. This means, an operating system supports multiple ABI contracts?\nThe GNU ABI is just an extension of the System V ABI.","introduction#Introduction":"To use static linking on your object files, use the -static flag.\ngcc hello.o -o dynamically_linked gcc -static hello.o -o statically_linked","program-headers#Program Headers":"# Dynamically Linked # Statically Linked PHDR INTERP LOAD LOAD LOAD LOAD LOAD LOAD LOAD LOAD DYNAMIC NOTE NOTE NOTE NOTE NOTE NOTE TLS GNU_PROPERTY GNU_PROPERTY GNU_EH_FRAME GNU_STACK GNU_STACK GNU_RELRO GNU_RELRO Section to segment mapping also differs because of different sections.\nThe size of every program header varies a lot as well. Except GNU_STACK, which is unused in both.","readelf-interpretation#`readelf` Interpretation":"readelf ./dynamically_linked -a \u003e dyn_read readelf ./statically_linked -a \u003e stat_read","relocations#Relocations":"Since there is no dynamic section, .rela.dyn doesn’t make any sense. It is also absent in the statically linked ELF.\nThere are 22 entries in .rela.plt and their type is R_X86_64_IRELATIV, which is a new relocation type. All of these entries are unknown, with no symbol name. Their info field also lacks symbol index.","section-headers#Section Headers":"# Dynamically Linked # Statically Linked NULL NULL .note.gnu.property .note.gnu.property .note.gnu.build-id .note.gnu.build-id .interp .gnu.hash .dynsym .dynstr .gnu.version .gnu.version_r .rela.dyn .rela.plt .rela.plt .init .init .plt .plt .plt.got .text .text .fini .fini .rodata .rodata rodata.cst32 .eh_frame_hdr .eh_frame .eh_frame .gcc_except_table .note.ABI-tag .note.ABI-tag .tdata .tbss .init_array .init_array .fini_array .fini_array .data.rel.ro .dynamic .got .got .got.plt .got.plt .data .data .bss .bss .comment .comment .symtab .symtab .strtab .strtab .shstrtab .shstrtab The statically linked ELF lacks these sections, which are mostly used by the dynamic interpreter program.\n.interp .gnu.hash .dynsym .dynstr .gnu.version .gnu.version_r .rela.dyn .plt.got .eh_frame_hdr .dynamic","size-difference#Size Difference":"$ ls -l ./*_obj -rw-rw-r-- 1 kali kali 34436 Aug 4 10:40 ./dyn_obj -rw-rw-r-- 1 kali kali 9892579 Aug 4 10:40 ./stat_obj $ ls -l ./*_obj -h -rw-rw-r-- 1 kali kali 34K Aug 4 10:40 ./dyn_obj -rw-rw-r-- 1 kali kali 9.5M Aug 4 10:40 ./stat_obj (34436/9892579)*100 = ~0.35%\nThe disassembly for the dynamically linked elf is ~0.35% of the total size of the statically linked one. That’s not even 1%.","size-difference-1#Size Difference":"$ ls -l ./*_read -rw-rw-r-- 1 kali kali 15584 Aug 4 11:06 ./dyn_read -rw-rw-r-- 1 kali kali 167259 Aug 4 11:06 ./stat_read $ ls -l ./*_read -h -rw-rw-r-- 1 kali kali 16K Aug 4 11:06 ./dyn_read -rw-rw-r-- 1 kali kali 164K Aug 4 11:06 ./stat_read (15584/167259)*100 = 9.31%\nThe output of readelf is significantly smaller for the dynamically linked elf.","symbol-tables#Symbol Tables":".dynsym as expected, absent. So as .dynstr .symtab , on the other hand, is heavily populated. It houses 2088 entries.","why#Why?":".interp, .dynsym, .dynstr, .rela.dyn, .plt.got. eh_frame_hdr, .dynamic are self-explanatory. These are specific to runtime resolution.\n.gnu.hash, .gnu.version, .gnu.version_r are also runtime used during runtime resolution.\nTo make symbol lookup fast, ELF uses hash tables for dynamic symbols. Since a static ELF has all the relocations performed during link-time, there is no need for a hash table for lookup. These sections are new in the static ELF.\nrodata.cst32 .gcc_except_table .tdata .tbss .data.rel.ro We can avoid this for now. Comparing the size for section headers,\nHeader Dynamically Linked Statically Linked .rela.plt 0x18 0x210 .plt 0x20 0xb0 .text 0x103 0x74ff9 rodata 0x12 0x1bba4 .eh_frame 0xac 0xb800 .init_array 0x10 0x08 .fini_array 0x10 0x08 .got 0x28 0x88 .got.plt 0x20 0xc8 .data 0x10 0x1a00 .bss 0x08 0x5808 .symtab 0x360 0xc3c0 .strtab 0x1db 0x7c84 .shstrtab 0x11a 0x00f0 Clearly, the size of major sections in a statically linked ELF Is much higher."},"title":"a-surface-level-overview-of-the-differences"},"/gitbook/docs/understanding-hello-world/static-linking-v-s-dynamic-linking/stripping-elf/":{"data":{"how-to-strip-a-binary#How to strip a binary?":"Using strip utility.\n$ strip dynamically_linked -o dynamically_linked_strip $ strip statically_linked -o statically_linked_strip $ ls -l *_linked* -rwxrwxr-x 1 kali kali 15952 Aug 6 14:18 dynamically_linked -rwxrwxr-x 1 kali kali 14472 Aug 6 14:18 dynamically_linked_strip -rwxrwxr-x 1 kali kali 758424 Aug 4 10:27 statically_linked -rwxrwxr-x 1 kali kali 676288 Aug 6 14:17 statically_linked_strip $ ls -l *_linked* -h -rwxrwxr-x 1 kali kali 16K Aug 6 14:18 dynamically_linked -rwxrwxr-x 1 kali kali 15K Aug 6 14:18 dynamically_linked_strip -rwxrwxr-x 1 kali kali 741K Aug 4 10:27 statically_linked -rwxrwxr-x 1 kali kali 661K Aug 6 14:17 statically_linked_strip","stripping-elf#Stripping ELF":"Stripping ELF","what-gets-stripped#What gets stripped?":"Lets analyze readelf to find that out.\nFor both the elfs, .symtab and .strtab are missing in the section headers.\nStripping removes only non-essential symbol information used for debugging and linking, not for execution.\nIf a section is not referenced by runtime execution and not marked SHF_ALLOC, it can be stripped."},"title":"stripping-elf"},"/gitbook/docs/understanding-hello-world/static-linking-v-s-dynamic-linking/use-cases/":{"data":{"use-cases#Use Cases":"Use Cases"},"title":"use-cases"},"/gitbook/docs/understanding-hello-world/whats-next/":{"data":{"whats-next#What\u0026rsquo;s Next?":"What’s Next? Dynamic analysis using gdb Statically linked hello world binary Static vs dynamic linking stripped vs non stripped binaries difference table b/w lazy and eager binding"},"title":"whats-next"},"/gitbook/docs/understanding-hello-world/why-main-function-shouldnt-be-of-type-void/":{"data":{"can-this-undefined-behavior-be-tested#Can this undefined behavior be tested?":"Well, I am still learning about it and if I find something, I’ll share.","if-thats-wrong-why-it-works#If That\u0026rsquo;s Wrong, Why It Works?":"In simple words, there is no guarantee it will work.\nIt is a case of undefined behavior. And the problem with undefined behavior is that it maybe what you expect or what you may not. Compilers optimize it.","why-main-function-shouldnt-be-of-type-void-#Why main Function Shouldn\u0026rsquo;t Be Of Type `void` ?":"Why main Function Shouldn’t Be Of Type `void` ?{% code title=“hello.c” %}\n#include void main(){ printf(\"Hello, World!\\n\"); } {% endcode %}\nBut when I started analyzing it, I found out that void signature for main function is not valid. And I was shocked.\nBefore I started this journey, I have explored processes in x86_64 Linux.\nThere I understood why every process must return an exit code. Although there can be many reasons for this, but the most easily comprehensible ones are:\nhow the OS will know that a process is finished?, or how the OS will know if an error occurred and the process can no longer execute? All in all, the operating system is the one that manages all these processes and it needs something to manage the state of processes.\nvoid data type returns nothing. If I used void main(); signature, I am returning nothing, which is a violation of how processes actually work in Linux.\nIf you look at ISO C standard, it clearly defines that the main function must be of type int and should return an integer value, which is considered the exit status.\nHere is that document.\nOpen page 24 in the PDF or, search for “Program startup”. It is located at “5.1.2.2.1” section.\nThe two valid signatures for main function are:\\\nint main(void); int main(int argc, char *argv[]); // Both returns 0 by default, if nothing else is mentioned. Then why does void main(); work despite being incorrect? Why old C tutorials use this signature?\nC++ raises an error during compile time, error: '::main' must return 'int' .","why-old-tutorials-use-it#Why old tutorials use it?":"Only those tutors can answer this. But I have a few ideas in mind.\nOnline tutorials are never really meant to understand low level engineering behind programming. They are aimed at learning to pass instructions to the computer, and high level abstractions allow that very easily. So, practically there is no need to understand this. And when someone do actual development in C, they themself find what is right and wrong. Understanding low level engineering is also not easy. Why a function needs a data type? Why does a function returns? Who calls main? What are processes? How Linux manages processes? Why a process returns an exit code? All the above mentioned things are largely a part of computer architecture. And you know how many people start with computer architecture. I too didn’t learn it until now. Not everyone might need it as well but those who need it will find it. It’s a complex space. So, lets not blame anyone. It is highly possible that only I don’t know about this. But I thought there is nothing bad in sharing this."},"title":"why-main-function-shouldnt-be-of-type-void"}}